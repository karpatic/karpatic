{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "> Assorted notes on the topic. Near entirly quotes from other sources.\n",
    "\n",
    "- tab: Correlation\n",
    "- keywords: ['data'] \n",
    "- hide_sitemap: false\n",
    "- hide_toc: true\n",
    "- hide_breadcrumbs: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n76TP3Ad_xu4"
   },
   "source": [
    "[Measures for the support-confidence framework for mining association rules](https://s2.smu.edu/~mhahsler/crawler_test/michael.hahsler.net/research/association_rules/measures.html):\n",
    "\n",
    "- **Support**\n",
    "- **Confidence**\n",
    "\n",
    "Some alternative measures:\n",
    "- All-confidence\n",
    "- Collective strength\n",
    "- Conviction\n",
    "- Coverage\n",
    "- Leverage\n",
    "- Lift (originally called interest)\n",
    "- Other measures \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z9kGikY_xu7"
   },
   "source": [
    "[Notes on Nonparametric Independence Tests](http://users.stat.umn.edu/~helwig/notes/npind-Notes.pdf)\n",
    "\n",
    "Kendall’s Rank Correlation: \n",
    "\n",
    "Spearman’s Rank Correlation:\n",
    "\n",
    "Suppose we have n bivariate observations \n",
    "\n",
    "(X1, Y1), . . . ,(Xn, Yn) where (Xi , Yi) is i-th subject’s data \n",
    "\n",
    "We want to make inferences about association between X and Y \n",
    "\n",
    "Let FX,Y denote joint distribution of X and Y \n",
    "\n",
    "Let FX and FY denote marginal distributions of X and Y \n",
    "\n",
    "Null hypothesis is statistical independence: \n",
    "\n",
    "FX,Y (x, y) = FX (x)FY (y) for all (x, y)\n",
    "\n",
    "Independence assumption: {(Xi , Yi)} n i=1 are iid from some bivariate population Continuity assumption: FX,Y is a continuous distribution\n",
    "\n",
    "\n",
    "some use cases for graph algorithms:\n",
    "- real-time fraud detection\n",
    "- real-time recommendations\n",
    "- streamline regulatory compliance\n",
    "- management and monitoring of complex networks\n",
    "- identity and access management\n",
    "- social applications/features\n",
    "\n",
    "There are 3 main categories of graph algorithms that are currently supported in most frameworks (networkx in Python, or in Neo4J for example) :\n",
    "- Pathfinding: identify the optimal path depending on availability and quality for example. We’ll also include search algorithms in this category. This can be used to identify the quickest route or traffic routing for example.\n",
    "- Centrality: determine the importance of the nodes in the network. This can be used to identify influencers in social media for example or identify potential attack targets in a network\n",
    "- Community detection: evaluate how a group is clustered. This can be used to segment customers and detect fraud for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz3UFTsJ_xu8"
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBVEdBb2_xu9"
   },
   "source": [
    "http://users.stat.umn.edu/~helwig/notes/cluster-Notes.pdf\n",
    "\n",
    "Let x = (x1, . . . , xp) 0 and y = (y1, . . . , yp) 0 denote two arbitrary vectors. \n",
    "Problem: We want some rule that measures the “closeness” or “similarity” between x and y. \n",
    "\n",
    "How we define closeness (or similarity) will determine how we group the objects into clusters. \n",
    "\n",
    "Rule 1: Pearson correlation between x and y \n",
    "\n",
    "Rule 2: e Minkowski Metric (Manhattan, Euclidean, Chebyshev) distance between x and y \n",
    "\n",
    "Rule 3: Number of matches, i.e., Pp j=1 1{xj=yj} \n",
    "\n",
    "__Hierarchical clustering__ uses a series of successive mergers or divisions to group N objects based on some distance. \n",
    "\n",
    "__Agglomerative Hierarchical Clustering__ (bottom up) 1 Begin with N clusters (each object is own cluster) 2 Merge the most similar objects 3 Repeat 2 until all objects are in the same cluster \n",
    "\n",
    "__Divisive Hierarchical Clustering__ (top down) 1 Begin with 1 cluster (all objects together) 2 Split the most dissimilar objects 3 Repeat 2 until all objects are in their own cluster\n",
    "\n",
    "We know how to define dissimilarity between objects (i.e., duv ), but how do we define dissimilarity between clusters of objects?\n",
    "\n",
    "To quantify the distance between two clusters, we could use: \n",
    "- Single Linkage: minimum (or nearest neighbor) distance\n",
    "- Complete Linkage: maximum (or furthest neighbor) distance \n",
    "- Average Linkage: average (across all pairs) distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9XqdFVt_xu-"
   },
   "source": [
    "\n",
    "Non-hierarchical clustering partitions a set of N objects into K distinct groups based on some distance (or dissimilarity). The number of clusters K can be known a priori or can be estimated as a part of the procedure.\n",
    "\n",
    "\n",
    "Why are Permutations Useful for Statistics? Classic statistical paradigm is: collect some data form null hypothesis H0 design test statistic derive sampling distribution of test statistic under H0 In many cases, the null hypothesis is the nil hypothesis, i.e., no effect. Under the nil hypothesis, all possible outcomes (permutations) are equally likely, so permutations relate to sampling distributions.\n",
    "\n",
    "Principal Components Analysis (PCA) finds linear combinations of variables that best explain the covariation structure of the variables. There are two typical purposes of PCA: 1 Data reduction: explain covariation between p variables using r < p linear combinations 2 Data interpretation: find features (i.e., components) that are important for explaining covariation\n",
    "\n",
    "The data matrix refers to the array of numbers X =   {x11 x1p,  x21 x2p, xn1 xnp} where xij is the j-th variable collected from the i-th item (e.g., subject). \n",
    "items/subjects are rows variables are columns X is a data matrix of order n × p \n",
    "(# items by # variables).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmfgTQv3_xu-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-learn\n",
    "\n",
    "# https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf\n",
    "\n",
    "# k-Means Clustering\n",
    "\n",
    "#As K Means calculates the distance between the features to decide if the following observation belongs to a certain centroid, we have to preprocess our data by encoding our Categorical Variables and filling up missing values. A simple function to preprocess is shown below.\n",
    "\n",
    "def simple_preprocessing(dataframe, train=True):\n",
    "    le = LabelEncoder()\n",
    "    X = dataframe.drop(['PassengerId', 'Cabin', 'Name', 'Ticket'], axis=1)\n",
    "    X['Age'] = X['Age'].fillna(value=X['Age'].mode()[0])\n",
    "    X['Embarked'] = le.fit_transform(X['Embarked'].fillna(value=X['Embarked'].mode()[0]))\n",
    "    X['Sex'] = np.where(X['Sex'] == 'male', 1, 0)\n",
    "    \n",
    "    if train:\n",
    "        X = X.drop(['Survived'], axis=1)\n",
    "        y = np.where(dataframe['Survived'] == 1, 'Alive', 'Dead')\n",
    "        y = pd.get_dummies(y, columns=['Survived'])\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "      \n",
    "# Now that we have treated our data, we have to perform feature scaling so that the distance calculated across the features can be compared. This is done easily via sklearn.preprocessing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoXtZ3Fe_xu_"
   },
   "outputs": [],
   "source": [
    "# Hierarchical Agglomerative Clustering\n",
    "# Agglomerative clustering uses a bottom-up approach where individual observations are joined together iteratively based on their distance. \n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "sample_train,sample_val, gt_train, gt_val = train_test_split(train_df,\n",
    "                                                             train_df['Survived'],\n",
    "                                                             test_size=0.05,\n",
    "                                                             random_state=99)\n",
    "\n",
    "sample_val_processed = simple_preprocessing(sample_val, train = False)\n",
    "sample_val_processed = scaler.fit_transform(sample_val_processed)\n",
    "mergings = linkage(sample_val_processed, method='complete')\n",
    "fig = plt.figure(figsize = (16,10))\n",
    "\n",
    "dendrogram(mergings,\n",
    "           labels=np.array(sample_val['Name']),\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Clustering Association Notes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
