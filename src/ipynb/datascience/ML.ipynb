{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.itl.nist.gov/div898/handbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-standard predictors** \n",
    "- Nonlinear [regression](https://en.wikipedia.org/wiki/Nonlinear_regression)\n",
    "- [Nonparametric](https://en.wikipedia.org/wiki/Nonparametric_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Classification**](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) Identifying to which category an object belongs to. \n",
    "**Applications** : Spam detection, Image recognition. \n",
    "\n",
    "**Algorithms** : \n",
    "- [SVM](https://scikit-learn.org/stable/modules/svm.html#svm-classification)- [nearest neighbors](https://scikit-learn.org/stable/modules/neighbors.html#classification)\n",
    "- [random forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)\n",
    "- [Examples](https://scikit-learn.org/stable/auto_examples/index.html#general-examples)\n",
    "\n",
    "**Classification**:\n",
    "\n",
    "- k-nearest neighbors - supervised\n",
    "\n",
    "- decision trees (c4.5) - supervised - noncontiguous, data is if/else\n",
    "\n",
    "- gradient boosted decision trees\n",
    "\n",
    "- Random Forest - Super/Unsuper - Best Split\n",
    "\n",
    "- classification and regresstion tree(cart)\n",
    "\n",
    "- SVM - Super/Unsuper - Maximum Margin\n",
    "\n",
    "- naive bayes - supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Regression**](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) Predicting a continuous-valued attribute associated with an object.\n",
    "\n",
    "1. **Applications** : Drug response, Stock prices. \n",
    "\n",
    "2. **Algorithms** : \n",
    "- [SVR](https://scikit-learn.org/stable/modules/svm.html#svm-regression), - [ridge regression](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n",
    "- [Lasso](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "- Simple linear regression\n",
    "- OLS\n",
    "- GLM\n",
    "- Bayesian regression \n",
    "- [Examples](https://scikit-learn.org/stable/auto_examples/index.html#general-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Clustering](https://scikit-learn.org/stable/modules/clustering.html#clustering) \n",
    "\n",
    "Automatic grouping of similar objects into sets.\n",
    "\n",
    "1. **Applications** : Customer segmentation, Grouping experiment outcomes \n",
    "\n",
    "2. **Algorithms** : \n",
    "- [k-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)- [spectral clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering)\n",
    "- [mean-shift](https://scikit-learn.org/stable/modules/clustering.html#mean-shift)\n",
    "- [Examples](https://scikit-learn.org/stable/auto_examples/index.html#cluster-examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Dimensionality reduction**](https://scikit-learn.org/stable/modules/decomposition.html#decompositions) Reducing the number of random variables to consider.\n",
    "\n",
    "1. **Applications** : \n",
    "- Visualization\n",
    "- Increased efficiency \n",
    " \n",
    "2. **Algorithms** : \n",
    "- [PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca), [feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection)\n",
    "- [non-negative matrix factorization](https://scikit-learn.org/stable/modules/decomposition.html#nmf)\n",
    "- [Examples](https://scikit-learn.org/stable/auto_examples/index.html#decomposition-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Model selection**](https://scikit-learn.org/stable/model_selection.html#model-selection) Comparing, validating and choosing parameters and models.\n",
    "\n",
    "1. **Goal** : Improved accuracy via parameter tuning \n",
    "\n",
    "2. **Modules** : \n",
    "- [grid search](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "- [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)\n",
    "- [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)\n",
    "- [Examples](https://scikit-learn.org/stable/auto_examples/index.html#general-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Preprocessing**](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) Feature extraction and normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Preprocessing**](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) Feature extraction and normalization. \n",
    "1. **Application** : Transforming input data such as text for use with machine learning algorithms. \n",
    "2. **Modules** : \n",
    "- [preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)\n",
    "- [feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#feature-extraction)\n",
    "- [Examples](https://scikit-learn.org/stable/auto_examples/index.html#general-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASSIFICATION AND REGRESSION PROBLEMS**\n",
    "\n",
    "There are numerous algorithms for predicting continuous variables or categorical variables from a set of continuous predictors and/or categorical factor effects. For example, in [_GLM (General Linear Models)_](http://www.statsoft.com/textbook/statistics-glossary/g.aspx?button=g#general%20Linear%20Model) and [_GRM (General Regression Models)_](http://www.statsoft.com/textbook/general-regression-models/), we can specify a linear combination (design) of continuous predictors and categorical factor effects (e.g., with two-way and three-way interaction effects) to predict a continuous dependent variable. In _GDA (General Discriminant Function Analysis)_, we can specify such designs for predicting categorical variables, i.e., to solve classification problems.\n",
    "\n",
    "**Regression-type problems.** Regression-type problems are generally those where we attempt to predict the values of a continuous variable from one or more continuous and/or [categorical predictor variables](http://www.statsoft.com/textbook/statistics-glossary/c.aspx?button=c#Categorical%20Predictor).\n",
    "\n",
    "**Classification-type problems.** Classification-type problems are generally those where we attempt to predict values of a categorical [dependent](http://www.statsoft.com/textbook/statistics-glossary/i.aspx?button=i#Independent%20vs.%20Dependent%20Variables) variable (class, group membership, etc.) from one or more continuous and/or [categorical predictor variables](http://www.statsoft.com/textbook/statistics-glossary/c.aspx?button=c#Categorical%20Predictor).\n",
    " There are a number of methods for analyzing classification-type problems and to compute predicted classifications, either from simple continuous predictors (e.g., binomial or multinomial logit regression in [_GLZ_](http://www.statsoft.com/textbook/statistics-glossary/g.aspx?button=g#generalized%20Linear%20Model)), from categorical predictors (e.g., [_Log-Linear analysis_](http://www.statsoft.com/textbook/statistics-glossary/l.aspx?button=l#Log-Linear%20Analysis) of multi-way frequency tables), or both (e.g., via ANCOVA-like designs in _GLZ_ or _GDA_).\n",
    "\n",
    "**Tree methods are nonparametric and nonlinear + Simplicity of results.\n",
    " -Specify Criteria for Predictive Accuracy, Selecting Splits, When to Stop Splitting.**\n",
    "\n",
    "**Data Classification** -\\&gt; Effectiveness [Data\\_classification](https://en.wikipedia.org/wiki/Data_classification_(business_intelligence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Models :**\n",
    " We can think of a model as a template. When data is processed through a learning model, what will come out the other end is insight.\n",
    " The model is nothing more than a set of operations performed on the data.\n",
    " Models are typically made in a static environment by (drilling/ rolling, pivoting, slicing/ dicing, Etc.) through data and may involve Integrating multiple mining functions (ex. Classifying than Clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Data classification](https://en.wikipedia.org/wiki/Data_classification_(business_intelligence))\n",
    "\n",
    "According to Golfarelli and Rizzi, these are the measures of effectiveness of the classifier:\n",
    "\n",
    "- _ **Predictive accuracy** _ : How well does it predict the categories for new observations?\n",
    "- _ **Speed** _ : What is the computational cost of using the classifier?\n",
    "- _ **Robustness** _ : How well do the models created perform if [**data quality**](https://en.wikipedia.org/wiki/Data_quality) is low?\n",
    "- _ **Scalability** _ : Does the classifier function efficiently with large amounts of data?\n",
    "- _ **Interpretability** _ : Are the results understandable to users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Models**\n",
    "\n",
    "Hierarchical Generative\n",
    "- Gaussian Mixture Models\n",
    "- Hidden Markov Models\n",
    "- Naive Bayes\n",
    "- GANS\n",
    "\n",
    "Discriminative\n",
    "- NN\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "\n",
    "Descriptive\n",
    "- Derived from the Attributes of Data (mean, median, mode, avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLE** is the workhorse estimation technique of frequentist statistics\n",
    "**latent variables** :\n",
    "\n",
    "- expectation maximization\n",
    "\n",
    "- methods of moments\n",
    "\n",
    "- signal separation\n",
    "\n",
    "-- principal component analysis\n",
    "\n",
    "-- singular value decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "properties of estimators (bias, consistency, efficiency, sufficiency, robustness).\n",
    "\n",
    "Testing: Type I and II errors, power, likelihood ratios\n",
    "\n",
    " Methodology of probabilistic process models: \n",
    " - Dirichlet\n",
    " - Gaussian\n",
    " - basis/kernel expansion\n",
    " - splines, wavelets\n",
    " - support vector machines \n",
    " - other local regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**latent variables**:\n",
    "- expectation maximization\n",
    "- methods of moments\n",
    "- signal separation\n",
    "- principal component analysis\n",
    "- singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Reduction:**\n",
    "\n",
    "- PCA - describe greatest features\n",
    "\n",
    "- Cross Correlation Analysis\n",
    "\n",
    "- Linear Discrimination, Combines Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**concepts**:\n",
    "\n",
    "accuracy (tp+tn/p+n)\n",
    "\n",
    "precision: tp/(tp+fp)\n",
    "\n",
    "specificity: tn/(fp+tn)\n",
    "\n",
    "sensitivity: tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**:\n",
    "\n",
    "- linear regressions - numeric\n",
    "\n",
    "- logistic regressions - categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ensemble learning** - bagging boosting stacking additive regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Nets**: (super/unsuper)\n",
    "\n",
    "- autoencoders\n",
    "\n",
    "- deep beliefe nets\n",
    "\n",
    "- hebbian learning\n",
    "\n",
    "- gans\n",
    "\n",
    "- implicit density model\n",
    "\n",
    "- som"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering**:\n",
    "\n",
    "- Hierarchal\n",
    "\n",
    "- Kmeans (Euclidean, Mikowski, Manhattan, #Clusters) \n",
    "- super/unsuper - sort by centroid\n",
    "\n",
    "- anomoly-detection - outliers, super/unsuper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN** \n",
    "- LSTM\n",
    "- Hierarchal\n",
    "- Stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FeedForward** :\n",
    "- MIP\n",
    "- Autoencoder\n",
    "- Probablistic\n",
    "- Convolusional\n",
    "- Time Delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online Learning** :\n",
    "- Data Efficient and Adaptable\n",
    "- No Data storage needed\n",
    "- Stochastic Gradient Descent\n",
    "- No data storage needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T-Distribution** - Visualize high density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**RISK** :Redundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g.,[linear regression](https://en.wikipedia.org/wiki/Linear_regression),[logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), and[distance based methods](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm)) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of[regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)).\n",
    " Presence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g.,[linear regression](https://en.wikipedia.org/wiki/Linear_regression),[logistic regression](https://en.wikipedia.org/wiki/Logistic_regression),[Support Vector Machines](https://en.wikipedia.org/wiki/Support_Vector_Machines),[naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)) and distance functions (e.g.,[nearest neighbor methods](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm),[support vector machines with Gaussian kernels](https://en.wikipedia.org/wiki/Support_Vector_Machines)) generally perform well. However, if there are complex interactions among features, then algorithms such as[decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning) and[neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.\n",
    "\n",
    "**concepts** :\n",
    "\n",
    "accuracy (tp+tn/p+n)\n",
    "\n",
    "precision: tp/(tp+fp)\n",
    "\n",
    "specificity: tn/(fp+tn)\n",
    "\n",
    "sensitivity: tp/(tp+fn)\n",
    "**properties of estimators** (bias, consistency, efficiency, sufficiency, robustness).\n",
    "\n",
    "**Testing** : Type I and II errors, power, likelihood ratios\n",
    "\n",
    "**Critical Review**\n",
    " Configuration and Risk\n",
    " Rational for Complexity\n",
    " Weighted parameters\n",
    " Weighted performance metrics\n",
    " Risk assessments and mitigations\n",
    " Technologies\n",
    " Roadmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Error Measures:\n",
    " - (root) Mean Squared Error - continuous data, sensitivity to outliers\n",
    " - median absolute deviation - continuous data, often more robust\n",
    " - sensitivity (Recall) - if you want few missed positives\n",
    " - specificity - if you want few negatives called positives\n",
    " - accuracy - weights false positives / negatives equally\n",
    " - Concordance - one example is kappa\n",
    "\n",
    " Key issues: accuracy, overfitting, interpretability, computational speed.\n",
    " Pay attention to - confounding variables, complicated interactions, skewness, outliers, nonlinear patterns, variance changes, units/scale issues, overloading, regression, correlation and causation\n",
    " Confounder: a variable that is correlated with both the outcome and covariates\n",
    " - confounders can change the regression line.\n",
    " - detected with exploration\n",
    "\n",
    "\n",
    "**Hierarchical** - Distance or similarity? - continuous(euclidean/correlation), binary - manhattan\n",
    "**Graphs** - help understand properties, find patterns, suggest future modelings, debug, and communicate.\n",
    "**Bagging and Boosting** - Combine classifiers to improve accuracy but make harder to interpret. Predictive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
