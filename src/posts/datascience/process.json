{"meta":{"title":"Data Science","summary":"Assorted notes on the topic. Near entirly quotes from other sources.","tab":"Data Science","keywords":"['data']","hide_sitemap":"false","hide_toc":"true","hide_breadcrumbs":"false","filename":"process"},"content":"<h1>Table Of Contents</h1>\n <ol start=\"0\">\n<li><strong>Background</strong></li>\n<li><strong>Business Understanding</strong></li>\n</ol>\n<ul>\n<li><ol>\n<li><strong>Meta-Data</strong></li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li><strong>Data Types</strong> - Text, Number, Date/Time, other</li>\n</ol>\n</li>\n<li><ol start=\"3\">\n<li><strong>Data Attributes</strong> - Nominal, Symmetry, Skew, modality, kurtosis, etc</li>\n</ol>\n</li>\n</ul>\n<ol start=\"2\">\n<li><strong>Data Understanding</strong></li>\n</ol>\n<ul>\n<li><ol>\n<li><strong>Data Exploration</strong></li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li><strong>Graphics</strong></li>\n</ol>\n</li>\n</ul>\n<ol start=\"3\">\n<li><strong>Data Preparation</strong></li>\n<li><strong>Data Modeling</strong></li>\n<li><strong>Evaluation</strong></li>\n</ol>\n <h2>0. Background</h2>\n <h4><strong>What is Data Science</strong></h4>\n<p> Principally, a data scientists job is to apply scientific methods to data in a process often refered to as data-mining. The purpose of this mining is to find informative patterns in the data and to then make meaningful /actionable knowledge of said information. Often times the exploration of the data and development of something actionable is accompanied Machine Learning Algorithms. While <a href=\"https://priceonomics.com/whats-the-difference-between-data-science-and/\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">contentious</a>, Statistics is generally understood as more focused on quantative descriptions of data and that data science grew out of statistics as an applied branch of the field with an emphasis on  qualatative data and actionability of their output. <a href=\"https://en.wikipedia.org/wiki/Data_science#:~:text=Data%20science%20is%20a%20%22concept,domain%20knowledge%20and%20information%20science.\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">source</a> </p>\n <blockquote>\n<p>The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes&#39; theorem (1700s) and regression analysis (1800s)... <strong>Data mining is the process of applying [machine learning] methods with the intention of uncovering hidden patterns.</strong><a href=\"https://archive.org/details/dataminingconcep0000kant\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">[16]</a> in large data sets. It bridges the gap from applied statistics and artificial intelligence.</p>\n</blockquote>\n <p><strong>Data Mining <a href=\"https://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1996-Fayyad.pdf\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Tasks</a></strong></p>\n<ul>\n<li>Anomaly Detection</li>\n<li>Association Rule Learning</li>\n<li>Clustering</li>\n<li>Classification</li>\n<li>Regression</li>\n<li>Summarization</li>\n</ul>\n <blockquote>\n<p> Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term &quot;knowledge discovery in databases&quot; for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities.[14] Currently, <strong>the terms data mining and knowledge discovery are used interchangeably.</strong> <a href=\"https://en.wikipedia.org/wiki/Data_mining\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">data-mining</a></p>\n</blockquote>\n <p>The most simple conceptual model of the the data scientists job is as follows:</p>\n<ol>\n<li><strong>Preproces</strong></li>\n<li><strong>Data Mine</strong></li>\n<li><strong>Resuls Validation</strong></li>\n</ol>\n<p> The Knowledge Discovery in Database (KDD) Process uses these 5 steps:</p>\n<ol>\n<li><strong>Selection</strong></li>\n<li><strong>Pre-processing</strong></li>\n<li><strong>Transformation</strong></li>\n<li><strong>Data mining</strong></li>\n<li><strong>Interpretation/evaluation</strong></li>\n</ol>\n<p> And a more modern competing approach is  to use the <a href=\"https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">CRISP-DM</a> approach which splits the data scientists responsibility along these 6 themes</p>\n<ol>\n<li><strong>Business understanding</strong></li>\n<li><strong>Data understanding</strong></li>\n<li><strong>Data preparation</strong></li>\n<li><strong>Modeling</strong></li>\n<li><strong>Evaluation</strong></li>\n<li><strong>Deployment</strong></li>\n</ol>\n <p>I like to break out data science steps as so:</p>\n<ol>\n<li><strong>Acquire</strong> - (Selection &amp; Business Understanding) Understanding what&#39;s needed, Retrieving data and injesting it through the act of preliminary exploration and storage</li>\n<li><strong>Explore</strong> - (Data Understanding) find extremes, range within, distributions, anomolies, etc. Performed after any data processing is done. </li>\n<li><strong>Pre-process</strong> - Cleaning the data for further analytical activities and exploration by filtering or performing non-critical prep work on the data like removing noise or missing vals or meet size/time limit constraints. Typically only done once.</li>\n<li><strong>Process</strong> - Once the raw data is cleaned and generically prepped, additional transformational processes may be required to make applicable use of data for a scientific specific model. augmentations may include sorting, filtering and arranging, and aggregating and integration. Many different models and precesses may be used, each deserving an exploration.</li>\n<li><strong>Model</strong> - test train predict, regressions, clusterings, etc. Analysis is performed by applying models on data and recording the output, Includes finding Correlations, and aontextualize any discoveries. </li>\n<li><strong>Act</strong> - Inteperite the data and draw evaluative conclusions and communicating them in the form of something presentable/ actionable.</li>\n</ol>\n<p> This way of thinking differentiates itself from the former two models of thought is that the first step is idealy only performed once wheras the 2nd step is performed after any data is changed. The third step is conditinoally applied from CRISP-DM in that in breaks out the pre-processing and integration/transformations steps as they can are sometimes optional and a big enough topic to deserve their own section. This article has been sectioned along CRISP-DM Standards. </p>\n <p>At One Point in time I also described it like so:</p>\n <p><strong>Process</strong> - Data - raw/ processed data.</p>\n<p> <strong>Figures</strong> - exploratory/ final figures.</p>\n<p> <strong>Code</strong> - raw scripts, final scripts.</p>\n<p> <strong>Text</strong> - readme / analysis.</p>\n<p> <strong>Steps</strong> - define the question, define the ideal data, obtain data, clean data, exploratory data analysis, statistical prediction/modeling, interpret results, challenge results, synthesize/write up results. Create reproducible code.</p>\n<p> <strong>Language</strong> ( describe, correlate/associated, lead to/causes, predicts). Interpret and explain</p>\n<p> <strong>Challenge all steps</strong> - question, data, processing, analysis, conclusion</p>\n<p> <strong>synthesize/write up</strong> results -&amp;gt; lead with question, summarize analysis, order analyses as story rather than chronologically. Include pretty figures.</p>\n <h4><strong>Geo-Spatial-Time-Series</strong></h4>\n<p> To work with this data, ds&#39;s can use these tools:</p>\n<ul>\n<li>Model </li>\n<li>Process </li>\n<li>Visual</li>\n<li>Exploratory spatial analysis</li>\n<li>spatial autocorrolation</li>\n<li>spatial regression</li>\n<li>interpolation</li>\n<li>grid based stats</li>\n<li>point based stats</li>\n<li>spatial network analysis</li>\n<li>spatial clustering.</li>\n</ul>\n <h3>Free Softwares</h3>\n <p> <strong>Free software for data analysis</strong></p>\n<ul>\n<li><p>DevInfo – a database system endorsed by the United Nations Development Group for monitoring and analyzing human development.</p>\n</li>\n<li><p>ELKI – data mining framework in Java with data mining oriented visualization functions.</p>\n</li>\n<li><p>KNIME – the Konstanz Information Miner, a user friendly and comprehensive data analytics framework.</p>\n</li>\n<li><p>Orange – A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.</p>\n</li>\n<li><p>Pandas – Python library for data analysis</p>\n</li>\n<li><p>PAW – FORTRAN/C data analysis framework developed at CERN</p>\n</li>\n<li><p>R – a programming language and software environment for statistical computing and graphics.</p>\n</li>\n<li><p>ROOT – C++ data analysis framework developed at CERN</p>\n</li>\n<li><p>SciPy – Python library for data analysis</p>\n</li>\n<li><p>Vega-Lite{ description, name, title, data, mark, encoding, transform}</p>\n</li>\n</ul>\n <p><strong>Existing Services</strong></p>\n<ul>\n<li>Northstar </li>\n<li>automl</li>\n<li>h20.autokeras</li>\n<li>autosklearn</li>\n<li>googlecloudml</li>\n<li>tpot</li>\n<li>dive </li>\n<li>orange</li>\n<li>xl</li>\n<li>spss</li>\n<li>vega_2</li>\n</ul>\n <p><strong>Existing Service Features</strong></p>\n<ul>\n<li><p>Available attributes -&amp;gt; Principle-Axis Visualization, Secondary/Tertiary Axis&#39;s</p>\n</li>\n<li><p>Sources | sheet1| + | + | + | + ..</p>\n</li>\n<li><p>Hierarchy Maps -&amp;gt; Dimensions</p>\n</li>\n<li><p>Marks(map.bar.etc) colors, label, tooltip, size, detail</p>\n</li>\n<li><p>Filter(range), show only relevant, include nulls</p>\n</li>\n<li><p>Colrs -&amp;gt; by freq or what?</p>\n</li>\n<li><p>Specific style rules for each column/row</p>\n</li>\n<li><p>transformation/prediction, nlp/queries</p>\n</li>\n</ul>\n <p>##1. Business Understanding</p>\n <p>Systems = Infrastructure =&amp;gt; Technical, (hw, ppl, processes), integrate</p>\n<p> Software = Apps =&amp;gt; Business</p>\n<p> Data provenance</p>\n<p> Features Geometry = Coordinates/ Rings</p>\n <h3>Challenges</h3>\n <p>80% of the time of client requests is spent on [access, format, connect, and fix], often the solutions to these client requests are client specific artifacts or heuristics/ classifications requirements.\t</p>\n <p>Administrative data is heavily Biased and Dependent. Survey data less so</p>\n<p> Dependent data does not lend itself to Bayesian methods.</p>\n<p> Depending on Collection Methodology</p>\n<ul>\n<li>I need to know (which/if any) of these are not applicable for our purposes</li>\n<li>Census(descriptive), </li>\n<li>Observational study(inferential), </li>\n<li>convenience sample(all types-may be biased), </li>\n<li>randomized trial (causal). \n Other types:</li>\n<li>prediction study(prediction), </li>\n<li>studies over time [cross sectional(inferential), </li>\n<li>longitudinal(inferential, predictive) ], </li>\n<li>retrospective ( inferential)</li>\n</ul>\n <p><strong>Conceptual Challenges</strong>:</p>\n<p> AutoEda Project Challenges</p>\n<ul>\n<li><p>Noteable Free Software</p>\n</li>\n<li><p>What&#39;s Novel?</p>\n</li>\n<li><p>philosophy - Semantic -&gt; noideal, structure/ ai </p>\n</li>\n<li><p>data and methodolgies reflect goals</p>\n</li>\n<li><p>complexity - Basic, many ways</p>\n</li>\n<li><p>Visualy - Basic, many ways, using the right tool</p>\n</li>\n<li><p>meaning - All relative</p>\n</li>\n<li><p>Resource utilization - Synergy capture</p>\n</li>\n<li><p>Conflict Of Interest statements. </p>\n</li>\n<li><p>Data provenance, results, purpose, assupmtions</p>\n</li>\n<li><p>Whats important to encode and why?</p>\n</li>\n<li><p>Types of systems</p>\n</li>\n<li><p>Central Limit Theorem applications</p>\n</li>\n</ul>\n<p> Options When Cleaning Unclean Data</p>\n<ul>\n<li>Fill Drop Replace</li>\n<li>isnan, isfinite, defaultVal</li>\n<li>Col Info / Processing Outline</li>\n<li>GisHandler()</li>\n<li>readFile()</li>\n<li>mergeBounds()</li>\n<li>filterBounds()</li>\n</ul>\n <h3>Ideal Data:</h3>\n <ul>\n<li>Descriptive - a whole population</li>\n<li>Exploratory - a random sample with many variables</li>\n<li>Inferential - the right population, randomly sampled</li>\n<li>Predictive - a training and test data set from the same population</li>\n<li>Causal - data from a randomized study</li>\n<li>Mechanistic - data about all components of the system.</li>\n</ul>\n <h3>Critical Questions</h3>\n <ul>\n<li>Dimensionality</li>\n<li>Table Multi Indexies</li>\n<li>Grouping Identifiers</li>\n<li>Arima/Kriging</li>\n<li>Anova/Ancova</li>\n<li>Temporal/Physical bounds</li>\n<li>Analysis of Avg of AVg&#39;s</li>\n<li>Data provenance</li>\n<li>Features Geometry = Coordinates/ Rings</li>\n<li>Is this an act of classifying or categorizing?</li>\n</ul>\n<p> Classes are categorized ideas. </p>\n<ul>\n<li>Which and what to apply? Does it truly matter in the end?</li>\n</ul>\n<p> Yes because shannon encoding. The approximation of the truth is not the truth but can be sometimes useful.</p>\n <h2>2. Data Understanding</h2>\n <h3>Meta Data</h3>\n <p>We won&#39;t get too deep into this but: data is like an ogre in that hey both have layers. Data about data is called meta-data. Universal law: Any and all &#39;observable&#39; data has meta-data which then too is also &#39;observable&#39;. There are thought to be 6 meta-physical cascading layers of meta-data that expand onto themselves indefinetely. When humans perform their superficial observation about the world around them, at that &#39;instant&#39; all the data and its accompanying meta-data is &#39;metabolized&#39; by our sensory processors solely at the &#39;instance&#39; level of this Hierarchy. Illicitation of further understanding of the data may be derived by critically introspecting into the observation. The deepest people feel comfortable without getting too abstract is to the structural level (3), ie: &quot;The relations between the observable properties of the original observation&quot;. lower levels get into abstract concepts like observing the relations between words and language itself.</p>\n <ol start=\"0\">\n<li><strong>Rules</strong> - cuts across other layers. buried in code at all levels</li>\n</ol>\n<ul>\n<li><strong>0A.</strong> Reaction &amp; Transformation Rules -&gt; Derivation Rules -&gt; Facts / Quries -&gt; Integration Constraints</li>\n<li><strong>0B.</strong> Example: From the Piano axioms in math we may derive further rules. From these rules</li>\n</ul>\n<ol>\n<li><strong>Domain</strong> - express local in terms of global. the sphere of all things local app should know</li>\n<li><strong>Referent</strong> (understand linkages between data models -&gt; xsl/t, topic maps, data driven) languages, abstractions</li>\n<li><strong>Structural</strong> (relational, object, hierarchical)</li>\n<li><strong>Syntactic</strong> (type, language, msg length, source, bitrate, encryption level),</li>\n<li><strong>Instance</strong> (actual data)</li>\n</ol>\n <h3>Data Types</h3>\n <p><strong>Text Types</strong></p>\n<ul>\n<li>CHAR( ) A fixed section from 0 to 255 characters long. VARCHAR( ) A variable section from 0 to 255 characters long. TINYTEXT A string with a maximum length of 255 characters.</li>\n<li>TEXT A string with a maximum length of 65535 characters. </li>\n<li>BLOB A string with a maximum length of 65535 characters. </li>\n<li>MEDIUMTEXT A string with a maximum length of 16777215 characters. </li>\n<li>MEDIUMBLOB A string with a maximum length of 16777215 characters. </li>\n<li>LONGTEXT A string with a maximum length of 4294967295 characters. </li>\n<li>LONGBLOB A string with a maximum length of 4294967295 characters.</li>\n<li>ENUM(x,y,z,etc.)</li>\n<li>SET A logical field can be displayed as Yes/No, True/False, or On/Off</li>\n</ul>\n <p><strong>Number Types</strong></p>\n<ul>\n<li>TINYINT -128 to 127 normal 0 to 255 </li>\n<li>SMALLINT -32768 to 32767 normal 0 to 65535 </li>\n<li>MEDIUMINT -8388608 to 8388607 normal 16777215</li>\n<li>INT -2147483648 to 2147483647 normal 0 to 4294967295 </li>\n<li>BIGINT -9223372036854775808 to 9223372036854775807 normal 0 to 18446744073709551615 </li>\n<li>FLOAT DOUBLE - A large number with a floating decimal point. </li>\n<li>DECIMAL - A DOUBLE stored as a string, allowing for a fixed decimal point.</li>\n<li>Money</li>\n<li>Real</li>\n</ul>\n <p><strong>Date/Time Types</strong></p>\n<p> Prefered Data Format : yyyy-mm-dd&#39;T&#39;hh:mm:ss.mmm</p>\n<ul>\n<li>DATE: YYYY-MM-DD </li>\n<li>DATETIMEL YYYY-MM-DD HH:MM:SS </li>\n<li>TIMESTAMP: YYYYMMDDHHMMSS </li>\n<li>TIME: HH:MM:SS </li>\n<li>YEAR: YYYY</li>\n</ul>\n <p><strong>Other Types of Data :</strong></p>\n<ul>\n<li>Records</li>\n<li>Objects</li>\n<li>Graph/Network</li>\n<li>Text</li>\n<li>Multimedia</li>\n<li>Relational/ Transactional</li>\n<li><strong>PYTHON</strong> (Integer, Boolean, Float, Object)</li>\n<li><strong>GIS</strong>( Point, Line, Polygon )( Spatial Projections )</li>\n</ul>\n <h3>Data Attributes</h3>\n <p>Collection Methodology attributes</p>\n<ul>\n<li><strong>Census</strong> (descriptive),</li>\n<li><strong>Observational study</strong> (inferential),</li>\n<li><strong>convenience sample</strong> (all types-may be biased),</li>\n<li><strong>randomized trial</strong> (causal).</li>\n<li><strong>prediction study</strong> (prediction)</li>\n<li><strong>studies over time</strong> [cross sectional(inferential)</li>\n<li><strong>longitudinal</strong> (inferential, predictive)</li>\n<li><strong>retrospective</strong> ( inferential)</li>\n</ul>\n <p>Basics attributes</p>\n<ul>\n<li><strong>Categorical Qualitative</strong> (Binomial, Nominal, Ordinal)</li>\n<li><strong>Numerical Quantitative</strong> (Discrete or Continuous)(Interval/ Ratio)</li>\n<li><a href=\"https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_distance_sect003.htm\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Binary</strong></a>- 1 or 0</li>\n<li><strong>Nominal</strong> - (Hair color, Gender, Favorite Ice Cream ) (Frequencies, Proportions, Percentages)(Transform to Numerical using One Hot Encoding)(Display using Pie or Bar Chart)</li>\n<li><strong>Ordinal</strong> - Ordered without a known Magnitude</li>\n</ul>\n <p><strong>DESCRIPTIVE STATISTICS</strong></p>\n <p><strong>What are Basic Statistics?</strong>\n Descriptive Statistics, Correlations, t-tests, frequency tables, cross tabulation</p>\n<p> <strong>One basic and straightforward method for analyzing data is via crosstabulation. Log-Linear provides a more &quot;sophisticated&quot; way of looking at crosstabulation tables. Specifically, you can test the different factors that are used in the crosstabulation (e.g., gender, region, etc.) and their</strong> <a href=\"http://www.statsoft.com/textbook/statistics-glossary/i.aspx?button=i#Interactions\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>interactions</strong></a> <strong>for statistical significance\n  Fitting marginal frequencies.</strong> Let us now turn to the analysis of our example table. We could ask ourselves what the frequencies would look like if there were no relationship between variables (the null hypothesis). Without going into details, intuitively one could expect that the frequencies in each cell would proportionately reflect the marginal frequencies (Totals). For example, consider the following table:</p>\n <p><strong>Meta-Data</strong></p>\n<ul>\n<li>Population</li>\n<li>Statistical power</li>\n<li>Sample size</li>\n<li>Treating Missing data</li>\n</ul>\n <p><strong>Central Tendency</strong> </p>\n<ul>\n<li>Mean </li>\n<li>arithmetic </li>\n<li>geometric </li>\n<li>harmonic</li>\n<li>Median</li>\n<li>Mode</li>\n</ul>\n <p><strong>Dispersion</strong> </p>\n<ul>\n<li>Variance</li>\n<li>Standard deviation</li>\n<li>Percentile</li>\n<li>Range</li>\n<li>Interquartile range</li>\n</ul>\n <p>Advanced attributes</p>\n<ul>\n<li><strong>Dispersion</strong> - Median, Min, Max, Quartiles, Box Plots, Iqr, Normal distribution</li>\n<li><strong>Bias</strong> - Expanded further below</li>\n<li><strong>Variance</strong> - Standard deviation, Square root means</li>\n<li><strong>Symmetry</strong> - Values are equally weighted. Folding a histogram in half.</li>\n<li><strong>Skew</strong> - Mean is smaller/larger than the median.</li>\n<li><strong>Kurtosis</strong> - Fat/Thing Tails</li>\n<li><strong>Normality - z-score</strong></li>\n<li><strong>Linearity -</strong> Alternative (Splines)</li>\n<li><strong>Heteroscedasticity -</strong> variability of a variable is unequal across the range of values of a second variable it predicts.</li>\n<li><strong>Monotonicity -</strong> slope remains Unchanged in the + or - direction</li>\n</ul>\n <p>Analytical attributes</p>\n<p> Statistics<a href=\"https://en.wikipedia.org/wiki/Summary_statistics\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Summary_statistics</a></p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Sufficient_statistic\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://en.wikipedia.org/wiki/Sufficient_statistic</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Descriptive_statistics\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://en.wikipedia.org/wiki/Descriptive_statistics</a></li>\n<li><strong>Location</strong> - Common measures of location, or <a href=\"https://en.wikipedia.org/wiki/Central_tendency\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">central tendency</a>, are the <a href=\"https://en.wikipedia.org/wiki/Arithmetic_mean\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">arithmetic mean</a>, <a href=\"https://en.wikipedia.org/wiki/Median\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">median</a>, <a href=\"https://en.wikipedia.org/wiki/Mode_(statistics)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">mode</a>, and <a href=\"https://en.wikipedia.org/wiki/Interquartile_mean\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">interquartile mean</a>.<a href=\"https://en.wikipedia.org/wiki/Summary_statistics#cite_note-2\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">[2]</a><a href=\"https://en.wikipedia.org/wiki/Summary_statistics#cite_note-3\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">[3]</a></li>\n<li><strong>Spread</strong> - Common measures of <a href=\"https://en.wikipedia.org/wiki/Statistical_dispersion\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">statistical dispersion</a> are the <a href=\"https://en.wikipedia.org/wiki/Standard_deviation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">standard deviation</a>, <a href=\"https://en.wikipedia.org/wiki/Variance\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">variance</a>, <a href=\"https://en.wikipedia.org/wiki/Range_(statistics)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">range</a>, <a href=\"https://en.wikipedia.org/wiki/Interquartile_range\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">interquartile range</a>, <a href=\"https://en.wikipedia.org/wiki/Absolute_deviation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">absolute deviation</a>, <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_difference\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">mean absolute difference</a> and the <a href=\"https://en.wikipedia.org/wiki/Distance_standard_deviation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">distance standard deviation</a>. Measures that assess spread in comparison to the typical size of data values include the <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_variation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">coefficient of variation</a>. The <a href=\"https://en.wikipedia.org/wiki/Gini_coefficient\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Gini coefficient</a> was originally developed to measure income inequality and is equivalent to one of the <a href=\"https://en.wikipedia.org/wiki/L-moment\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">L-moments</a>. A simple summary of a dataset is sometimes given by quoting particular <a href=\"https://en.wikipedia.org/wiki/Order_statistics\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">order statistics</a> as approximations to selected <a href=\"https://en.wikipedia.org/wiki/Percentiles\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">percentiles</a> of a distribution.</li>\n<li><strong>Shape</strong> - Common measures of the shape of a distribution are <a href=\"https://en.wikipedia.org/wiki/Skewness\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">skewness</a> or <a href=\"https://en.wikipedia.org/wiki/Kurtosis\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">kurtosis</a>, while alternatives can be based on <a href=\"https://en.wikipedia.org/wiki/L-moment\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">L-moments</a>. A different measure is the <a href=\"https://en.wikipedia.org/wiki/Skewness#Distance_skewness\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">distance skewness</a>, for which a value of zero implies central symmetry.</li>\n<li><strong>Dependence</strong> - The common measure of dependence between paired random variables is the <a href=\"https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Pearson product-moment correlation coefficient</a>, while a common alternative summary statistic is <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Spearman&#39;s rank correlation coefficient</a>. A value of zero for the <a href=\"https://en.wikipedia.org/wiki/Distance_correlation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">distance correlation</a> implies independence.</li>\n</ul>\n <p><strong>Attribute set operators</strong></p>\n<ul>\n<li>Boolean an or not</li>\n<li>Location adjacent contains intersects distance With.touches.crosses.overlaps</li>\n<li>Analy distance-;inner/outer/ mean avg std/ first second order effecs, analy centrography,</li>\n<li>analyze global and local densities, quadrratic/kernal.</li>\n<li>semanticweb-w3cdataintegrity- domain ontology solve syntax/structure</li>\n<li>Innventory Systems show</li>\n<li>Query systems reveal</li>\n<li>Analysis systems explore</li>\n<li>Decisions systems support</li>\n<li>Modeling systems process</li>\n<li>Monitoring systems are time centric</li>\n<li>Gis analysis - img processing, classifier sufface analysis, visibility gradient, aspect, network slows</li>\n<li>Voronoi for knn/iwd</li>\n</ul>\n <p><a href=\"http://pysal.org/notebooks/explore/pointpats/distance_statistics\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Central Tendency</a></p>\n<ul>\n<li>mean_center: calculate the mean center of the unmarked point pattern.</li>\n<li>weighted_mean_center: calculate the weighted mean center of the marked point pattern.</li>\n<li>manhattan_median: calculate the manhattan median</li>\n<li>euclidean_median: calculate the Euclidean median Dispersion and Orientation</li>\n<li>std_distance: calculate the standard distance</li>\n</ul>\n <p>_ <strong>Descriptive statistics.</strong>\n _ _When one&#39;s data are not normally distributed, and the measurements at best contain rank order information, then computing the </p>\n<ul>\n<li>standard descriptive statistics (e.g., mean, standard deviation) is sometimes not the most informative way to summarize the data.</li>\n</ul>\n <p> Nonparametrics and Distributions will compute a wide variety of measures of location (_ <a href=\"http://www.statsoft.com/textbook/statistics-glossary/m.aspx?button=m#Mean\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>mean</em></a><em>,</em><a href=\"http://www.statsoft.com/textbook/statistics-glossary/m.aspx?button=m#Median\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>median</em></a><em>,</em><a href=\"http://www.statsoft.com/textbook/statistics-glossary/m.aspx?button=m#Mode\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>mode</em></a><em>, etc.) and dispersion (</em><a href=\"http://www.statsoft.com/textbook/statistics-glossary/v.aspx?button=v#variance\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>variance</em></a><em>, average deviation, quartile range, etc.) to provide the &quot;complete picture&quot; of one&#39;s data.</em></p>\n <p><strong>Dataset</strong></p>\n<ul>\n<li>Size -&gt; Number of variables</li>\n<li>Shape -&gt; Number of observations</li>\n<li>Dimension</li>\n<li>total missing, memory, avg size</li>\n<li>count of columns by type</li>\n<li>important info/ warnings</li>\n</ul>\n <p><strong>Univariate</strong></p>\n<p> setDtype</p>\n<ul>\n<li>getUnique/Count/Percent/Rate</li>\n<li>getMissing/Count/Percent/Rate</li>\n</ul>\n<h2> getDtype\n *   Categorical Qualitative (Binomial, Nominal, Ordinal)\n *   Numerical Quantitative (Discrete or Continuous)(Interval/ Ratio)\n *   Binary - 1 or 0\n *   Nominal - (Hair color, Gender, Favorite Ice Cream ) (Frequencies, Proportions, Percentages)(Transform to Numerical using One Hot *   Encoding)(Display using Pie or Bar Chart)\n *   Ordinal - Ordered without a known Magnitude</h2>\n<ul>\n<li>Boolean</li>\n<li>Numeric\n -- Skew \n -- kurtosis \n -- Normality Test\n -- Mean \n -- stdev \n -- median \n -- mode \n -- min \n -- max\n -- Visualization of (Sample) Data</li>\n</ul>\n<hr>\n<ul>\n<li>Dispersion - Median, Min, Max, Quartiles, Box Plots, Iqr, Normal distribution </li>\n<li>Bias - Expanded further below</li>\n<li>Variance - Standard deviation, Square root means</li>\n<li>Symmetry - Values are equally weighted. Folding a histogram in half.</li>\n<li>Skew - Mean is smaller/larger than the median.</li>\n<li>Kurtosis - Fat/Thing Tails</li>\n<li>Normality - z-score</li>\n<li>Linearity - Alternative (Splines)</li>\n<li>Heteroscedasticity - variability of a variable is unequal across the range of values of a second variable it predicts.</li>\n<li>Monotonicity - slope remains Unchanged in the + or - direction</li>\n<li>Big-Data, Structure(Semi/Un), Time-Stamped, Spatial, Spatio-Temporal, Ordered, Stream, Dimensionality, </li>\n<li>Primary Keys, Unique Values, Index, Spatial, Auto Increment, Default Values, Null Values</li>\n</ul>\n<ul>\n<li>noise </li>\n<li>Categorical</li>\n<li>Geospatial (country,countrycode, city, metro, etc...)</li>\n<li>String</li>\n</ul>\n<hr>\n<ul>\n<li>Location - Common measures of location, or central tendency, are the arithmetic mean, median, mode, and interquartile mean. </li>\n<li>Spread - Common measures of statistical dispersion are the standard deviation, variance, range, interquartile range, absolute deviation, mean absolute difference and the distance standard deviation. Measures that assess spread in comparison to the typical size of data values include the coefficient of variation. The Gini coefficient was originally developed to measure income inequality and is equivalent to one of the L-moments. A simple summary of a dataset is sometimes given by quoting particular order statistics as approximations to selected percentiles of a distribution.</li>\n<li>Shape - Common measures of the shape of a distribution are skewness or kurtosis, while alternatives can be based on L-moments. A different measure is the distance skewness, for which a value of zero implies central symmetry.</li>\n<li>Dependence - The common measure of dependence between paired random variables is the Pearson product-moment correlation coefficient, while a common alternative summary statistic is Spearman&#39;s rank correlation coefficient. A value of zero for the distance correlation implies independence. The Chi-Square test helps you determine if two discrete variables are associated</li>\n</ul>\n<hr>\n<ul>\n<li>Outlier detection:</li>\n<li><ul>\n<li>BoxPlot -&gt; IQR</li>\n</ul>\n</li>\n<li><ul>\n<li>Q3+1.5IQR</li>\n</ul>\n</li>\n<li><ul>\n<li>Count, Mean, Std, Min, 25%, 50%, 75%, Max</li>\n</ul>\n</li>\n<li><ul>\n<li>Z score -&gt; Z = (x-u)/o. U = Standard Deviation from x =Mean.</li>\n</ul>\n</li>\n<li><ul>\n<li>stats.zscore(df)</li>\n</ul>\n</li>\n</ul>\n<hr>\n<ul>\n<li>Date/Time Types\n (quarter, month, dayofweek, dayofmonth, hour, minute</li>\n</ul>\n<ul>\n<li>DATE \t\tYYYY-MM-DD \t\t \t\t \t\t\t \t\t\t</li>\n<li>DATETIME \tYYYY-MM-DD HH:MM:SS \t\t \t\t \t\t\t \t</li>\n<li>TIMESTAMP \tYYYYMMDDHHMMSS \t\t \t\t \t\t\t \t\t</li>\n<li>TIME \t\tHH:MM:SS \t\t \t\t \t\t\t \t\t\t</li>\n<li>YEAR \t\tYYYY</li>\n</ul>\n<hr>\n <h3><strong>Data Exploration</strong></h3>\n <p>Refere to <pre class='prettyprint' style='display:inline'>methods_tutorial.ipynb</pre> and <pre class='prettyprint' style='display:inline'>Viz Notes Examples.ipynb</pre> and <em>DistributionsAndTest.ipynb</em> for more </p>\n <p>Median difference from Quartiles represent skew. Whiskers represent variance.</p>\n <h2>4. Data Preparation</h2>\n <h3>Data Prep</h3>\n <p><a href=\"https://en.wikipedia.org/wiki/Data_preparation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Data prep</a></p>\n<blockquote>\n<p>Data preparation is the act of manipulating (or pre-processing) raw data (which may come from disparate data sources) into a form that can readily and accurately be analysed, e.g. for business purposes... Data preparation is the first step in data analytics projects and can include many discrete tasks such as loading data or data ingestion, data fusion, data cleaning, data augmentation, and data delivery... The issues to be dealt with fall into two main categories: systematic errors involving large numbers of data records, probably because they have come from different sources;\n individual errors affecting small numbers of data records, probably due to errors in the original data entry.</p>\n</blockquote>\n <p><a href=\"https://en.wikipedia.org/wiki/Data_pre-processing\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">pre-processing</a></p>\n<blockquote>\n<p>Data preprocessing has the objective to add missing values, aggregate information, label data with categories (Data binning) and smooth a trajectory</p>\n<p>Tasks of data pre-processing</p>\n</blockquote>\n<ul>\n<li>Data <a href=\"https://en.wikipedia.org/wiki/Data_cleansing#Data_quality\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">cleansing</a> -  replacing, modifying, or deleting incomplete, incorrect, inaccurate or irrelevant parts of the data</li>\n<li>Data <a href=\"https://en.wikipedia.org/wiki/Data_editing\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">editing</a> - For Quality Control</li>\n<li>Data <a href=\"https://en.wikipedia.org/wiki/Data_reduction\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">reduction</a></li>\n<li>Data <a href=\"https://en.wikipedia.org/wiki/Data_wrangling\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">wrangling</a> - Transforming the data format for better handling</li>\n</ul>\n <p>OUT OF ALL THESE LINKS PLEASE READ THE WIKI PAGE ON <a href=\"https://en.wikipedia.org/wiki/Data_cleansing#Data_Quality\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Data cleansing</a> and <a href=\"https://en.wikipedia.org/wiki/Data_quality\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Data Quality</a>. </p>\n<p> It covers materials for High-quality data needs to pass a set of quality criteria and the process thereinvolved</p>\n <p><a href=\"https://en.wikipedia.org/wiki/Data_processing\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Data_processing</a> </p>\n<blockquote>\n<p>Data processing may involve various processes, including:</p>\n</blockquote>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Data_validation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Validation</a> – Ensuring that supplied data is correct and relevant.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Sorting\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Sorting</a> – &quot;arranging items in some sequence and/or in different sets.&quot;</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Summary_statistic\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Summarization</a> – reducing detail data to its main points.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Aggregate_data\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Aggregation</a> – combining multiple pieces of data.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Statistical_analysis\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Analysis</a> – the &quot;collection, <a href=\"https://en.wikipedia.org/wiki/Organization\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">organization</a>, analysis, interpretation and presentation of data.&quot;</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Business_reporting\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Reporting</a> – list detail or summary data or computed information.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Data_classification_(business_intelligence)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Classification</a> – separation of data into various categories.</li>\n</ul>\n <p>Data processing system <a href=\"https://en.wikipedia.org/wiki/Data_processing_system\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Data_processing_system</a></p>\n<p> A Data processing system may involve some combination of:</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Data_conversion\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Conversion</a> converting data to another form or Language.</li>\n<li>Reporting – list detail or summary data or computed information.</li>\n</ul>\n <p>Data Processing Systems by service type</p>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Transaction_processing_system\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Transaction processing systems</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Information_retrieval\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Information storage and retrieval systems</a></li>\n<li>Command and control systems</li>\n<li>Computing service systems</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Control_system\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Process control systems</a></li>\n<li>Message switching systems</li>\n</ul>\n <h3>Validation</h3>\n <p>Validation Types <a href=\"https://en.wikipedia.org/wiki/Data_validation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Data_validation</a></p>\n<ul>\n<li>Range and constraint validation;</li>\n<li>Code and Cross-reference validation;</li>\n<li>Structured validation</li>\n</ul>\n <p>Validation methods</p>\n<ul>\n<li>Allowed character checks</li>\n<li>Batch totals</li>\n<li>Cardinality check</li>\n<li>Check digits</li>\n<li>Consistency checks</li>\n<li>Control totals</li>\n<li>Cross-system consistency checks</li>\n<li>Data type checks</li>\n<li>File existence check</li>\n<li>Format or picture check</li>\n<li>Hash totals</li>\n<li>Limit check</li>\n<li>Logic check</li>\n<li>Presence check</li>\n<li>Range check</li>\n<li>Referential integrity</li>\n<li>Spelling and grammar check</li>\n<li>Uniqueness check</li>\n<li>Table lookup check</li>\n</ul>\n <p>Post-Validation Actions</p>\n<ul>\n<li><strong>Enforcement Action</strong></li>\n<li><strong>Advisory Action</strong></li>\n<li><strong>Verification Action</strong></li>\n<li><strong>Log of validation</strong></li>\n</ul>\n <h3>Data <a href=\"https://en.wikipedia.org/wiki/Data_transformation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Transformation</a></h3>\n  <p>May include: Projecting data, Transforming Multiple Classes to Binary ones, Calibrating Class Probabilities, Cleaning the data and even sampling it. Typically it follows the following path:</p>\n <blockquote>\n<ul>\n<li><strong>Data discovery</strong> is the first step in the data transformation process. Typically the data is profiled using profiling tools or sometimes using manually written profiling scripts to better understand the structure and characteristics of the data and decide how it needs to be transformed.</li>\n</ul>\n</blockquote>\n<ul>\n<li><strong>Data mapping</strong> is the process of defining how individual fields are mapped, modified, joined, filtered, aggregated etc. to produce the final desired output. Developers or technical data analysts traditionally perform data mapping since they work in the specific technologies to define the transformation rules (e.g. visual ETL tools,<a href=\"https://en.wikipedia.org/wiki/Data_transformation#cite_note-3\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">[3]</a> transformation languages).</li>\n<li><strong>Code generation</strong> is the process of generating executable code (e.g. SQL, Python, R, or other executable instructions) that will transform the data based on the desired and defined data mapping rules.<a href=\"https://en.wikipedia.org/wiki/Data_transformation#cite_note-4\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">[4]</a> Typically, the data transformation technologies generate this code<a href=\"https://en.wikipedia.org/wiki/Data_transformation#cite_note-5\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">[5]</a> based on the definitions or metadata defined by the developers.</li>\n<li><strong>Code execution</strong> is the step whereby the generated code is executed against the data to create the desired output. The executed code may be tightly integrated into the transformation tool, or it may require separate steps by the developer to manually execute the generated code.</li>\n<li><strong>Data review</strong> is the final step in the process, which focuses on ensuring the output data meets the transformation requirements. It is typically the business user or final end-user of the data that performs this step. Any anomalies or errors in the data that are found and communicated back to the developer or data analyst as new requirements to be implemented in the transformation process.<a href=\"https://en.wikipedia.org/wiki/Data_transformation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">1</a></li>\n</ul>\n <h2>5. Modeling</h2>\n <h3>Statistical Challenges With Data</h3>\n <ul>\n<li>Options tree to show Pessimistic, Nominal Optimistic versions</li>\n<li>Performance vs Risk vs Design analysis</li>\n</ul>\n<p> <strong>Changing variance</strong> - what can you do</p>\n<ul>\n<li>box cox transform</li>\n<li>variance stabilizing transform</li>\n<li>weighted least squares</li>\n<li>huber white standard errors</li>\n</ul>\n<p> <strong>ANOVA Requirements</strong></p>\n<ul>\n<li>Normal Distribution</li>\n<li>Independent Samples/Groups</li>\n<li>Independent Samples <em>t</em> Test requires the assumption of <em>homogeneity of variance.</em> </li>\n<li>a test for the homogeneity of variance, called <strong>Levene&#39;s Test</strong> , whenever you run an independent samples T test</li>\n</ul>\n<p> <strong>MISC</strong></p>\n<ul>\n<li>P Values require knowing how many records exist are in the database.</li>\n<li>Outliers profoundly influence on the slope of the regression line and the correlation coefficient.</li>\n<li>Correlation coefficient alone is not enough for decision making (i.e., scatterplots are always recommended)</li>\n<li>Prefered Data Format : yyyy-mm-dd&#39;T&#39;hh:mm:ss.mmm</li>\n</ul>\n<p> <strong>Bias:</strong></p>\n<ul>\n<li>Quantitative Approach to Outliers.</li>\n<li>Correlations in Non-homogeneous Groups</li>\n<li>Nonlinear Relations between Variables. - Pearson R measures linearity</li>\n<li>Exploratory Examination of Correlation Matrices</li>\n<li>Casewise vs. Pairwise Deletion of Missing Data</li>\n<li>Overfitting: Pruning/ Cross Validation</li>\n<li>Breakdown Analysis</li>\n<li>Frequency Tables</li>\n<li>Cross Tabulation</li>\n<li>Marginal Frequencies</li>\n<li>Association Rules</li>\n</ul>\n <h3><strong>ML and ADE</strong></h3>\n<p> ML is a helpful tool that can help us with our data exploration and more and more, tools can even be used to make predictions! </p>\n<p> Services usually satisfy one or more of these steps:</p>\n<ul>\n<li>processing</li>\n<li><ul>\n<li>Partitions</li>\n</ul>\n</li>\n<li><ul>\n<li>Parallelizations</li>\n</ul>\n</li>\n<li><ul>\n<li>Map Reduce</li>\n</ul>\n</li>\n<li>visualizing</li>\n<li>exploring</li>\n<li>feature engineering</li>\n<li>model validation</li>\n<li>predictive modeling</li>\n<li><ul>\n<li>Train, Test, Appply</li>\n</ul>\n</li>\n<li>Feature eng</li>\n<li>architecture search/transfer learning </li>\n<li>parameter tunin g </li>\n<li>model selection </li>\n<li>model ensambling </li>\n<li>model distillation</li>\n</ul>\n <h2>6. Evaluation</h2>\n\n  <script src=\"https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js\"></script>\n  <link rel=\"stylesheet\" href=\"https://cdn.rawgit.com/google/code-prettify/master/styles/desert.css\"/>\n  "}