{"meta":{"title":"Machine Learning","summary":"Assorted notes on the topic. Near entirly quotes from other sources.","tab":"Machine Learning","keywords":"['data']","hide_sitemap":"false","hide_toc":"true","hide_breadcrumbs":"false","filename":"ml"},"content":"<p><a href=\"https://www.itl.nist.gov/div898/handbook/\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://www.itl.nist.gov/div898/handbook/</a></p>\n <p><strong>Non-standard predictors</strong> </p>\n<ul>\n<li>Nonlinear <a href=\"https://en.wikipedia.org/wiki/Nonlinear_regression\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">regression</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Nonparametric_regression\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Nonparametric</a></li>\n</ul>\n <p><a href=\"https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Classification</strong></a> Identifying to which category an object belongs to. \n <strong>Applications</strong> : Spam detection, Image recognition. </p>\n<p> <strong>Algorithms</strong> : </p>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/svm.html#svm-classification\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">SVM</a>- <a href=\"https://scikit-learn.org/stable/modules/neighbors.html#classification\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">nearest neighbors</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/ensemble.html#forest\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">random forest</a></li>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/index.html#general-examples\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Examples</a></li>\n</ul>\n<p> <strong>Classification</strong>:</p>\n<ul>\n<li><p>k-nearest neighbors - supervised</p>\n</li>\n<li><p>decision trees (c4.5) - supervised - noncontiguous, data is if/else</p>\n</li>\n<li><p>gradient boosted decision trees</p>\n</li>\n<li><p>Random Forest - Super/Unsuper - Best Split</p>\n</li>\n<li><p>classification and regresstion tree(cart)</p>\n</li>\n<li><p>SVM - Super/Unsuper - Maximum Margin</p>\n</li>\n<li><p>naive bayes - supervised</p>\n</li>\n</ul>\n <p><a href=\"https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Regression</strong></a> Predicting a continuous-valued attribute associated with an object.</p>\n<ol>\n<li><p><strong>Applications</strong> : Drug response, Stock prices. </p>\n</li>\n<li><p><strong>Algorithms</strong> :</p>\n</li>\n</ol>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/svm.html#svm-regression\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">SVR</a>, - <a href=\"https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">ridge regression</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/linear_model.html#lasso\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Lasso</a></li>\n<li>Simple linear regression</li>\n<li>OLS</li>\n<li>GLM</li>\n<li>Bayesian regression </li>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/index.html#general-examples\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Examples</a></li>\n</ul>\n <p><a href=\"https://scikit-learn.org/stable/modules/clustering.html#clustering\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Clustering</a> </p>\n<p> Automatic grouping of similar objects into sets.</p>\n<ol>\n<li><p><strong>Applications</strong> : Customer segmentation, Grouping experiment outcomes </p>\n</li>\n<li><p><strong>Algorithms</strong> :</p>\n</li>\n</ol>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/clustering.html#k-means\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">k-Means</a>- <a href=\"https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">spectral clustering</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/clustering.html#mean-shift\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">mean-shift</a></li>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/index.html#cluster-examples\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Examples</a></li>\n</ul>\n <p><a href=\"https://scikit-learn.org/stable/modules/decomposition.html#decompositions\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Dimensionality reduction</strong></a> Reducing the number of random variables to consider.</p>\n<ol>\n<li><strong>Applications</strong> :</li>\n</ol>\n<ul>\n<li>Visualization</li>\n<li>Increased efficiency</li>\n</ul>\n<ol start=\"2\">\n<li><strong>Algorithms</strong> :</li>\n</ol>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/decomposition.html#pca\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">PCA</a>, <a href=\"https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">feature selection</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/decomposition.html#nmf\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">non-negative matrix factorization</a></li>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/index.html#decomposition-examples\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Examples</a></li>\n</ul>\n <p><a href=\"https://scikit-learn.org/stable/model_selection.html#model-selection\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Model selection</strong></a> Comparing, validating and choosing parameters and models.</p>\n<ol>\n<li><p><strong>Goal</strong> : Improved accuracy via parameter tuning </p>\n</li>\n<li><p><strong>Modules</strong> :</p>\n</li>\n</ol>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/grid_search.html#grid-search\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">grid search</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">cross validation</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">metrics</a></li>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/index.html#general-examples\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Examples</a></li>\n</ul>\n <p><a href=\"https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Preprocessing</strong></a> Feature extraction and normalization.</p>\n <p><a href=\"https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Preprocessing</strong></a> Feature extraction and normalization. </p>\n<ol>\n<li><strong>Application</strong> : Transforming input data such as text for use with machine learning algorithms. </li>\n<li><strong>Modules</strong> :</li>\n</ol>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">preprocessing</a></li>\n<li><a href=\"https://scikit-learn.org/stable/modules/feature_extraction.html#feature-extraction\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">feature extraction</a></li>\n<li><a href=\"https://scikit-learn.org/stable/auto_examples/index.html#general-examples\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Examples</a></li>\n</ul>\n <h1>ML</h1>\n <p><strong>CLASSIFICATION AND REGRESSION PROBLEMS</strong></p>\n<p> There are numerous algorithms for predicting continuous variables or categorical variables from a set of continuous predictors and/or categorical factor effects. For example, in <a href=\"http://www.statsoft.com/textbook/statistics-glossary/g.aspx?button=g#general%20Linear%20Model\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>GLM (General Linear Models)</em></a> and <a href=\"http://www.statsoft.com/textbook/general-regression-models/\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>GRM (General Regression Models)</em></a>, we can specify a linear combination (design) of continuous predictors and categorical factor effects (e.g., with two-way and three-way interaction effects) to predict a continuous dependent variable. In <em>GDA (General Discriminant Function Analysis)</em>, we can specify such designs for predicting categorical variables, i.e., to solve classification problems.</p>\n<p> <strong>Regression-type problems.</strong> Regression-type problems are generally those where we attempt to predict the values of a continuous variable from one or more continuous and/or <a href=\"http://www.statsoft.com/textbook/statistics-glossary/c.aspx?button=c#Categorical%20Predictor\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">categorical predictor variables</a>.</p>\n<p> <strong>Classification-type problems.</strong> Classification-type problems are generally those where we attempt to predict values of a categorical <a href=\"http://www.statsoft.com/textbook/statistics-glossary/i.aspx?button=i#Independent%20vs.%20Dependent%20Variables\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">dependent</a> variable (class, group membership, etc.) from one or more continuous and/or <a href=\"http://www.statsoft.com/textbook/statistics-glossary/c.aspx?button=c#Categorical%20Predictor\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">categorical predictor variables</a>.\n  There are a number of methods for analyzing classification-type problems and to compute predicted classifications, either from simple continuous predictors (e.g., binomial or multinomial logit regression in <a href=\"http://www.statsoft.com/textbook/statistics-glossary/g.aspx?button=g#generalized%20Linear%20Model\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>GLZ</em></a>), from categorical predictors (e.g., <a href=\"http://www.statsoft.com/textbook/statistics-glossary/l.aspx?button=l#Log-Linear%20Analysis\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>Log-Linear analysis</em></a> of multi-way frequency tables), or both (e.g., via ANCOVA-like designs in <em>GLZ</em> or <em>GDA</em>).</p>\n<p> <strong>Tree methods are nonparametric and nonlinear + Simplicity of results.\n  -Specify Criteria for Predictive Accuracy, Selecting Splits, When to Stop Splitting.</strong></p>\n<p> <strong>Data Classification</strong> -&amp;gt; Effectiveness <a href=\"https://en.wikipedia.org/wiki/Data_classification_(business_intelligence)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Data_classification</a></p>\n <p><strong>Learning Models :</strong>\n  We can think of a model as a template. When data is processed through a learning model, what will come out the other end is insight.\n  The model is nothing more than a set of operations performed on the data.\n  Models are typically made in a static environment by (drilling/ rolling, pivoting, slicing/ dicing, Etc.) through data and may involve Integrating multiple mining functions (ex. Classifying than Clustering).</p>\n <p><a href=\"https://en.wikipedia.org/wiki/Data_classification_(business_intelligence)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Data classification</a></p>\n<p> According to Golfarelli and Rizzi, these are the measures of effectiveness of the classifier:</p>\n<ul>\n<li>_ <strong>Predictive accuracy</strong> _ : How well does it predict the categories for new observations?</li>\n<li>_ <strong>Speed</strong> _ : What is the computational cost of using the classifier?</li>\n<li>_ <strong>Robustness</strong> _ : How well do the models created perform if <a href=\"https://en.wikipedia.org/wiki/Data_quality\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>data quality</strong></a> is low?</li>\n<li>_ <strong>Scalability</strong> _ : Does the classifier function efficiently with large amounts of data?</li>\n<li>_ <strong>Interpretability</strong> _ : Are the results understandable to users?</li>\n</ul>\n <p><strong>Types of Models</strong></p>\n<p> Hierarchical Generative</p>\n<ul>\n<li>Gaussian Mixture Models</li>\n<li>Hidden Markov Models</li>\n<li>Naive Bayes</li>\n<li>GANS</li>\n</ul>\n<p> Discriminative</p>\n<ul>\n<li>NN</li>\n<li>SVM</li>\n<li>Logistic Regression</li>\n</ul>\n<p> Descriptive</p>\n<ul>\n<li>Derived from the Attributes of Data (mean, median, mode, avg)</li>\n</ul>\n <p><strong>MLE</strong> is the workhorse estimation technique of frequentist statistics\n <strong>latent variables</strong> :</p>\n<ul>\n<li><p>expectation maximization</p>\n</li>\n<li><p>methods of moments</p>\n</li>\n<li><p>signal separation</p>\n</li>\n</ul>\n<p> -- principal component analysis</p>\n<p> -- singular value decomposition</p>\n <p>properties of estimators (bias, consistency, efficiency, sufficiency, robustness).</p>\n<p> Testing: Type I and II errors, power, likelihood ratios</p>\n<p>  Methodology of probabilistic process models: </p>\n<ul>\n<li>Dirichlet</li>\n<li>Gaussian</li>\n<li>basis/kernel expansion</li>\n<li>splines, wavelets</li>\n<li>support vector machines </li>\n<li>other local regression models</li>\n</ul>\n <p><strong>latent variables</strong>:</p>\n<ul>\n<li>expectation maximization</li>\n<li>methods of moments</li>\n<li>signal separation</li>\n<li>principal component analysis</li>\n<li>singular value decomposition</li>\n</ul>\n <p><strong>Feature Reduction:</strong></p>\n<ul>\n<li><p>PCA - describe greatest features</p>\n</li>\n<li><p>Cross Correlation Analysis</p>\n</li>\n<li><p>Linear Discrimination, Combines Features</p>\n</li>\n</ul>\n <p><strong>concepts</strong>:</p>\n<p> accuracy (tp+tn/p+n)</p>\n<p> precision: tp/(tp+fp)</p>\n<p> specificity: tn/(fp+tn)</p>\n<p> sensitivity: tp/(tp+fn)</p>\n <p><strong>Regression</strong>:</p>\n<ul>\n<li><p>linear regressions - numeric</p>\n</li>\n<li><p>logistic regressions - categorical</p>\n</li>\n</ul>\n <p><strong>ensemble learning</strong> - bagging boosting stacking additive regression</p>\n <p><strong>Neural Nets</strong>: (super/unsuper)</p>\n<ul>\n<li><p>autoencoders</p>\n</li>\n<li><p>deep beliefe nets</p>\n</li>\n<li><p>hebbian learning</p>\n</li>\n<li><p>gans</p>\n</li>\n<li><p>implicit density model</p>\n</li>\n<li><p>som</p>\n</li>\n</ul>\n <p><strong>Clustering</strong>:</p>\n<ul>\n<li><p>Hierarchal</p>\n</li>\n<li><p>Kmeans (Euclidean, Mikowski, Manhattan, #Clusters) </p>\n</li>\n<li><p>super/unsuper - sort by centroid</p>\n</li>\n<li><p>anomoly-detection - outliers, super/unsuper</p>\n</li>\n</ul>\n <p><strong>RNN</strong> </p>\n<ul>\n<li>LSTM</li>\n<li>Hierarchal</li>\n<li>Stochastic</li>\n</ul>\n <p><strong>FeedForward</strong> :</p>\n<ul>\n<li>MIP</li>\n<li>Autoencoder</li>\n<li>Probablistic</li>\n<li>Convolusional</li>\n<li>Time Delay</li>\n</ul>\n <p><strong>Online Learning</strong> :</p>\n<ul>\n<li>Data Efficient and Adaptable</li>\n<li>No Data storage needed</li>\n<li>Stochastic Gradient Descent</li>\n<li>No data storage needed</li>\n</ul>\n <p><strong>T-Distribution</strong> - Visualize high density</p>\n <p> <strong>RISK</strong> :Redundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g.,<a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">linear regression</a>,<a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">logistic regression</a>, and<a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">distance based methods</a>) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of<a href=\"https://en.wikipedia.org/wiki/Regularization_(mathematics)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">regularization</a>.\n  Presence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g.,<a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">linear regression</a>,<a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">logistic regression</a>,<a href=\"https://en.wikipedia.org/wiki/Support_Vector_Machines\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Support Vector Machines</a>,<a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">naive Bayes</a>) and distance functions (e.g.,<a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">nearest neighbor methods</a>,<a href=\"https://en.wikipedia.org/wiki/Support_Vector_Machines\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">support vector machines with Gaussian kernels</a>) generally perform well. However, if there are complex interactions among features, then algorithms such as<a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">decision trees</a> and<a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">neural networks</a> work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.</p>\n<p> <strong>concepts</strong> :</p>\n<p> accuracy (tp+tn/p+n)</p>\n<p> precision: tp/(tp+fp)</p>\n<p> specificity: tn/(fp+tn)</p>\n<p> sensitivity: tp/(tp+fn)\n <strong>properties of estimators</strong> (bias, consistency, efficiency, sufficiency, robustness).</p>\n<p> <strong>Testing</strong> : Type I and II errors, power, likelihood ratios</p>\n<p> <strong>Critical Review</strong>\n  Configuration and Risk\n  Rational for Complexity\n  Weighted parameters\n  Weighted performance metrics\n  Risk assessments and mitigations\n  Technologies\n  Roadmaps</p>\n <p>Common Error Measures:</p>\n<ul>\n<li>(root) Mean Squared Error - continuous data, sensitivity to outliers</li>\n<li>median absolute deviation - continuous data, often more robust</li>\n<li>sensitivity (Recall) - if you want few missed positives</li>\n<li>specificity - if you want few negatives called positives</li>\n<li>accuracy - weights false positives / negatives equally</li>\n<li>Concordance - one example is kappa</li>\n</ul>\n<p>  Key issues: accuracy, overfitting, interpretability, computational speed.\n  Pay attention to - confounding variables, complicated interactions, skewness, outliers, nonlinear patterns, variance changes, units/scale issues, overloading, regression, correlation and causation\n  Confounder: a variable that is correlated with both the outcome and covariates</p>\n<ul>\n<li>confounders can change the regression line.</li>\n<li>detected with exploration</li>\n</ul>\n<p> <strong>Hierarchical</strong> - Distance or similarity? - continuous(euclidean/correlation), binary - manhattan\n <strong>Graphs</strong> - help understand properties, find patterns, suggest future modelings, debug, and communicate.\n <strong>Bagging and Boosting</strong> - Combine classifiers to improve accuracy but make harder to interpret. Predictive</p>\n"}