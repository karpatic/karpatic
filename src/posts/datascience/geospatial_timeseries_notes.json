{"meta":{"filename":"geospatial_timeseries_notes"},"content":"<p>You first determine if a parameter is best expressed with a polynomial or linear model.\n Concatenating models create a single multivariate equation.</p>\n<ul>\n<li><p>Polynomial equations are special Multivariates *</p></li>\n</ul>\n <p><a href=\"https://automating-gis-processes.github.io/2018/notebooks/L4/reclassify.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://automating-gis-processes.github.io/2018/notebooks/L4/reclassify.html</a></p>\n <p><a href=\"https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/</a>\n stationary series, random walks , Rho Coefficient, Dickey Fuller Test of Stationarity</p>\n<ol>\n<li><p>The mean of the series should not be a function of time rather should be a constant.</p></li>\n<li><p>The variance of the series should not a be a function of time. </p></li>\n<li><p>The covariance of the i th term and the (i + m) th term should not be a function of time.</p></li>\n</ol>\n<p> Now, we will vary the value of Rho to see if we can make the series stationary.\n Introduced coefficient : Rho\n X(t) = Rho * X(t-1) + Er(t)\n Auto-Regressive Time Series Model\n Exploring data becomes most important in a time series model – without this exploration, you will not know whether a series is stationary or not.</p>\n<p> The current GDP of a country say x(t) is dependent on the last year’s GDP i.e. x(t – 1).\n Hence, we can formally write the equation of GDP as:\n x(t) = alpha *  x(t – 1) + error (t)\n The alpha is a coefficient which we seek so as to minimize the error function.\n Moving Average Time Series Model\n This equation is known as AR(1) formulation. The numeral one (1) denotes that the next instance is solely dependent on the previous instance.  The alpha is a coefficient which we seek so as to minimize the error function.\n x(t) = beta *  error(t-1) + error (t)\n In MA model, noise / shock quickly vanishes with time. The AR model has a much lasting effect of the shock.</p>\n<p> There are three commonly used technique to make a time series stationary:</p>\n<ol>\n<li>Detrending : Here, we simply remove the trend component from the time series. For instance, the equation of my time series is:\n x(t) = (mean + trend * t) + error\n We’ll simply remove the part in the parentheses and build model for the rest.</li>\n<li>Differencing : This is the commonly used technique to remove non-stationarity. Here we try to model the differences of the terms and not the actual term. For instance,\n x(t) – x(t-1) = ARMA (p ,  q)\n This differencing is called as the Integration part in AR(I)MA. Now, we have three parameters… p : AR, d : I, q : MA</li>\n<li><p>Seasonality : Seasonality can easily be incorporated in the ARIMA model directly. More on this has been discussed in the applications part below.</p></li>\n</ol>\nundefined\n <p><a href=\"https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8</a></p>\n<p> <a href=\"https://towardsdatascience.com/detecting-stationarity-in-time-series-data-d29e0a21e638\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/detecting-stationarity-in-time-series-data-d29e0a21e638</a>\n <a href=\"https://towardsdatascience.com/how-to-predict-a-time-series-part-1-6d7eb182b540\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/how-to-predict-a-time-series-part-1-6d7eb182b540</a>\n classical forecasting methodology (arima, exponential smoothing state space models , moving average etc…)</p>\n<p> <a href=\"https://nbviewer.jupyter.org/github/pmaji/data-science-toolkit/blob/master/time-series/forecasting_with_prophet.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://nbviewer.jupyter.org/github/pmaji/data-science-toolkit/blob/master/time-series/forecasting_with_prophet.ipynb</a>\n <a href=\"https://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/</a></p>\n<p> Time Series: \n <a href=\"https://towardsdatascience.com/time-series-forecasting-arima-models-7f221e9eee06\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/time-series-forecasting-arima-models-7f221e9eee06</a>\n <a href=\"https://towardsdatascience.com/time-series-forecasting-with-prophet-54f2ac5e722e\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/time-series-forecasting-with-prophet-54f2ac5e722e</a>\n <a href=\"https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b</a>\n <a href=\"https://towardsdatascience.com/forecasting-exchange-rates-using-arima-in-python-f032f313fc56\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/forecasting-exchange-rates-using-arima-in-python-f032f313fc56</a>\n <a href=\"https://towardsdatascience.com/forecasting-with-prophet-d50bbfe95f91\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/forecasting-with-prophet-d50bbfe95f91</a>\n <a href=\"https://towardsdatascience.com/exploring-the-sp500-with-r-part-2-asset-analysis-657d3c1caf60\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/exploring-the-sp500-with-r-part-2-asset-analysis-657d3c1caf60</a>\n <a href=\"https://towardsdatascience.com/financial-analytics-exploratory-data-analysis-of-stock-data-d98cbadf98b9\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/financial-analytics-exploratory-data-analysis-of-stock-data-d98cbadf98b9</a>\n <a href=\"https://nbviewer.jupyter.org/github/changhiskhan/talks/blob/master/pydata2012/pandas_timeseries.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://nbviewer.jupyter.org/github/changhiskhan/talks/blob/master/pydata2012/pandas_timeseries.ipynb</a>\n <a href=\"https://towardsdatascience.com/time-series-machine-learning-regression-framework-9ea33929009a\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/time-series-machine-learning-regression-framework-9ea33929009a</a></p>\n<p> <a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb#scrollTo=LK8dQnn2_U7\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb#scrollTo=LK8dQnn2_U7</a>_</p>\n<p> <a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb</a></p>\n<p> <a href=\"https://colab.research.google.com/notebooks/widgets.ipynb#scrollTo=QKk_E6-QRVPW\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://colab.research.google.com/notebooks/widgets.ipynb#scrollTo=QKk_E6-QRVPW</a></p>\n <p> <a href=\"https://towardsdatascience.com/detecting-stationarity-in-time-series-data-d29e0a21e638\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/detecting-stationarity-in-time-series-data-d29e0a21e638</a></p>\n<p> <a href=\"https://towardsdatascience.com/how-to-predict-a-time-series-part-1-6d7eb182b540\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/how-to-predict-a-time-series-part-1-6d7eb182b540</a></p>\n<p> classical forecasting methodology (arima, exponential smoothing state space models , moving average etc…)</p>\n<p> <a href=\"https://nbviewer.jupyter.org/github/pmaji/data-science-toolkit/blob/master/time-series/forecasting_with_prophet.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://nbviewer.jupyter.org/github/pmaji/data-science-toolkit/blob/master/time-series/forecasting_with_prophet.ipynb</a></p>\n<p> <a href=\"https://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/</a></p>\n<p> Time Series: \n <a href=\"https://towardsdatascience.com/time-series-forecasting-arima-models-7f221e9eee06\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/time-series-forecasting-arima-models-7f221e9eee06</a></p>\n<p> <a href=\"https://towardsdatascience.com/time-series-forecasting-with-prophet-54f2ac5e722e\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/time-series-forecasting-with-prophet-54f2ac5e722e</a></p>\n<p> <a href=\"https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b</a></p>\n<p> <a href=\"https://towardsdatascience.com/forecasting-exchange-rates-using-arima-in-python-f032f313fc56\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/forecasting-exchange-rates-using-arima-in-python-f032f313fc56</a></p>\n<p> <a href=\"https://towardsdatascience.com/forecasting-with-prophet-d50bbfe95f91\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/forecasting-with-prophet-d50bbfe95f91</a></p>\n<p> <a href=\"https://towardsdatascience.com/exploring-the-sp500-with-r-part-2-asset-analysis-657d3c1caf60\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/exploring-the-sp500-with-r-part-2-asset-analysis-657d3c1caf60</a></p>\n<p> <a href=\"https://towardsdatascience.com/financial-analytics-exploratory-data-analysis-of-stock-data-d98cbadf98b9\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/financial-analytics-exploratory-data-analysis-of-stock-data-d98cbadf98b9</a></p>\n<p> <a href=\"https://nbviewer.jupyter.org/github/changhiskhan/talks/blob/master/pydata2012/pandas_timeseries.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://nbviewer.jupyter.org/github/changhiskhan/talks/blob/master/pydata2012/pandas_timeseries.ipynb</a></p>\n<p> <a href=\"https://towardsdatascience.com/time-series-machine-learning-regression-framework-9ea33929009a\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/time-series-machine-learning-regression-framework-9ea33929009a</a></p>\n<p> <a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb#scrollTo=LK8dQnn2_U7\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb#scrollTo=LK8dQnn2_U7</a>_</p>\n<p> <a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb</a></p>\n<p> <a href=\"https://colab.research.google.com/notebooks/widgets.ipynb#scrollTo=QKk_E6-QRVPW\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://colab.research.google.com/notebooks/widgets.ipynb#scrollTo=QKk_E6-QRVPW</a></p>\n  <p>Applied Spatial Statistics</p>\n<ul>\n<li><p>Prior Posterior Distribution</p></li>\n<li><p>Hierarchal Models</p></li>\n<li><p>Markov Chain Monte Carlo</p></li>\n<li><p>Kernal Methods</p></li>\n<li><p>Dynamic State Space Modeling</p></li>\n<li><p>Multiple linear Regressions</p></li>\n<li><p>Spatial Models (Car Sar) Kriging| Time series models: ARM ARMA ,Dynamic linear models</p></li>\n<li><p>multi level models - causal inference - meta analysis</p></li>\n<li><p>multi agent decision making</p></li>\n<li><p>variable transformations</p></li>\n<li><p>eigenvalues</p></li>\n</ul>\n  <p>Internally,PostGIS stores geometries in a binary specification, but it is queried and viewed outside as a hex-encoded string. There are two popular variations of well-known binary (WKB):</p>\n<p> <a href=\"http://andrewgaidus.com/Build_Query_Spatial_Database/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">http://andrewgaidus.com/Build_Query_Spatial_Database/</a>\n <a href=\"https://gis.stackexchange.com/questions/89323/postgis-parse-geometry-wkb-with-ogr\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://gis.stackexchange.com/questions/89323/postgis-parse-geometry-wkb-with-ogr</a>\n Well-known binary (WKB):</p>\n<ul>\n<li><p>ST_AsGeoJSON returns geometry object</p></li>\n<li><p>ST_GeomFromWKB(bytea) returns geometry</p></li>\n<li><p>ST_AsBinary(geometry) returns bytea</p></li>\n<li><p>ST_AsEWKB(geometry) returns bytea</p></li>\n</ul>\n<p> BEST: </p>\n<ul>\n<li><p>ST_AsText(ST_Transform(the_geom,4326))</p></li>\n</ul>\n<p> Pandas offers the ability to iterate over the database by specifying the chunksize keyword argument, where chunksize is the number of rows to include in each chunk</p>\n<p> df = pd.DataFrame()</p>\n<p> for chunk in pd.read_sql(&#39;select * from mdprop_2017v2&#39;, con=conn, chunksize=5000):</p>\n<ul>\n<li><p>df = df.append(chunk)</p></li>\n</ul>\n<p> from shapely.geometry import Point\n gdf = gpd.GeoDataFrame(df, crs={&#39;init&#39; :&#39;epsg:4326&#39;}, geometry=Point( df[&#39;the_geom&#39;]) )</p>\n<p> <a href=\"https://gis.stackexchange.com/questions/267801/csv-to-geodataframe-how-to-have-valid-geometry-objects?rq=1\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://gis.stackexchange.com/questions/267801/csv-to-geodataframe-how-to-have-valid-geometry-objects?rq=1</a></p>\n<p> <a href=\"https://github.com/geopandas/geopandas/blob/master/geopandas/io/sql.py\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://github.com/geopandas/geopandas/blob/master/geopandas/io/sql.py</a></p>\n<p> <a href=\"http://geopandas.org/reference.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">http://geopandas.org/reference.html</a></p>\n<p> 2248 is What we use internally at BNIA #<a href=\"http://www.spatialreference.org/ref/epsg/2248/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">http://www.spatialreference.org/ref/epsg/2248/</a></p>\n<p> gdf = gdf.to_crs(epsg=4326)</p>\n<p> ST_AsText(ST_Transform(the_geom,4326))</p>\n <p>ANOVA stands for Analysis of Variance. It is performed to figure out the relation between the different group of categorical data.\n Under ANOVA we have two measures as result:\n – F-testscore : which shows the variaton of groups mean over variation\n – p-value: it shows the importance of the result\n   we can say that there is a strong correlation between other variables and a categorical variable if the ANOVA test gives us a large F-test value and a small p-value.\n   </p>\n  <p>The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969.[1] Ordinarily, regressions reflect &quot;mere&quot; correlations, but Clive Granger argued that causality in economics could be tested for by measuring the ability to predict the future values of a time series using prior values of another time series. Since the question of &quot;true causality&quot; is deeply philosophical, and because of the post hoc ergo propter hoc fallacy of assuming that one thing preceding another can be used as a proof of causation, econometricians assert that the Granger test finds only &quot;predictive causality&quot;.[2]</p>\n<p> A time series X is said to Granger-cause Y if it can be shown, usually through a series of t-tests and F-tests on lagged values of X (and with lagged values of Y also included), that those X values provide statistically significant information about future values of Y.</p>\n <p><a href=\"https://dusk.geo.orst.edu/gis/lec11_12.pdf\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://dusk.geo.orst.edu/gis/lec11_12.pdf</a></p>\n<p> 6.2 Uncertainty in the conception of geographic phenomena\n Many spatial objects are not well defined or their definition is to some extent\n arbitrary, so that people can reasonably disagree about whether a particular\n object is x or not. There are at least four types of conceptual uncertainty.</p>\n<p> Spatial uncertainty\n  Spatial uncertainty occurs when objects do not have a discrete, well\n defined extent. They may have indistinct boundaries (where exactly does a\n wetland end?), they may have impacts that extend beyond their boundaries\n (should an oil spill be defined by the dispersion of pollutants or by the area of\n environmental damage?), or they may simply be statistical entities. The attributes\n ascribed to spatial objects may also be subjective—for example, the spatial\n distributions of poverty and biodiversity depend on human interpretations of what\n these things mean.\n  Vagueness\n  Vagueness occurs when the criteria that define an object as x are not\n explicit or rigorous. In a land cover analysis, how many oaks (or what proportion\n of oaks) must be found in a tract of land to qualify it as oak woodland? What\n incidence of crime (or resident criminals) defines a high crime neighborhood?\n  Ambiguity\n  Ambiguity occurs when y is used as a substitute, or indicator, for x\n because x is not available. The link between direct indicators and the\n phenomena for which they substitute is straightforward and fairly unambiguous.\n Soil nutrient levels (y) are a direct indicator of crop yields (x). Indirect indicators\n tend to be more ambiguous and opaque. Wetlands (y) are an indirect indicator of\n animal species diversity (x). Of course, indicators are not simply direct or indirect;\n they occupy a continuum. The more indirect they are, the greater the ambiguity\n and the less certain it is that an object being approximated using y really is x.</p>\n<p> Physical measurement error\n Instruments and procedures used to make physical measurements are not\n perfectly accurate. For example, a survey of Mount Everest might find its height\n to be 8,850 meters, with an accuracy of plus or minus 5 meters.</p>\n<p> Digitizing error\n  A great deal of spatial data has been digitized from paper maps.\n Digitizing, or the electronic tracing of paper maps, is prone to human error. </p>\n<p> Error caused by combining data sets with different lineages\n  Data sets produced by different agencies or vendors may not match\n because different processes were used to capture or automate the data. For\n example, buildings in one data set may appear on the opposite side of the street\n in another data set.</p>\n<p> 6.4 Uncertainty in the analysis of geographic phenomena\n Spatial analysis methods can create further uncertainty.\n  The ecological fallacy\n  The ecological fallacy is the mistake of assuming that an overall\n characteristic of a zone is also a characteristic of any location or individual within\n the zone.</p>\n<p> The Modifiable Areal Unit Problem (MAUP)\n  The results of data analysis are influenced by the number and sizes of\n the zones used to organize the data. The Modifiable Area Unit Problem has at\n least three aspects:\n 1.The number, sizes, and shapes of zones affect the results of analysis.\n 2.The number of ways in which fine-scale zones can be aggregated into larger\n units is often great.\n 3.There are usually no objective criteria for choosing one zoning scheme over\n another.\n An example of the influence of the number of zones on analysis is the 1950 study\n by Yule and Kendall which found that the correlation between wheat and potato\n yields in England changed from low to high as the data were grouped into fewer\n and fewer zones (starting with 48 and ending with 2).</p>\n <p><a href=\"https://mgimond.github.io/Spatial/uncertainty-in-census-data.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://mgimond.github.io/Spatial/uncertainty-in-census-data.html</a></p>\n<p> Many census datasets such as the U.S. Census Bureau’s American Community Survey (ACS) data1 are based on surveys from small samples. This entails that the variables provided by the Census Bureau are only estimates with a level of uncertainty often provided as a margin of error (MoE) or a standard error (SE). Note that the Bureau’s MoE encompasses a 90% confidence interval2 (i.e. there is a 90% chance that the MoE range covers the true value being estimated). This poses a challenge to both the visual exploration of the data as well as any statistical analyses of that data</p>\n<p> 7.2 Mapping uncertainty\n One approach to mapping both estimates and SE’s is to display both as side-by-side maps.\n While there is nothing inherently wrong in doing this, it can prove to be difficult to mentally process the two maps, particularly if the data consist of hundreds or thousands of small polygons.\n Another approach is to overlay the measure of uncertainty (SE or MoE) as a textured layer on top of the income layer.\n Or, one could map both the upper and lower ends of the MoE range side by side.</p>\n<p> <a href=\"https://en.wikipedia.org/wiki/Time_geography\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://en.wikipedia.org/wiki/Time_geography</a></p>\n <h1>VS Considerations</h1>\n <p>Indicators</p>\n<ul>\n<li><p>Population Change</p>\n</li>\n<li><p>Number of Children Attending Baltimore City Public Schools</p>\n</li>\n<li><p>Aggregated assessment scores for children in the neighborhood</p>\n</li>\n<li><p>Vacant building notices</p>\n</li>\n<li><p>Rehabilitation Permits</p>\n</li>\n<li><p>Demolition Permits</p>\n</li>\n<li><p>New Construction/ Certificates of Occupancy Permits</p>\n</li>\n<li><p>Real Estate Projects Under Development Review</p>\n</li>\n<li><p>Home Sales price and Arms-length transaction values (non-sales)</p>\n</li>\n<li><p>Owner-occupancy rates</p>\n</li>\n<li><p>311 calls for service rates for trash, street light outages and clogged storm drains.</p>\n</li>\n<li><p>911 calls for narcotics</p>\n</li>\n<li><p>Crime incidences (all part 1 crimes including gun related homicides)</p>\n</li>\n<li><p>Number of businesses and number of employees in the neighborhood</p>\n</li>\n<li><p>Normalized</p>\n</li>\n<li><p>Values vary across indicator</p>\n</li>\n<li><p>Disparity and Outliers exist</p>\n</li>\n<li><p>Favorable outcomes to Stable Neighborhoods</p>\n</li>\n<li><p>Negative outcomes in Stressed Neighborhoods</p>\n</li>\n<li><p>Hidden Variables exist impacting Neighborhoods</p>\n</li>\n<li><p>Identify characteristics regarding Neighborhood Trajectory</p>\n</li>\n<li><p>Extend existing infrastructure</p>\n</li>\n<li><p>Understand Dynamics</p>\n</li>\n<li><p>Track Progress of Indicators Over Time.</p>\n</li>\n<li><p>Monitor Trends</p>\n</li>\n<li><p>Develop Solutions</p>\n</li>\n<li><p>Measure Organizational/ Programmatic Outcomes</p>\n</li>\n<li><p>Track Quantitative measures across 7 years</p>\n</li>\n<li><p>Track 2 indicators at a time</p>\n</li>\n<li><p>Compare X Neighborhoods across Z indicators</p>\n</li>\n<li><p>Interactive (what part of this)</p>\n</li>\n<li><p>Relationships across indicators and neighborhoods</p>\n</li>\n<li><p>Cluster into Typology</p>\n</li>\n<li><p>Inform the BIW entrepreneurship strategies with respect to *   existing trends opportunity needs</p>\n</li>\n<li><p>Inform the kinds of issues social entrepreneurs aim to address. *   Ground business strategies in cluster-types that grow to scale across typologies.</p>\n</li>\n</ul>\n <p>Quantative Data - Subjects in  controlled experiments that do not recieve the treatment\n Control Group - Repition of experiment under same or similar conditions\n replication - numerical description that summarizes data for an entire population</p>\n<p> data maturity - people need the tools to explore the data before going futher</p>\n <p>GeoId = sate + county + tract ( = blockgroups == tracts = blocklots = lots + blocks)</p>\n<p> measures have multi facet dimensions with each dimension with possible granularity that form hierarchy</p>\n<p> to explore data is less meaningfull than to act on it bevause p != np. assessing all possible realities is often not possible and we experience biases (like from mental models anchoring, survivorship) that coellesce to unforeseable black swan events.</p>\n<p> Convexity of risk tolerence</p>\n<p> classification and labeling</p>\n<p> gestalt processess. small multiples. loops. psychophysics -&gt; DNA Database</p>\n<p> <a href=\"https://colah.github.io/posts/2015-09-Visual-Information/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://colah.github.io/posts/2015-09-Visual-Information/</a></p>\n <h4>Geospatials</h4>\n<p> Line or Polygon from a Collection of Point</p>\n<p> Union, Difference, Distance</p>\n<p> Intersects, Touches, Crosses, Within</p>\n<p> using a function called .within() that checks if a point is within a polygon</p>\n<p> using a function called .contains() that checks if a polygon contains a point</p>\n<p> if objects intersect, the boundary and interior of an object needs to intersect in any way with those of the other.</p>\n<p> If an object touches the other one, it is only necessary to have (at least) a single point of their boundaries in common but their </p>\n<p> interiors shoud NOT intersect.</p>\n<p> nearest_points()</p>\n <h3>API Stuff</h3>\n  <h3>CheckColumns SaveGeoData</h3>\n  <h3>Projections</h3>\n \n  <script src=\"https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js\"></script>\n  <link rel=\"stylesheet\" href=\"https://cdn.rawgit.com/google/code-prettify/master/styles/desert.css\"/>\n  "}