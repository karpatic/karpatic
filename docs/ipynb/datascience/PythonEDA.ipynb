{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","name":"python2"},"colab":{"name":"PythonEDA.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"sC0J3Ued5biR"},"source":["https://www.itl.nist.gov/div898/handbook/"]},{"cell_type":"markdown","metadata":{"id":"Q_6js81C5biS"},"source":["# Welcome"]},{"cell_type":"markdown","metadata":{"id":"AVZElU2e5biS"},"source":["This notebook has functions to help handle common tasks"]},{"cell_type":"markdown","metadata":{"id":"DFBQOMW95biS"},"source":["**Functions**\n","- FillNa DropNa Replace\n","- CustomLambda = Lambda X:(x+x%2)\n","- df.groupby().transform(customLambda):sumAgg\n","- Pd.melt()-\\&gt;columnRows\n","- DummyEncode\n","- Df.Stack.Unstack\n","- Infer data types isfinite-inf/nan, isnan, first char is symbol, default value."]},{"cell_type":"markdown","metadata":{"id":"FUXzO5KY5biS"},"source":["#### Common Python Data Manipulations"]},{"cell_type":"markdown","metadata":{"id":"3pmqTayb5biS"},"source":["https://datascience.stackexchange.com/questions/37878/difference-between-isna-and-isnull-in-pandas\n","\n","**Common Python Data Manipulations**\n"," - .isna(), .fillna(), .isnull()\n"," - .dropna(how=&#39;any&#39;),\n"," - .fillna(method=&#39;ffill&#39;, inplace=true), method=&#39;ffill&#39;, .fillna(value=0, inplace=true)\n"," - .duplicated(), .unique(), .drop\\_duplicates()\n"," - .replace()\n"," - groupby()\n","\n","- contains(), within() for geospatial data.\n","**Common Python Cleaning operations:**\n","1. Check the data types of all column in the data-frame\n"," 2. Create a new data-frame excluding all the &#39;object&#39; types column\n"," 3. Select elements from each column that lie within 3 units of Z score\n","- .cut() will bin your data\n"," - .dtypes, -.select\\_dtypes(exclude=[&#39;object&#39;])\n"," - stats.zscore(df)"]},{"cell_type":"markdown","metadata":{"id":"lQ4Y4Ti-5biS"},"source":["#### FILTERING"]},{"cell_type":"markdown","metadata":{"id":"iqa2fCAt5biS"},"source":["* DataFrame.isna()\tDetect missing values.\n","* DataFrame.any(axis=0, bool_only=None, skipna=True, level=None, **kwargs)\n","* DataFrame.all(axis=0, bool_only=None, skipna=True, level=None, **kwargs)\n","* DataFrame.filter([items, like, regex, axis])\tSubset rows or columns of dataframe according to labels in the specified index.\n","* DataFrame.dropna([axis, how, thresh, …])\tRemove missing values.\n","* DataFrame.fillna([value, method, axis, …])\tFill NA/NaN values using the specified method.\n","* DataFrame.replace([to_replace, value, …])\tReplace values given in to_replace with value.\n","* DataFrame.interpolate([method, axis, limit, …])\tInterpolate values according to different methods.\n","* DataFrame.nlargest(n, columns[, keep])\tReturn the first n rows ordered by columns in descending order.\n","* DataFrame.nsmallest(n, columns[, keep])\tReturn the first n rows ordered by columns in ascending order."]},{"cell_type":"markdown","metadata":{"id":"ub_z31pR5biS"},"source":["#### GROUPING/ Aggregating/ Manipulating"]},{"cell_type":"markdown","metadata":{"id":"e_wqvSYZ5biS"},"source":["* DataFrame.pivot([index, columns, values])\tReturn reshaped DataFrame organized by given index / column values.\n","*df.agg(\"mean\", axis=\"columns\") # axis : {0 or ‘index’, 1 or ‘columns’}, default 0\n","* DataFrame.compound(axis=None, skipna=None, level=None)\n","* DataFrame.count(axis=0, level=None, numeric_only=False)[source]\n","* df.groupby(['1_tpop']).mean()\n","* DataFrame.insert(loc, column, value[, …])\tInsert column into DataFrame at specified location."]},{"cell_type":"markdown","metadata":{"id":"zuMUJQR_5biS"},"source":["#### Common Python Cleaning operations:"]},{"cell_type":"markdown","metadata":{"id":"d_iStUNb5biS"},"source":["*   1. Check the data types of all column in the data-frame\n","*   2. Create a new data-frame excluding all the ‘object’ types column\n","*   3. Select elements from each column that lie within 3 units of Z score\n","*   .cut() will bin your data\n","*   .dtypes, -.select_dtypes(exclude=[‘object’])"]},{"cell_type":"markdown","metadata":{"id":"v81WwHQf5biS"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"GSUP83-65biT"},"source":["biggest data cleaning task, missing values\n","\n","Pandas will recognize both empty cells and “NA” types as missing values. Anything else should to be specified on import"]},{"cell_type":"code","metadata":{"id":"euYNxHDm5biT"},"source":["print df['ST_NUM'].isnull()\n","\n","# Making a list of missing value types\n","missing_values = [\"n/a\", \"na\", \"--\"]\n","df = pd.read_csv(\"property data.csv\", na_values = missing_values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1Y9mZ5k5biT"},"source":["#Dealing with missing values? How many np.nan per column?\n","df.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mqNvfMFJ5biT"},"source":["In the code we’re looping through each entry in the “Owner Occupied” column. To try and change the entry to an integer, we’re using int(row).\n","If the value can be changed to an integer, we change the entry to a missing value using Numpy’s np.nan.\n","On the other hand, if it can’t be changed to an integer, we pass and keep going.  The .loc method is the preferred Pandas method for modifying entries in place. https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.loc.html"]},{"cell_type":"code","metadata":{"id":"C5rWdgYh5biU"},"source":["# Total missing values for each feature\n","print df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qnu3ZKxV5biU"},"source":["# Any missing values?\n","print df.isnull().values.any()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvGdKgQR5biU"},"source":["# Total number of missing values\n","print df.isnull().sum().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TaxZcH0w5biU"},"source":["# Replace missing values with a number\n","df['ST_NUM'].fillna(125, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dwuDQdue5biU"},"source":["# Location based replacement\n","df.loc[2,'ST_NUM'] = 125"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ch3iiqD_5biU"},"source":["# Replace using median \n","median = df['NUM_BEDROOMS'].median()\n","df['NUM_BEDROOMS'].fillna(median, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ob-_i-Zi5biU"},"source":["# This aggreates the data by its column names, then we pass the aggregation function (size = count)\n","df.groupby(by =['class', 'doctor_name']).size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlxpkmv95biU"},"source":[" #drop rows with any column having np.nan values\n","df = df.dropna(axis = 1, how = 'all') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDfqX6J85biU"},"source":["# Detecting numbers \n","cnt=0\n","for row in df['OWN_OCCUPIED']:\n","    try:\n","        int(row)\n","        df.loc[cnt, 'OWN_OCCUPIED']=np.nan\n","    except ValueError:\n","        pass\n","    cnt+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oSrf7x75biU"},"source":["# Number of unique values in each column.\n","df.nunique()\n","\n","# We see here that although there are 699 rows, there are only 645 unique patient_id’s. \n","# This could mean that some patient appear more than once in the dataset\n","# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html\n","df[df.duplicated(subset = 'patient_id', keep =False)].sort_values('patient_id')\n","\n","# The number of times a patient shows up in the dataset can also be viewed.\n","repeat_patients = df.groupby(by = 'patient_id').size().sort_values(ascending =False)\n","\n","# to remove patients that show up more that 2 times in the data set.\n","filtered_patients = repeat_patients[repeat_patients > 2].to_frame().reset_index()\n","filtered_df = df[~df.patient_id.isin(filtered_patients.patient_id)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oi5o7F--5biU"},"source":["# Its good to inspect unique key identifiers\n","df.nunique()\n","\n","# This shows rows that show up more than once and have the exact same column values. \n","df[df.duplicated(keep = 'last')]\n","\n","# # This shows all instances where pantient_id shows up more than once, but may have varying column values\n","# df[df.duplicated(subset = 'patient_id', keep =False)].sort_values('patient_id')\n","\n","#Now that I have seen that there are some duplicates, I am going to go ahead and remove any duplicate rows\n","#, same things that occours twice\n","\n","df = df.drop_duplicates(subset = None, keep ='first')\n","\n","repeat_patients = df.groupby(by = 'patient_id').size().sort_values(ascending =False)\n","repeat_patients\n","\n","filtered_patients = repeat_patients[repeat_patients > 2].to_frame().reset_index()\n","filtered_df = df[~df.patient_id.isin(filtered_patients.patient_id)]\n","filtered_df\n","\n","# This is all the repeating patients details\n","\n","df[df.patient_id.isin(filtered_patients.patient_id)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUCjIOUE5biU"},"source":["# One Hot Encoding Catergorical Data\n","categorical_df = df[['patient_id', 'doctor_name']]\n","\n","# This specifies all rows (':') and column name 'doctor_count'\n","categorical_df.loc[:,'doctor_count'] = 1\n","\n","doctors_one_hot_encoded  = pd.pivot_table(categorical_df\n","                                  ,index = categorical_df.index, \n","                                  columns = ['doctor_name'], values = ['doctor_count'])\n","\n","doctors_one_hot_encoded = doctors_one_hot_encoded.fillna(0)\n","\n","doctors_one_hot_encoded.columns = doctors_one_hot_encoded.columns.droplevel()\n","\n","# Typically a left join in pandas looks like this:\n","# leftJoin_df = pd.merge(df1, df2, on ='col_name', how='left')\n","# However we are joining on the index so we pass the “left_index” and “right_index” option \n","# to specify that the join key is the index of both tables\n","combined_df = pd.merge(df, doctors_one_hot_encoded, left_index = True,right_index =True, how ='left')\n","combined_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DoYyFKFf5biU"},"source":["# How to convert benign & malingant to 0 and 1\n","class_to_numerical_dictionary = {'benign':0, 'malignant':1}\n","combined_df['class'] = combined_df['class'].map(class_to_numerical_dictionary)\n","combined_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOAc7YDJ5biU"},"source":["# Feature building: \n","# combined_df[~(combined_df.cell_size_uniformity >5) & (combined_df.cell_shape_uniformity >5)]\n","def celltypelabel(x):\n","    if ((x['cell_size_uniformity'] > 5) & (x['cell_shape_uniformity'] > 5)): return('normal')\n","    else: return('abnormal')\n","      \n","combined_df['cell_type_label'] = combined_df.apply(lambda x: celltypelabel(x), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQDysAMO5biU"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"FmslS0Xd5biU"},"source":["# Read In Data"]},{"cell_type":"code","metadata":{"id":"zWqYDTTy5biU"},"source":["import pandas as pd\n","import csv\n","import IPython\n","from google.colab import output\n","import json\n","import geopandas\n","\n","# Functions created here are called by the server\n","df = 'test'\n","\n","# All data is returned in the same exact fasion. \n","# pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html\n","# ‘split’ : dict like {‘index’ -> [index], ‘columns’ -> [columns], ‘data’ -> [values]}\n","def importDataCSVPy(fileName):\n","  global df\n","  df = pd.read_csv(fileName,quoting=csv.QUOTE_ALL)\n","  return df.to_json(orient='split') \n"," \n","def importDataXLPy(fileName):\n","  global df\n","  df = geopandas.read_excel(fileName)\n","  return df.to_json(orient='split')\n","\n","def importDataGEOJSONPy(fileName):\n","  global df\n","  df = geopandas.read_file(fileName)\n","  return df.to_json(orient='split')\n","\n","def importDataJSONPy(fileName):\n","  global df\n","  df = pd.read_json(fileName)\n","  return df.to_json(orient='split') \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gHYOVJNj5biU"},"source":["# dashboards notes reduced\n","\n","> The functions that transform notebooks in a library"]},{"cell_type":"markdown","metadata":{"id":"_qm6dwEp5biU"},"source":["Basic Text"]},{"cell_type":"markdown","metadata":{"id":"CYEMWEvR5biU"},"source":["TODO :\n","*  Provide a user of Import Options ,\n","*  Ask For file, Default = False\n","*  Ask For Delimiters, Default = ,\n","*  Ask For String Delimiters, Default = \"\n","*  Ask If First Column Represents Header, Default = False\n","*  Ask If the Column Names are Correct\n","\n","FillNA = -1, avg\n","\n","FillNA THEN Coerce"]},{"cell_type":"markdown","metadata":{"id":"rnXnmycp5biU"},"source":["### Todo: \n","* Interactive Inputs allow user to perform Simple Querys\n","* Fixed Dictionary [ distinct, not, like, avg, min, max, mean, median, mode ]\n","*   Query Replaces the Imported Dataset\n","*   Repeat until user specifies otherwise\n","*  Template: Select From Where GroupBy Having"]},{"cell_type":"markdown","metadata":{"id":"j5nt7QYZ5biU"},"source":["## MISC "]},{"cell_type":"markdown","metadata":{"id":"E2bGRfsi5biU"},"source":["### Import "]},{"cell_type":"code","metadata":{"id":"wBXV3c0P5biU"},"source":["td.filter(regex=\"Births\").hist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"950TRE1E5biU"},"source":["## Parse The DataTypes"]},{"cell_type":"code","metadata":{"id":"sLzk1TRQ5biU"},"source":["# CASTING NOTES\n","# DataFrame.infer_objects()\tAttempt to infer better dtypes for object columns.\n","# DataFrame.astype(dtype[, copy, errors])\tCast a pandas object to a specified dtype dtype.\n","# CASTING and COERCING => df['y'] = pd.to_numeric(df['y'],errors='coerce')\n","# Coerce: Ignore, Coerce, Raise\n","# DataFrame.to_period([freq, axis, copy])\tConvert DataFrame from DatetimeIndex to PeriodIndex with desired frequency (inferred from index if not passed).\n","# DataFrame.to_timestamp([freq, how, axis, copy])\tCast to DatetimeIndex of timestamps, at beginning of period.\n","# DataFrame.tz_convert(tz[, axis, level, copy])\tConvert tz-aware axis to target time zone.\n","\n","df3.head()\n","# To Numeric\n","df3 = df3[ df3.columns ].apply(pd.to_numeric, errors='ignore')\n","\n","# Parse Date Times\n","# DataFrame.to_timestamp([freq, how, axis, copy])\tCast to DatetimeIndex of timestamps, at beginning of period.\n","\n","# Parse GEOMETRY Coordinates \n","# df['y'] = pd.to_numeric(df['y'],errors='coerce')\n","# df = df.replace(np.nan, 0, regex=True)\n","# gdf = GeoDataFrame(df.drop(['x', 'y'], axis=1), crs={'init': 'epsg:2248'}, geometry=[shapely.geometry.Point(xy) for xy in zip(df.x, df.y)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Viihq1k5biU"},"source":["# Perform the Casting.\n","def get_var_category(series):\n","    unique_count = series.nunique(dropna=False)\n","    total_count = len(series)\n","    if pd.api.types.is_numeric_dtype(series):\n","        return 'Numerical'\n","    elif pd.api.types.is_datetime64_dtype(series):\n","        return 'Date'\n","    elif unique_count==total_count:\n","        return 'Text (Unique)'\n","    else:\n","        return 'Categorical'\n","\n","def print_categories(df):\n","    for column_name in df.columns:\n","        print(column_name, \": \", get_var_category(df[column_name]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqf7Bo3w5biU"},"source":["# Get File Types By Type\n","types = df.dtypes\n","bools = df.select_dtypes(include='bool')\n","ints = df.select_dtypes(include=['float','integer'] )\n","df.ftypes\n","df.get_dtype_counts()\n","df.get_ftype_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NX2XT7rT5biU"},"source":["Data Type Conversion using to_datetime() and astype() methods:\n","\n","Example of to_datetime(): train['Date.of.Birth']= pd.to_datetime(train['Date.of.Birth'])\n","\n","Example of astype(): train['ltv'] = train['ltv'].astype('int64')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fw81uwG15biU"},"source":["# Length - The len function counts the number of observations in a Series. The function will count all observations, regardless if there are missing or null values.\n","length = len(beers[\"ibu\"])\n","# Count - The count function will return the number of non-NA/non-null observations in a Series.\n","count = beers[\"ibu\"].count()\n","# Missing Values - With the Length and the Count, we are now able to calculate the number of missing values. The number of missing values is the difference between the Length and the Count.\n","pct_of_missing_values = \"{0:.1f}%\".format( float(length - count / length) *100)\n","# Minimum/Maximum Value - The minimum and maximum value of a dataset can easily be obtained with the min and max function on a Series.\n","# Quantile Statistics - Quantiles are cut points that split a distribution in equal sizes. Many quantiles have their own name. If you split a distribution into four equal groups, the quantile you created is named quartile. You can easily create quantile using the quantile function on a Series. You can pass to that function an array with the different quantiles to compute. In the case below, we want to split our distribution in four equal groups.\n","quantile = beers[\"ibu\"].quantile([.25, .5, .75])\n","# we can’t talk about data profiling without mentioning the importance of a frenquency-distribution plot. It is one of the simplest yet most powerful visualization. It demonstrates the frequency of each value in our dataset.\n","sns.distplot(beers[\"ibu\"].dropna());\n","# Correlations - Correlations are a great way to discover relationships between numerical variables. There are various ways to calculate the correlation. The Pearson correlation coefficient is a widely used approach that measures the linear dependence between two variables. The correlation coefficient ranges from -1 to 1. A correlation of 1 is a total positive correlation, a correlation of -1 is a total negative correlation and a correlation of 0 is non-linear correlation. We can perform that calculation using the corr function on a Series. By default, this function will use the Pearson correlation coefficient calculation. It is possible to use different methods of calculation with this function.\n","beers[[\"abv\", \"ibu\", \"ounces\"]].corr()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pYN-F_Io5biU"},"source":["# NOTES"]},{"cell_type":"code","metadata":{"id":"IdbBZ6h25biV"},"source":["  # To 'import' a script you wrote, map its filepath into the sys\n","  \n","# https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats\n","\n","Exponentially-weighted moving window functions ( mean std var corr cov) \n","Standard expanding window functions ( sum, mean, median, var, std, min, max, corr, cov, skew, kurt, agg, quant)\n","Standard moving window functions (count, sum, mean, median, var, std, min, max, corr, cov, skew, kurt, aggr, quant)\n","\n","df( columns, dtypes, ftypes, select_dtypes, values[ReturnsNumpyArrOfArrays], shape[records,Cols] size(recordsXcols), empty[T/F]\n","df( astype[castDtype], infer_objects, copy, isna, notna )\n","df( add, sub, mul, div, mod, pow)\n","df( abs, all, any, corr, count, cov, cummax, cummin, cumprod, cumsum, describe, kurt, mad[mean_abs_dev], max, mean, median, min, mode, pct_change[curVSpriorElem] , prod, quantile, rank, round, sem[stdErrMean], skew, sum, std, var, nunique[num_unique] )\n","\n","(addprefix, addsuffix, at_time, between_time, drop, drop_duplicates, equals, head, last)\n","(dropna, fillna, replace, interpolate)\n","(pivot, pivotTable, sort_values, nlargest, nsmallest, )\n","DataFrame.to_timestamp(self[, freq, how, …])\tCast to DatetimeIndex of timestamps, at beginning of period.\n","   \n","DataFrame.plot([x, y, kind, ax, ….])\tDataFrame plotting accessor and method\n","DataFrame.plot.area(self[, x, y])\tDraw a stacked area plot.\n","DataFrame.plot.bar(self[, x, y])\tVertical bar plot.\n","DataFrame.plot.barh(self[, x, y])\tMake a horizontal bar plot.\n","DataFrame.plot.box(self[, by])\tMake a box plot of the DataFrame columns.\n","DataFrame.plot.density(self[, bw_method, ind])\tGenerate Kernel Density Estimate plot using Gaussian kernels.\n","DataFrame.plot.hexbin(self, x, y[, C, …])\tGenerate a hexagonal binning plot.\n","DataFrame.plot.hist(self[, by, bins])\tDraw one histogram of the DataFrame’s columns.\n","DataFrame.plot.kde(self[, bw_method, ind])\tGenerate Kernel Density Estimate plot using Gaussian kernels.\n","DataFrame.plot.line(self[, x, y])\tPlot Series or DataFrame as lines.\n","DataFrame.plot.pie(self, \\*\\*kwargs)\tGenerate a pie plot.\n","DataFrame.plot.scatter(self, x, y[, s, c])\tCreate a scatter plot with varying marker point size and color.\n","DataFrame.boxplot(self[, column, by, ax, …])\tMake a box plot from DataFrame columns.\n","DataFrame.hist(data[, column, by, grid, …])\tMake a histogram of the DataFrame’s.\n","   \n","   \n","df.to_html()\n","df.to_latex()\n","DataFrame.to_records(self[, index, …])\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9ORYBPN5biV"},"source":["### Plot Histograms"]},{"cell_type":"code","metadata":{"id":"v-mjgRR55biV"},"source":["td.filter(regex=\"Births\").hist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BBY8tmjo5biV"},"source":["## Basic ops"]},{"cell_type":"code","metadata":{"id":"c36KpCb15biV"},"source":["# Encoding Mechanism\n","pd.get_dummies(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9I07dacN5biV"},"source":["# If the data lends itself to it\n","def norm(x):\n","  return (x - train_stats['mean']) / train_stats['std']\n","normed_train_data = norm(train_dataset)\n","normed_test_data = norm(test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMiuVYIw5biV"},"source":["# DataFrame.shape\tReturn a tuple representing the dimensionality of the DataFrame.\n","df3.shape\n","\n","#DataFrame.size\tReturn an int representing the number of elements in this object.\n","df.size\n","\n","# DataFrame.ndim\tReturn an int representing the number of axes / array dimensions.\n","df.ndim\n","\n","# Note Used : \n","# DataFrame.axes\tReturn a list representing the axes of the DataFrame.\n","\n","df3.dtypes\n","\n","# Return unbiased kurtosis over requested axis using Fisher’s definition of kurtosis (kurtosis of normal == 0.0).\n","df.kurtosis()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8uUKii15biV"},"source":["# Available Operations\n","df.describe()\n","DataFrame.median([axis, skipna, level, …])\t# Return the median of the values for the requested axis.\n","DataFrame.mode([axis, numeric_only, dropna])\t# Get the mode(s) of each element along the selected axis.\n","DataFrame.sem([axis, skipna, level, ddof, …])\t# Return unbiased standard error of the mean over requested axis.\n","DataFrame.nunique([axis, dropna])\t# Count distinct observations over requested axis.\n","DataFrame.var([axis, skipna, level, ddof, …])\t# Return unbiased variance over requested axis.\n","DataFrame.drop_duplicates([subset, keep, …])\t# Return DataFrame with duplicate rows removed, optionally only considering certain columns."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LqOsE5j95biV"},"source":["##  Categorical Analysis:"]},{"cell_type":"markdown","metadata":{"id":"9P_qaKEs5biV"},"source":["Count, Unique, Top, Frequency"]},{"cell_type":"code","metadata":{"id":"97ybaIM95biV"},"source":["# \n","# Count each Unique \n","# Plot a histogram\n","# \n","def getCategoricalCounts(df):\n","  df = df.iloc[1:]\n","  for column in df:\n","    print(column)\n","    return df[column]\n","    #print( df[ df.columns[column] ].counts() )\n","  # return df.value_counts().plot.bar(title=\"Catagorical Distribution\")\n","  # print(sales_data[sales_data['File_Type'] == 'Historical']['SKU_number'].count())\n","# tmp = df3[ df3.columns[0] ]\n","# getCategoricalCounts( tmp )\n","# df3.select_dtypes(include=['int','float'] )\n","# df3.describe(include='all')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwKkU0J85biV"},"source":["# Print the value counts for categorical columns\n","for col in df.columns:\n","    if df[col].dtype == 'object':\n","        print('\\nColumn Name:', col,)\n","        print(df[col].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulDPFEC-5biV"},"source":["### Distribution by Categorical Column Values\n","\n","# Grade distribution by address\n","sns.kdeplot(df.ix[df['address'] == 'U', 'Grade'], label = 'Urban', shade = True)\n","sns.kdeplot(df.ix[df['address'] == 'R', 'Grade'], label = 'Rural', shade = True)\n","plt.xlabel('Grade'); plt.ylabel('Density'); plt.title('Density Plot of Final Grades by Location');\n","\n","Variable Correlations with Final Grade\n","# Correlations of numerical values\n","df.corr()['Grade'].sort_values()\n","Categorical Correlations using One-Hot Encoding\n","# Select only categorical variables\n","category_df = df.select_dtypes('object')\n","# One hot encode the variables\n","dummy_df = pd.get_dummies(category_df)\n","# Put the grade back in the dataframe\n","dummy_df['Grade'] = df['Grade']\n","# Correlations in one-hot encoded dataframe\n","dummy_df.corr()['Grade'].sort_values()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccuc3JH85biV"},"source":["# A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-learn\n","\n","# https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf\n","  \n","def categorical_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', verbose=True):\n","    '''\n","    Helper function that gives a quick summary of a given column of categorical data\n","    Arguments\n","    =========\n","    dataframe: pandas dataframe\n","    x: str. horizontal axis to plot the labels of categorical data, y would be the count\n","    y: str. vertical axis to plot the labels of categorical data, x would be the count\n","    hue: str. if you want to compare it another variable (usually the target variable)\n","    palette: array-like. Colour of the plot\n","    Returns\n","    =======\n","    Quick Stats of the data and also the count plot\n","    '''\n","    if x == None:\n","        column_interested = y\n","    else:\n","        column_interested = x\n","    series = dataframe[column_interested]\n","    print(series.describe())\n","    print('mode: ', series.mode())\n","    if verbose:\n","        print('='*80)\n","        print(series.value_counts())\n","\n","    sns.countplot(x=x, y=y, hue=hue, data=dataframe, palette=palette)\n","    plt.show()\n","    \n","# Target Variable: Survival\n","# Univariate Analysis\n","c_palette = ['tab:blue', 'tab:orange']\n","categorical_summarized(train_df, y = 'Survived', palette=c_palette)\n","\n","\n","# Feature Variable: Gender\n","# Bivariate Analysis\n","categorical_summarized(train_df, y = 'Sex', hue='Survived', palette=c_palette)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rULXTUD65biV"},"source":["##  Numeric Analysis:"]},{"cell_type":"code","metadata":{"id":"GWUeph6x5biV"},"source":["# A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-learn\n","\n","# https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf\n","\n","def quantitative_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', ax=None, verbose=True, swarm=False):\n","    '''\n","    Helper function that gives a quick summary of quantattive data\n","    Arguments\n","    =========\n","    dataframe: pandas dataframe\n","    x: str. horizontal axis to plot the labels of categorical data (usually the target variable)\n","    y: str. vertical axis to plot the quantitative data\n","    hue: str. if you want to compare it another categorical variable (usually the target variable if x is another variable)\n","    palette: array-like. Colour of the plot\n","    swarm: if swarm is set to True, a swarm plot would be overlayed\n","    Returns\n","    =======\n","    Quick Stats of the data and also the box plot of the distribution\n","    '''\n","    series = dataframe[y]\n","    print(series.describe())\n","    print('mode: ', series.mode())\n","    if verbose:\n","        print('='*80)\n","        print(series.value_counts())\n","\n","    sns.boxplot(x=x, y=y, hue=hue, data=dataframe, palette=palette, ax=ax)\n","\n","    if swarm:\n","        sns.swarmplot(x=x, y=y, hue=hue, data=dataframe,\n","                      palette=palette, ax=ax)\n","\n","    plt.show()\n","    \n","# quantitative_summarized can take in one Quantitative Variable and up to two Categorical Variables, where the Quantitative Variable has to be assigned to y and the other two Categorical Variables can be assigned to x and hue respectively. \n","\n","# univariate analysis\n","quantitative_summarized(dataframe= train_df, y = 'Age', palette=c_palette, verbose=False, swarm=True)\n","\n","# bivariate analysis with target variable\n","quantitative_summarized(dataframe= train_df, y = 'Age', x = 'Survived', palette=c_palette, verbose=False, swarm=True)\n","\n","\n","# multivariate analysis with Embarked variable and Pclass variable\n","quantitative_summarized(dataframe= train_df, y = 'Age', x = 'Embarked', hue = 'Pclass', palette=c_palette3, verbose=False, swarm=False)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bb0lAswX5biV"},"source":["# Geo"]},{"cell_type":"markdown","metadata":{"id":"HMdHFgP05biV"},"source":["**Future Self Service Tool**\n","\n","Data analytics\n","1. Self Service\n","2. Reccurent Reports \n","3. Embedded Analytics.\n","\n","**GisHandler**() \n","\n","- Check Columns\n","- Check If Operations will work as expected\n","- perform operations\n","- tidy up\n","- save\n","- return\n","\n","**Main**( Check For Missing Values, Perform Operation)\n","\n","**readFile**() - csv/postgis -df -reverseGeocode? ColumnToCords? -Geodf\n","\n","**Geodataframe** -toCrs, - saveGeoDataFrame\n","\n","**MergeBounds**() \n","\n","**FilterBounds**() \n","\n","**FilterPoints**() Bounds Points \n","\n","**PoinsInPoly**()\n"]},{"cell_type":"markdown","metadata":{"id":"682FqOZL5biV"},"source":["**Applied Spatial Statistics**\n","- Prior Posterior Distribution\n","- Hierarchal Models\n","- Markov Chain Monte Carlo\n","- Kernal Methods\n","- Dynamic State Space Modeling\n","- Multiple linear Regressions\n","- Spatial Models (Car Sar) Kriging\n","- Time series models: ARM ARMA \n","- Dynamic linear models\n","- multi level models - causal inference - meta analysis\n","- multi agent decision making\n","- variable transformations\n","- eigenvalues"]},{"cell_type":"markdown","metadata":{"id":"5hG0qvD35biV"},"source":["**Applied Spatial Statistics** -\\&gt; Prior/Posteriors, MCMC, Kernel methods, dynamic state space modeling, multiple linear regression, multilevel models(causal inference, meta analysis), multi agent decision making, variable transformations, eigenvalues,\n","**Spatial models** (Car,Sar) Kriging\n","**Time Series Models** : ARM ARMA Dynamic linear models"]},{"cell_type":"markdown","metadata":{"id":"Xrz_TvMX5biV"},"source":["Exploratory spatial analysis, spatial autocorrelation, spatial regression, interpolation, grid based stats, point based stats, spatial network analysis, spatial clustering."]},{"cell_type":"markdown","metadata":{"id":"C9kQqncT5biV"},"source":[" Big-Data, Structure(Semi/Un), Time-Stamped, Spatial, Spatio-Temporal, Ordered, Stream, Dimensionality,\n"," Primary Keys, Unique Values, Index, Spatial, Auto Increment, Default Values, Null Values\n"]},{"cell_type":"markdown","metadata":{"id":"MpHwCOvb5biV"},"source":["**Geographic Inquery**:\n","- Describe real world phenomena\n","- Study of Spatial Arrangement of features\n","- Patterns arise as a result of process operating within space\n","- Measure compare generate\n","- Size distribution pattern contiguity shape community scale orientation relation\n","- How comparE? How describe analyze? How predict?\n","- Entry, conversion, storage, query, manipulation, analysis, presentation,\n","- Req, process, clean ,explore, model …\n","- Hot spot analysis \\_\\&gt; cluster points\n","- Line of sight/visibility analysis -\\&gt; network, overlay, proximity, risk\n","- Heat maps\n","- GeoCoding\n","- Distance Decay\n","- Clip Analysis\n","- post analaysis\n","- land use analysis\n","- voronoi crop by bounds of other ds,\n","- Buffering -radius around a point\n","\n","- Map coverage, \n","  spatial resource allocation, \n","- impact assesment, \n","- pollutant reduction, \n","- decision support, \n","- facility management (water plant mgmt), \n","- operations mgmt, \n","- site selection - where to do xyz, \n","- business/marketing"]},{"cell_type":"markdown","metadata":{"id":"tqG4W34n5biV"},"source":["http://pysal.org/notebooks/explore/esda/Spatial_Autocorrelation_for_Areal_Unit_Data.html\t\n","Python Spatial Analysis library.\t\n","https://pysal.org/notebooks/intro\tPython Spatial Analysis library.\n","Shape Analysis\t\n","hull: calculate the convex hull of the point pattern\t\n","mbr: calculate the minimum bounding box (rectangle)\t\n","The python file centrography.py contains several functions with which we can conduct centrography analysis.\t\n","\t\n","Random point patterns are the outcome of CSR. https://en.wikipedia.org/wiki/Complete_spatial_randomness CSR has two major characteristics:\t\n","Uniform: each location has equal probability of getting a point (where an event happens)\t\n","Independent: location of event points are independent\t\n","It usually serves as the null hypothesis in testing whether a point pattern is the outcome of a random process.\t\n","There are two possible objectives in a discriminant analysis:\t\n","- finding a predictive equation for classifying new individuals\t\n","- interpreting the predictive equation to better understand the relationships that may exist among the variables.\t\n","It was demonstrated by Clark and Evans(1954) that mean nearest neighbor distance statistics distribution is a normal distribution under null hypothesis (underlying spatial process is CSR). We can utilize the test statistics to determine whether the point pattern is the outcome of CSR.\t"]},{"cell_type":"markdown","metadata":{"id":"qN7rARz05biV"},"source":["# Misc"]},{"cell_type":"markdown","metadata":{"id":"cF5fZ3E_5biV"},"source":["https://www.gnu.org/philosophy/open-source-misses-the-point.html\n","\n","It seems to me that the chief difference between the MIT license and GPL is that the MIT doesn't require modifications be open sourced whereas the GPL does. \n","\n","You don't have to open-source your changes if you're using GPL.\tYou could modify it and use it for your own purpose as long as you're not distributing it\t\n","\n","BUT... \n","\n","if you DO distribute it, then your entire project that is using the GPL code also becomes GPL automatically Which means, it must be open-sourced, and the recipient gets all the same rights as you - meaning, they can turn around and distribute it, modify it, sell it, etc. \n","\n","And that would include your proprietary code which would then no longer be proprietary - it becomes open source.\n","\n","with MIT is that even if you actually distribute your proprietary code that is using the MIT licensed code you do not have to make the code open source you can distribute it as a closed app where the code is encrypted or is a binary.\n","\n","\tIncluding the MIT-licensed code can be encrypted, as long as it carries the MIT license notice. \n","\t"]},{"cell_type":"code","metadata":{"id":"2Awk4dGF5biV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9OVAxQV5biV"},"source":["- File-\\&gt;UncleanData-\\&gt;ToCsvFormat(filename,data)\n","- ProcessCsv -\\&gt; Unclean Data\n","- IndexDB\n","- URL-\\&gt;browser or server? callServer(url)\n","- json/geojson/xl/csv -\\&gt; tocsvformat -\\&gt; iscsv-\\&gt;stringreplace, isjson-\\&gt;papaunparse, isxl-\\&gt;readxlsx[0] -\\&gt;tocsv, isgeoj-\\&gt;json-\\&gt;papaunparce\n","- JSN.Parse at runtime is faster than inlining the data when 10KB\\&gt;\n","- Code Caching occurs when inlineJs \\&gt; 1KB\n","- V8 reduced parse/compilation by 40% using workerThreads\n","- /v8RawJS parse speed is 2x since chrome60"]},{"cell_type":"markdown","metadata":{"id":"Zf-5Rxy75biV"},"source":["Clear indexdb -\\&gt; readFile. Insert into IndexDB V.1.0\n"]},{"cell_type":"markdown","metadata":{"id":"uNaX_uaH5biV"},"source":["- jpl- sweet ontology\n","- Geoincubator group\n","- Rdf, qsparql, gml, kml"]}]}