{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OrkXWx8KlMXV"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lrnb0YeelMXY"},"source":["##### P2P"]},{"cell_type":"markdown","metadata":{"id":"Jn2n6zK2lMXc"},"source":["https://github.com/kgryte/awesome-peer-to-peer\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZvdCF3RClMXd"},"source":["###### GUN"]},{"cell_type":"markdown","metadata":{"id":"m1V2O-dllMXe"},"source":["GUN is a decentralized, offline-first, graph database. It provides an easy interface for developers to create apps that work offline, and that are synced automatically to other nodes once the app is connected to a coordinating server."]},{"cell_type":"markdown","metadata":{"id":"0SWpjwkIlMXf"},"source":["###### Signal-less"]},{"cell_type":"markdown","metadata":{"id":"nFbx5juHlMXf"},"source":["> **What is [signaling](https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/)?** Signaling is the process of coordinating communication. \n","\n","> [WebRTC without a signaling server ](https://blog.printf.net/articles/2013/05/17/webrtc-without-a-signaling-server/)   \n","  - I wanted to experiment with WebRTC and understand its datachannels better, and I also felt like the existing code examples I’ve seen are unsatisfying in a specific way: it’s a peer-to-peer protocol, but the first thing you do (for example, on sites like conversat.io) is have everyone go to the same web server to find each other (this is called “signaling” in WebRTC) and share connection information.\n","  - If we’re going to have a peer-to-peer protocol, can’t we use it without all visiting the same centralized website first? \n","  - Could we instead make a WebRTC app that just runs out of a file:/// path on your local disk, even if it means you have to manually tell the person you’re trying to talk to how to connect to you?\n","  - It turns out that we can: I’ve created a serverless-webrtc project on [GitHub]((https://github.com/cjb/serverless-webrtc/)) that decouples the “signaling server” exchange of connection information from the WebRTC code itself. To run the app:\n","> download Firefox Nightly.\n","git clone git://github.com/cjb/serverless-webrtc.git\n","load file:///path/to/serverless-webrtc/serverless-webrtc.html\n","You’ll be asked whether you want to create or join a channel, and then you’re prompted to manually send the first party’s “WebRTC offer” to the second party (for example, over an instant message chat) and then to do the same thing with the second party’s “WebRTC answer” reply back. Once you’ve done that, the app provides text chat and file transfer between peers, all without any web server. (A STUN server is still used to find out your external IP for NAT-busting.)\n","> https://github.com/lesmana/webrtc-without-signaling-server\n","> https://github.com/cjb/serverless-webrtc\n","> https://github.com/xem/miniWebRTC"]},{"cell_type":"markdown","metadata":{"id":"LzJ1OUfYlMXi"},"source":["###### Signaling > Stun Turn Ice NATS"]},{"cell_type":"markdown","metadata":{"id":"CxVEJQfmlMXj"},"source":["> A host uses **Session Traversal Utilities for NAT (STUN)** to discover its public IP address when it is located behind a NAT/Firewall. \n","\n","> When this host wants to receive an incoming connection from another party, it provides this public IP address as a possible location where it can receive a connection. \n","\n","> If the NAT/Firewall still won't allow the two hosts to connect directly, they make a connection to a server implementing **Traversal Using Relay around NAT (TURN)**, which will relay media between the two parties.\n","\n","> **Interactive Connectivity Establishment (ICE)** is a blanket standard that describes how to coordinate STUN and TURN to make a connection between hosts. Twilio's Network Traversal Service implements STUN and TURN for ICE-compatible clients, such as browsers supporting the WebRTC standard."]},{"cell_type":"markdown","metadata":{"id":"QX0S9epglMXl"},"source":["##### Misc Read"]},{"cell_type":"markdown","metadata":{"id":"3r29_mBplMXm"},"source":["[no code tools](https://www.testcraft.io/no-code-tools-application-development/)"]},{"cell_type":"markdown","metadata":{"id":"7sXRt-felMXn"},"source":["- How cybernetics connects computing, counterculture, and [design](http://www.dubberly.com/wp-content/uploads/2015/10/Cybernetics_and_Counterculture.pdf)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZUtEnOd_lMXo"},"source":["##### Innovation"]},{"cell_type":"markdown","metadata":{"id":"FKpfYPrZlMXo"},"source":["- [dougengelbart](https://www.dougengelbart.org/content/view/374/464/) website\n","\n","- https://news.ycombinator.com/item?id=23535853"]},{"cell_type":"markdown","metadata":{"id":"XsUv0C_HlMXp"},"source":["###### Andrei A. Markov "]},{"cell_type":"markdown","metadata":{"id":"Sojym10XlMXp"},"source":["THE FIVE GREATEST APPLICATIONS OF [MARKOV CHAINS](http://langvillea.people.cofc.edu/MCapps7.pdf)\n","\n","> 1. Markov’s own application of his chains to Alexander S. Pushkin’s poem “Eugeny Onegin.”\n","- In 1913, for the 200th anniversary of Jakob Bernoulli’s publication [4], Markov ... published < his third edition, in which > he writes, “Let us finish the article and the whole book with a good example of dependent trials, which approximately can be regarded as a simple chain.” In what has now become the famous first application of Markov chains\n","- , A. A. Markov, studied the\n","sequence of 20,000 letters in A. S. Pushkin’s poem “Eugeny Onegin,” and the sequence of 100,000 letters in S. T. Aksakov’s novel “The Childhood of\n","Bagrov, the Grandson.” He compared the probability of different distributions of\n","letters taken from the book with probabilities of sequences of vowels and consonants in term of his chains.\n","- Until that time,\n","the theory of probability ignored temporal aspects related to random events. Mathematically speaking,  dependent random events do not necessarily imply a temporal aspect. In contrast, a temporal aspect is fundamental in Markov’s chains. \n","- Markov’s novelty was the notion that a\n","random event can depend only on the most recent past. \n","- In doing so, Markov demonstrated to other scholars a method of accounting for time dependencies.\n","- This method was later applied to\n","the diffusion of gas molecules, Mendel’s genetic experiments, and the random walk\n","behavior of certain particles\n","- Morozov enthusiastically credited Markov’s method as a “new weapon\n","for the analysis of ancient scripts” ... - Markov found few of Morozov’s\n","experiments to be convincing. Markov, however, did mention that a more advanced\n","model and an extended set of data might be more successful at identifying an author solely by mathematical analysis of his writings\n","\n","> 2. C. E. Shannon’s Application to Information Theory.\n","- introduced “A Mathematical Theory of Communication” in 1948\n","- intention was to present a general framework for communication based on the principles of the new digital media. \n","-  Shannon’s information theory gives mathematically formulated answers to:\n","- - how an optimal utilization of a given bandwidth of a communication channel\n","could be ensured.\n","- - questions such as how analog signals could be transformed into digital ones\n","- - how digital signals then could be coded in such way that noise and interference would not do harm to the original message represented by such signals, \n","- This formula\n","is the entropy of a source of discrete events. In Shannon’s words this formula “gives\n","values ranging from zero—when one of the two events is certain to occur\n","\n","- - These situations correspond intuitively to the minimum information produced by a particular\n","event (when it is already certain what will occur) and the greatest information or the\n","greatest prior uncertainty of the event”\n","\n","- - It is evident that if something is known about a message beforehand, then the\n","receiver in a communication system should somehow be able to take advantage of\n","this fact.\n","\n","- - **Shannon suggested that any source transmitting data is a Markov process.**\n","\n","- - **This assumption leads to the idea of determining a priori the transition probabilities of communication symbols, i.e., the probability of a symbol following another symbol or group of symbols**\n","\n","- - If, for example, the information source consists of words of the\n","English language and excludes acronyms, then the transition probability of the letter\n","“u” following the letter “q” is 1.\n","\n","- It is significant that Kolmogorov himself contributed an extension to the theory of Markov chains by showing that “it is a matter of indifference which of the two following assumptions is made: either the time variable t runs through all real values, or only through the integers” [15].1 In other words, Kolmogorov made the theory suitable not only for discrete cases, but also for all kinds of physical applications that includes continuous cases. In fact, he explicitly mentioned Erwin Schr¨oding wave mechanics as an application for Markov chains.\n","\n","- , Shannon went beyond Markov’s\n","work with his information theory application. Shannon used Markov chains not solely as a method for analyzing stochastic events but also to generate such events.  Shannon’s first attempt at using Mar! kov chains to produce English sentences resulted in “IN NO IST LAT WHEY CRATICT FOURE BIRS GROCID”\n","\n","- - Shannon’s machine, it deserves credit as being the first computer to have implemented a Markov model\n","\n","- - French psychoanalyst Jacques Lacan introduced Markov chains as the underlying mechanism for explaining the process by which unconscious choices are made. Lacan hints that Shannon’s machine was the model for his theory\n","\n","- - , a group of influential linguists claimed that the modus operandi of language is a Markov process [12]. Such a general assumption provoked a controversial debate between Roman Jakobson and Noam Chomsky. Chomsky argued that language models based on Markov chains do not capture some nested structures of sentences, which are quite common in many languages such as English [10]. We now recognize that Chomsky’s account of the limitations of Markovian language models was too harsh.\n","\n","\n","> 3. A. L. Scherr’s Application to Computer Performance Evaluation.\n","\n","He used a continuous-time Markov chain to model M.I.T’s Compatible Time-Sharing\n","System. The chain not only added enough mathematics, it also led Scherr to a\n","surprising result. Scherr’s quick and dirty measure gave a very good approximation\n","to system performance. According to Scherr, this was surprising because “this very simple, analytic model that ignored 99 percent of the details of the system was just as accurate in predicting performance and capacity as the very e! laborate, very accurate simulation model” \n","- a simple Markov model came to within 4-5 percent accuracy of the very detailed simulation\n","-  Scherr discovered that system updates and upgrades are the primary reason for the superiority of the simplified analytical model over detailed simulation. The simple analytical models give a better return on investment. By the time a detailed simulation is finally complete, the system is usually updated, and a new simulation required. Also, Scherr found that the simple Markov models force designers to get to the heart of the system, because only the essential information can be modeled; all extraneous information must be identified and omitted\n","\n","> 4. L. E. Baum’s Application to Hidden Markov Models.\n","\n","- Today, now that speech recognition software is available off the shelf, it is no\n","longer a secret that, thanks to HMMs, “word spotting” in stream of spoken language\n","is done by a computer—even in the presence of noise. In addition, speech cannot only\n","be recognized algorithmically with great accuracy, but the individual voice of the\n","speaker can be identified as well. It stands to reason that such features are of great\n","interest, especially for intelligence services, in order to scan communication channels\n","for key words. Moreover, HMMs enable the extraction of significant information from\n","the acoustical pattern of a spoken language without requiring any semantic knowledge\n","of the language in question.\n","\n","- An HMM is valuable because it helps uncover this hidden information, or at least gives a reasonable approximation to it.\n","\n","- In the late 1960s and particularly the early 1970s, Leonard E. Baum, supported by the work of other scholars, demonstrated that the underlying model can be recovered from a sufficiently long observation sequence by an iterative procedure, which maximizes the probability of the observation sequence.\n","\n","- Instead of calculating the probability\n","of the observed sequence by finding each possible sequence of the hidden states and summing these probabilities, a short cut exists. It is called the forward algorithm and minimizes computational efforts from exponential growth to linear growth by calculating partial probabilities at every time step in a recursive manner \n","\n","- To summarize, hidden Markov models require the solution of some fundamental, interrelated problems of modeling and analyzing stochastic events. Several mathematicians (only a few are mentioned in this brief survey) defined the mainproblems and developed different techniques to solve them. Once tutorial material spread among scholars from other fields, HMMs quickly demonstrated their potential through real-life applications. For instance, in 1989 Gary A. Churchill used HMMs to separate genomic sequences into segments [11]. Since then, HMMs have become an off-the-shelf technology in biological sequence analysis\n","\n","> 5. S. Brin and L. Page’s Application to Web Search.\n","- By Brin and Page’s own admission, PageRank, which is the stationary vector of\n","an enormous Markov chain, is the driving force behind Google’s success in ranking\n","webpages. In the PageRank context, the web and its hyperlinks create an enormous\n","directed graph. Brin and Page’s vision of a web surfer taking a random walk on this\n","graph led to the formulation of the world’s largest Markov chain \n"]},{"cell_type":"markdown","metadata":{"id":"2J7xQ7JflMXr"},"source":["###### Ray [Solomonoff](https://en.wikipedia.org/wiki/Ray_Solomonoff)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tYs-fmbYlMXs"},"source":["Notes on the Scientific [Method](http://raysolomonoff.com/raysnotes/scimethodtranscript.pdf)\n","\n","Algorithmic Information [theory](https://en.wikipedia.org/wiki/Algorithmic_information_theory)"]},{"cell_type":"markdown","metadata":{"id":"YHZkIk5xlMXs"},"source":["##### How things work"]},{"cell_type":"markdown","metadata":{"id":"sLUVYJIKlMXs"},"source":["[Letters of a Radio-Engineer to His Son](https://www.gutenberg.org/files/30688/30688-h/30688-h.htm)\n","\n","Low Tech [thermoelectrics](https://www.lowtechmagazine.com/2020/05/thermoelectric-stoves-ditch-the-solar-panels.html)"]},{"cell_type":"markdown","metadata":{"id":"pAJVeEh_lMXt"},"source":["###### Neuro Biology"]},{"cell_type":"markdown","metadata":{"id":"82EmDH5xlMXt"},"source":["Quanta - [Wired to care](https://www.the-tls.co.uk/articles/conscience-patricia-churchland-book-review/) - Does neurobiology really explain everything?\n","\n","> Patricia Churchland’s reputation was established in 1986 with Neurophilosophy: Toward a unified science of the mind-brain. Its central claim was that mental processes are brain processes; from this it follows that if you want to understand the mind the best approach is to peer into the intracranial darkness to find out what the brain is up to. Traditional philosophizing about the mind is, or should be, dead.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6jNIP2YClMXu"},"source":["###### Music"]},{"cell_type":"markdown","metadata":{"id":"c-gNFEMqlMXu"},"source":["[Just_intonation](https://en.wikipedia.org/wiki/Just_intonation)\n","> pure intonation is the tuning of musical intervals as whole number ratios (such as 3:2 or 4:3) of frequencies.\n","\n","\n",">  jupyter notebook anyway where I try to start with biological principles and deduce the shape of the piano keyboard https://fiftysevendegreesofrad.github.io/JupyterNotes/piano.html\n","\n","[Hackernews - audio](https://news.ycombinator.com/item?id=23624596)\n","> Quite a bold title. My understanding (while limited) is that the human ear senses sound by means of tiny hairs in the inner ear that vibrate in resonance with incoming sound waves. I've always believed that the reason certain intervals are more harmonious to us than others was because there would be some overlap in hairs that resonated for the different harmonious pitches. The same way guitar strings will resonate with other strings that are tuned an octave, or perfect fifth, or fourth higher, these ear-hairs ought to behave by the same physical laws and principles.\n","\n","> I feel looking for musical scores in peoples brains is a great place to start decoding. We clearly load data into a conscious signal somehow like CAN-bus but it must also be stored maybe on actin filaments in the dendritic spines as loops we load in? Also mentally we can change the music and add any number of instruments over the top change the tune and deviate the course of the melody, add vocal harmonies from crowds of people and violins etc. so where does that part happen?, in the signal?. can it be 'heard'? . and is the music 'composer' happening in the same part of the brain as the language generating part or is it just similar capabilities in a different part of the brain?. Decoding must be possible. I feel if they had like millions of brains all wired up to computers that were using AI to find match patterns . i.e. audio patterns from songs in brains data. they could brute force it? Is getting people to sing stuff and making assumptions weak science?\n","\n","> Having a harmonic at twice the frequency is not arbitrary. Lots of things that vibrate will have a fundamental frequency and harmonics at integer ratios. The name \"octave\" implying 8 is a bit more complicated, but if you just treat it as a name for a moment, the \"octave\" is the ratio 2:1. Another common ratio is 3:2, and again, if you ignore the meaning of the name, it's called the \"fifth\".\n","So why 8 notes per octave? Well, it's really 12 notes per octave. Maybe think of it as the white and black keys on a piano. 8 white keys gets you back to where you were, but you skipped some black keys along the way. So why 12 notes per octave? Well, that's because (3/2)^12 (1.5 to the 12th power) is almost a power of 2. So if you step up by fifths 12 times, you very nearly land 7 octaves up. Each of the notes you stepped on along the way becomes one of the 12. Heh, there are also \"fourths\" at a ratio of 4:3. So music has a \"fourth\" plus a \"fifth\" equals an \"octave\"! It's kind of silly :-) Anyways, that's the quick version. If you go further down this road, there are \"wolf fifths\", various tunings with subtle (but perceptible) differences, and you can even find 19 and 31 tone scales. It kind of goes on and on.\n","\n",">Mostly because primitive flutes and horns historically had those seven notes (plus octave) [1], or they sounded good when sung or played in sequence or together. It \"just made sense\" to stumble upon them, because of the harmonic series [2].\n","Some intervals were more obvious (octave, perfect fifth, major third). Others probably took hundreds or thousands of years to be discovered. But keep in mind that different cultures used different scales. It wasn't always the same. Some had minor seventh, others major seventh. Mesopotamians used a sharp fourth. This is still visible in different cultures today (blues/rock uses a lot of flat seventh!). As for \"why seven?\": Since \"mixing\" major/minor sevenths/thirds/etc is very dissonant and weird, people ended up having seven notes regardless. I would say that \"note choice\" was more cultural, but the options were obviously influenced by the harmonic series. After a while people started seeing patterns and those historical scales converged into the major scale we know today. Later in the 1500s some geniuses found a way to transpose the scales but still maintain the ratio between notes, but without having to retune the instrument. The trick was to divide the octave in 12 notes but only use seven at a time [3]. It wasn't \"perfect\" like just intonation [4], but it was in the ballpark. That became the new normal. Equal temperament is not perfectly in tune with the harmonic series, but people got used to it (to the point that just intonation sounds \"off\" to a lot of musicians).\n","\n","> An octave is the interval between one musical pitch and another with double its frequency, which isn’t arbitrary; however the number of notes within an octave (and, I think, their exact tuning) is arbitrary. \n","\n","> To elaborate, intervals in the harmonic series are not arbitrary because the frequencies of the two notes forms a ratio:\n","  2:1 octave [1]\n","  3:2 perfect fifth [2]\n","  4:3 perfect fourth [3]\n","\n","\n","> If you play two sine wave tones of different frequencies at the same time you get beats[0] caused by the alternating constructive and destructive interference.\n","Helmholtz hypothesized[1] that the dissonance of a pair of sine wave tones was related to these beats. Slow beats sound like a pleasant vibrato effect. Extremely fast beats are not perceived as beats at all, with only two separate tones heard. Only moderately fast beats sound dissonant. This was confirmed experimentally[2] by Plomp and Levelt. Most Western musicals instruments are harmonic or approximately harmonic[4]. They produce a waveform with partials of frequencies that are an integer multiple of the lowest frequency partial (called the \"fundamental\"). Increasing pitch by an octave doubles the frequency of all partials. An integer multiplied by two is still an integer, so if you play harmonic notes separated by octaves the partials will overlap. All pairs of partials will be either identical or far apart, so none form dissonant beats. This maximizes consonance. But music with only octave intervals would be very boring, so the octave in standard Western music theory is divided into 12 equal parts. This is an excellent choice for harmonic instruments, because it closely approximates several small-integer ratios. The interval of a \"fifth\" (actually seven steps away in the octave, but music theory uses strange numbering to simplify playing the most common musical styles) is a frequency ratio of 3:2. This results in half the partials overlapping, and the other half still being positioned so they avoid dissonant beats, so the fifth is also highly consonant.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ssE6R3SflMXw"},"source":["##### Conceptual"]},{"cell_type":"markdown","metadata":{"id":"nZLx8YDqlMXx"},"source":["How to [Disagree](http://www.paulgraham.com/disagree.html)\n","\n","https://qualiacomputing.com/2020/06/"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}