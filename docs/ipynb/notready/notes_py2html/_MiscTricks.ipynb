{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA = Eigenvector of a Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlation: {df[column1].corr(df[column2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[column].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8ecf0180324190a6fc3442d4fd85b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='column1', options=('claps', 'days_since_publication', 'fans', 'num…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def correlations(column1=list(df.select_dtypes('number').columns), \n",
    "                 column2=list(df.select_dtypes('number').columns)):\n",
    "    print(f\"Correlation: {df[column1].corr(df[column2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb#scrollTo=LK8dQnn2_U7_\n",
    "\n",
    "https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb\n",
    "    \n",
    "https://colab.research.google.com/notebooks/widgets.ipynb#scrollTo=QKk_E6-QRVPW\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b108e98598f542339f3b101adcbe1e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='column', options=('claps', 'days_since_publication', 'fans', 'link…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def describe(column=list(df.columns)):\n",
    "    print(df[column].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.w3schools.com/python/python_classes.asp\n",
    "# All classes have a function called __init__(), which is always executed when the class is being initiated.\n",
    "# The self parameter is a reference to the current instance of the class, and is used to access variables that belongs to the class.\n",
    "# It does not have to be named self , you can call it whatever you like, but it has to be the first parameter of any function in the class:\n",
    "  \n",
    "# For each column \n",
    "## Analyze column.\n",
    "\n",
    "# Bin Numeric Columns\n",
    "# Melt every single categorical columns (included Bins -> issues?) \n",
    "# Dummy Encode Boolean Values\n",
    "  \n",
    "class ColumnInfo:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "    \n",
    "  def myfunc(self):\n",
    "    print(\"Hello my name is \" + self.name)  \n",
    "    \n",
    "p1 = ColumnInfo(\"John\", 36)\n",
    "\n",
    "print(p1.name)\n",
    "print(p1.age)\n",
    "p1.myfunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.select_dtypes('number').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "root_dir = '../'\n",
    "dirs = [d for d in os.listdir(root_dir) if not '.' in d]\n",
    "subprocess.check_output(f\"cd {root_dir}{dir} && ls -a -t -r -l -h\", shell=True).decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this function, we use a `Dropdown` and a `DatePicker` to plot one column cumulatively up to a certain time. Instead of having to write this ourselves, we can just let `ipywidgets` do all the work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_up_to(column, date):\n",
    "    date = pd.Timestamp(date)\n",
    "    plot_df = df.loc[df.index <= date].copy()\n",
    "    plot_df[column].cumsum().iplot(mode='markers+lines', \n",
    "                                   xTitle='published date',\n",
    "                                   yTitle=column, \n",
    "                                  title=f'Cumulative {column.title()} Until {date.date()}')\n",
    "    \n",
    "_ = interact(plot_up_to, column=widgets.Dropdown(options=list(df.select_dtypes('number').columns)), \n",
    "             date = widgets.DatePicker(value=pd.to_datetime('2019-01-01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab Tricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now (for fun) I want to list out all the geojson files in the folder\n",
    "for file in os.listdir(\"./\"):\n",
    "    if file.endswith(\".geojson\"):\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how one would use addPath() to import an external module named 'retrieve_acs_data'\n",
    "file = 'retrieve_acs_data.py'\n",
    "addPath( '../../', file)\n",
    "from retrieve_acs_data import retrieve_acs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics Notes\n",
    "\n",
    "https://www.textbook.ds100.org\n",
    "\n",
    "http://www.statsoft.com/Textbook/Nonparametric-Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVERVIEW \n",
    "\n",
    "Dataset info\n",
    "\n",
    "- Number of variables\t11\n",
    "- Number of observations\t2410\n",
    "- Total Missing (%)\t4.0%\n",
    "- Total size in memory\t225.9 KiB\n",
    "- Average record size in memory\t96.0 B\n",
    "\n",
    "Variables types\n",
    "\n",
    "- Numeric\t5\n",
    "- Categorical\t5\n",
    "- Date\t0\n",
    "- Text (Unique)\t0\n",
    "- Rejected\t1\n",
    "\n",
    "Warnings\n",
    "\n",
    "- abv has 62 / 2.6% missing values Missing\n",
    "- city has a high cardinality: 384 distinct values Warning\n",
    "- ibu has 1005 / 41.7% missing values Missing\n",
    "- id_brewery is highly correlated with brewery_id (ρ = 1) Rejected\n",
    "- name_beer has a high cardinality: 2305 distinct values Warning\n",
    "- name_brewery has a high cardinality: 551 distinct values Warning\n",
    "- state has a high cardinality: 51 distinct values Warning\n",
    "- style has a high cardinality: 100 distinct values Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Profiling Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERVIEW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset info\n",
    "\n",
    "- Number of variables\t11\n",
    "- Number of observations\t2410\n",
    "- Total Missing (%)\t4.0%\n",
    "- Total size in memory\t225.9 KiB\n",
    "- Average record size in memory\t96.0 B\n",
    "\n",
    "Variables types\n",
    "\n",
    "- Numeric\t5\n",
    "- Categorical\t5\n",
    "- Date\t0\n",
    "- Text (Unique)\t0\n",
    "- Rejected\t1\n",
    "\n",
    "Warnings\n",
    "\n",
    "- abv has 62 / 2.6% missing values Missing\n",
    "- city has a high cardinality: 384 distinct values Warning\n",
    "- ibu has 1005 / 41.7% missing values Missing\n",
    "- id_brewery is highly correlated with brewery_id (ρ = 1) Rejected\n",
    "- name_beer has a high cardinality: 2305 distinct values Warning\n",
    "- name_brewery has a high cardinality: 551 distinct values Warning\n",
    "- state has a high cardinality: 51 distinct values Warning\n",
    "- style has a high cardinality: 100 distinct values Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARIABLES Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abv: Numeric\n",
    "\n",
    "Statistics | Histogram | Common Values | Extreme Values\n",
    "\n",
    "- Distinct count\t75\n",
    "- Unique (%)\t3.2%\n",
    "- Missing (%)\t2.6%\n",
    "- Missing (n)\t62\n",
    "- Infinite (%)\t0.0%\n",
    "- Infinite (n)\t0\n",
    "- Mean\t0.059773\n",
    "- Minimum\t0.001\n",
    "- Maximum\t0.128\n",
    "- Zeros (%)\t0.0%\n",
    "\n",
    "Statistics:\n",
    "\n",
    "Quantile statistics:\n",
    "- Minimum\t8.4\n",
    "- 5-th percentile\t12\n",
    "- Q1\t12\n",
    "- Median\t12\n",
    "- Q3\t16\n",
    "- 95-th percentile\t16\n",
    "- Maximum\t32\n",
    "- Range\t23.6\n",
    "- Interquartile range\t4\n",
    "\n",
    "Descriptive statistics:\n",
    "- Standard deviation\t2.3522\n",
    "- Coef of variation\t0.17305\n",
    "- Kurtosis\t9.04\n",
    "- Mean\t13.592\n",
    "- MAD\t2.0194\n",
    "- Skewness\t2.0467\n",
    "- Sum\t32757\n",
    "- Variance\t5.5329\n",
    "- Memory size\t37.7 KiB\n",
    "\n",
    "Common Values: Value\tCount\tFrequency (%)\t \n",
    "\n",
    "Extreme Values: Minimum 5 values ( Value\tCount\tFrequency (%) )\t Maximum 5 values ( Value\tCount\tFrequency (%) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VARIABLES \n",
    "\n",
    "abv: Numeric\n",
    "\n",
    "Statistics | Histogram | Common Values | Extreme Values\n",
    "\n",
    "- Distinct count\t75\n",
    "- Unique (%)\t3.2%\n",
    "- Missing (%)\t2.6%\n",
    "- Missing (n)\t62\n",
    "- Infinite (%)\t0.0%\n",
    "- Infinite (n)\t0\n",
    "- Mean\t0.059773\n",
    "- Minimum\t0.001\n",
    "- Maximum\t0.128\n",
    "- Zeros (%)\t0.0%\n",
    "\n",
    "Statistics:\n",
    "\n",
    "Quantile statistics:\n",
    "- Minimum\t8.4\n",
    "- 5-th percentile\t12\n",
    "- Q1\t12\n",
    "- Median\t12\n",
    "- Q3\t16\n",
    "- 95-th percentile\t16\n",
    "- Maximum\t32\n",
    "- Range\t23.6\n",
    "- Interquartile range\t4\n",
    "\n",
    "Descriptive statistics:\n",
    "- Standard deviation\t2.3522\n",
    "- Coef of variation\t0.17305\n",
    "- Kurtosis\t9.04\n",
    "- Mean\t13.592\n",
    "- MAD\t2.0194\n",
    "- Skewness\t2.0467\n",
    "- Sum\t32757\n",
    "- Variance\t5.5329\n",
    "- Memory size\t37.7 KiB\n",
    "\n",
    "Common Values: Value\tCount\tFrequency (%)\t \n",
    "\n",
    "Extreme Values: Minimum 5 values ( Value\tCount\tFrequency (%) )\t Maximum 5 values ( Value\tCount\tFrequency (%) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARIABLES Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "style: Categorical\n",
    "\n",
    "- Distinct count\t100\n",
    "- Unique (%)\t4.2%\n",
    "- Missing (%)\t0.2%\n",
    "- Missing (n)\t5\n",
    "- Value\tCount\tFrequency (%)\n",
    "\n",
    "CORRELATIONS (NUMERIC ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "style: Categorical\n",
    "\n",
    "- Distinct count\t100\n",
    "- Unique (%)\t4.2%\n",
    "- Missing (%)\t0.2%\n",
    "- Missing (n)\t5\n",
    "- Value\tCount\tFrequency (%)\n",
    "\n",
    "CORRELATIONS (NUMERIC ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Up Your Exploratory Data Analysis With Pandas-Profiling\n",
    "https://towardsdatascience.com/speed-up-your-exploratory-data-analysis-with-pandas-profiling-88b33dc53625\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/python-data-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Length\n",
    "The len function counts the number of observations in a Series. The function will count all observations, regardless if there are missing or null values.\n",
    "\n",
    "length = len(beers[\"ibu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count\n",
    "The count function will return the number of non-NA/non-null observations in a Series.\n",
    "\n",
    "count = beers[\"ibu\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing Values\n",
    "With the Length and the Count, we are now able to calculate the number of missing values. The number of missing values is the difference between the Length and the Count.\n",
    "\n",
    "number_of_missing_values = length - count\n",
    "pct_of_missing_values = float(number_of_missing_values / length)\n",
    "pct_of_missing_values = \"{0:.1f}%\".format(pct_of_missing_values*100)\n",
    "print(pct_of_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minimum/Maximum Value\n",
    "The minimum and maximum value of a dataset can easily be obtained with the min and max function on a Series.\n",
    "\n",
    "print(\"Minimum value: \", beers[\"ibu\"].min())\n",
    "print(\"Maximum value: \", beers[\"ibu\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantile Statistics\n",
    "Quantiles are cut points that split a distribution in equal sizes. Many quantiles have their own name. If you split a distribution into four equal groups, the quantile you created is named quartile. You can easily create quantile using the quantile function on a Series. You can pass to that function an array with the different quantiles to compute. In the case below, we want to split our distribution in four equal groups.\n",
    "\n",
    "quantile = beers[\"ibu\"].quantile([.25, .5, .75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we can’t talk about data profiling without mentioning the importance of a frenquency-distribution plot. It is one of the simplest yet most powerful visualization. It demonstrates the frequency of each value in our dataset.\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "\n",
    "sns.distplot(beers[\"ibu\"].dropna());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correlations\n",
    "Correlations are a great way to discover relationships between numerical variables. There are various ways to calculate the correlation. The Pearson correlation coefficient is a widely used approach that measures the linear dependence between two variables. The correlation coefficient ranges from -1 to 1. A correlation of 1 is a total positive correlation, a correlation of -1 is a total negative correlation and a correlation of 0 is non-linear correlation. We can perform that calculation using the corr function on a Series. By default, this function will use the Pearson correlation coefficient calculation. It is possible to use different methods of calculation with this function.\n",
    "\n",
    "beers[[\"abv\", \"ibu\", \"ounces\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html\n",
    "\n",
    "# Specifying na to be False instead of NaN replaces NaN values with False. \n",
    "# If Series or Index does not contain NaN values the resultant dtype will be bool, otherwise, an object dtype.\n",
    "# >>> s1.str.contains('house|dog', regex=True)\n",
    "# >>> import re\n",
    "# >>> s1.str.contains('PARROT', flags=re.IGNORECASE, regex=True)\n",
    "# Returning any digit using regular expression.\n",
    "# >>> s1.str.contains('\\d', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATISTICS\n",
    "\n",
    "# Compute pairwise correlation of columns, excluding NA/null values.\n",
    "# DataFrame.corr(method='pearson', min_periods=1)[source]\n",
    "# method : {‘pearson’, ‘kendall’, ‘spearman’} \n",
    "df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out the records based on two conditions using the AND Operator: train[(train['Employment.Type'] == \"Salaried\") & (train['branch_id'] == 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Python None will arise and we wish to also consider that “missing” or “not available” or “NA”.\n",
    "# Note If you want to consider inf and -inf to be “NA” in computations, you can set pandas.options.mode.use_inf_as_na = True.\n",
    "isna() and notna()\n",
    "numeric containers will always use NaN regardless of the missing value type chosen:\n",
    "Missing values propagate naturally through arithmetic operations between pandas objects.\n",
    "When summing data, NA (missing) values will be treated as zero.\n",
    "If the data are all NA, the result will be 0.\n",
    "Cumulative methods like cumsum() and cumprod() ignore NA values by default,\n",
    "but preserve them in the resulting arrays. To override this behaviour and include NA values, use skipna=False.\n",
    "\n",
    "address = pointData[(pointData['address'] == False) | (pointData['address'] == '') | (pointData['address'] == None)]\n",
    "address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get File Types By Type\n",
    "types = df.dtypes\n",
    "bools = df.select_dtypes(include='bool')\n",
    "ints = df.select_dtypes(include=['float','integer'] )\n",
    "df.ftypes\n",
    "df.get_dtype_counts()\n",
    "df.get_ftype_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.isna().sum()\n",
    "df = df.dropna(axis = 0, how = 'any') # drop rows with any missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Python Pandas tricks to make data analysis more enjoyable\n",
    "\n",
    "https://towardsdatascience.com/10-python-pandas-tricks-to-make-data-analysis-more-enjoyable-cb8f55af8c30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the **count of unique values**: train['loan_default'].value_counts()\n",
    "\n",
    "To get the **list & number of unique values**: train['branch_id'].nunique()\n",
    "\n",
    "Filtering based on Conditions: train[(train['Employment.Type'] == \"Salaried\")]\n",
    "\n",
    "**filter** out the records based **on two conditions** using the AND  Operator: train[(train['Employment.Type'] == \"Salaried\") & (train['branch_id'] == 100)]\n",
    "\n",
    "**Finding null values**: train.apply(lambda x: sum(x.isnull()),axis=0)\n",
    "\n",
    "**Data Type Conversion** using to_datetime() and astype() methods:\n",
    "\n",
    "Example of **to_datetime()**: train['Date.of.Birth']= pd.to_datetime(train['Date.of.Birth'])\n",
    "\n",
    "\n",
    "Example of **astype()**: train['ltv'] = train['ltv'].astype('int64')\n",
    "\n",
    "\n",
    "** Histograms ** : train['ltv'].hist(bins=25)\n",
    "\n",
    "train['asset_cost'].hist(bins=200)\n",
    "\n",
    "** Box Plots **:\n",
    "\n",
    "print(train.boxplot(column='disbursed_amount'))\n",
    "\n",
    "train.boxplot(column=’disbursed_amount’, by = ‘Employment.Type’)\n",
    "\n",
    "sns.boxplot(x=train['asset_cost'])\n",
    "\n",
    "**Count Plots**: sns.countplot(train.loan_default)\n",
    "\n",
    "sns.countplot(train.manufacturer_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/python-data-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correlations\n",
    "Correlations are a great way to discover relationships between numerical variables. There are various ways to calculate the correlation. The Pearson correlation coefficient is a widely used approach that measures the linear dependence between two variables. The correlation coefficient ranges from -1 to 1. A correlation of 1 is a total positive correlation, a correlation of -1 is a total negative correlation and a correlation of 0 is non-linear correlation. We can perform that calculation using the corr function on a Series. By default, this function will use the Pearson correlation coefficient calculation. It is possible to use different methods of calculation with this function.\n",
    "\n",
    "beers[[\"abv\", \"ibu\", \"ounces\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = [\"n/a\", \"na\", \"--\"]\n",
    "dbd = pd.read_csv('dashboardData.csv', na_values = missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any unruly characters\n",
    "dbd = dbd[dbd.columns].replace({'\\$': '', ',': ''}, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CSA as our Index\n",
    "dbd.set_index('CSA2010', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Columns by Dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dbd.select_dtypes(include=['object']).columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For relative imports to work in Python 3.6 \n",
    "import os, sys; \n",
    "os.path.realpath('./') \n",
    "sys.path.append(os.path.dirname(os.path.realpath('/gdrive/My Drive/colabs/Exploratory Analysis/utils/merge.py'))) \n",
    "sys.path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'merge' from '/gdrive/My Drive/colabs/Exploratory Analysis/utils/merge.py'>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import merge\n",
    "from merge import merge as mg\n",
    "from importlib import reload\n",
    "reload(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture #supress_output\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# pd.set_option('display.expand_frame_repr', False)\n",
    "# pd.set_option('display.precision', 2)\n",
    "pd.set_option('max_colwidth', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will just beautify the output\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.precision', 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
