{"meta":{"filename":"distributions_and_regression_tests"},"content":"<p><a href=\"http://www.statsoft.com/Textbook/Nonparametric-Statistics\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Nonparametric-Statistics</a></p>\n<p> Correlation Research</p>\n<ul>\n<li><p>Relations between Variables </p></li>\n<li><p>Magnitude</p></li>\n<li><p>Reliability</p></li>\n<li><p>PValue,</p></li>\n<li><p>Small relations can only be proven in large samples</p></li>\n</ul>\n<p> Test for Normality </p>\n<ul>\n<li><p>Anova/Manova, </p></li>\n<li><p>Nonparametrics</p></li>\n<li><p>Test Statistics - Normal(T, F, Chi-Square)</p></li>\n</ul>\n<p> <strong>Differences between independent groups.</strong>\n -Normal Distributions \n -&amp;gt; T test for Independent Samples</p>\n<p> T Test Alternative for Independent Nonparametrics samples </p>\n<ul>\n<li><p>the Mann-Whitney U test</p></li>\n<li><p>the Kolmogorov-Smirnov two-sample test and Wald-Wolfowitz</p></li>\n</ul>\n <p><a href=\"https://en.wikipedia.org/wiki/Partition_of_sums_of_squares\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Partition of variance</strong></a> </p>\n<ul>\n<li><p>Analysis of variance <a href=\"https://en.wikipedia.org/wiki/Analysis_of_variance\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">(ANOVA)</a></p></li>\n<li><p>Analysis of <a href=\"https://en.wikipedia.org/wiki/Analysis_of_covariance\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">covariance</a></p></li>\n<li><p>Multivariate <a href=\"https://en.wikipedia.org/wiki/Multivariate_analysis_of_variance\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">ANOVA</a></p></li>\n</ul>\n <p> <strong>Differences between dependent groups.</strong></p>\n<p> Normal Distributions </p>\n<ul>\n<li><p>T-Test for Dependent Samples</p></li>\n</ul>\n<p> Nonparametric alternatives to this test are </p>\n<ul>\n<li><p>the <em>Sign</em> test and </p>\n</li>\n<li><p><em>Wilcoxon&#39;s matched pairs</em> test. </p>\n</li>\n<li><p>If the variables of interest are dichotomous in nature (i.e., &quot;pass&quot; vs. &quot;no pass&quot;) then McNemar&#39;s <a href=\"http://www.statsoft.com/textbook/statistics-glossary/c.aspx?button=c#Chi-square%20Distribution\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Chi-square</a> test is appropriate.</p>\n</li>\n</ul>\n<p> If there are more than two variables that were measured in the same sample</p>\n<ul>\n<li><p>then we would customarily use repeated measures ANOVA.</p></li>\n</ul>\n<p> Nonparametric alternatives to this method are </p>\n<ul>\n<li><p>Friedman&#39;s two-way analysis of variance </p></li>\n<li><p>and Cochran Q test (if the variable was measured in terms of categories, e.g., &quot;passed&quot; vs. &quot;failed&quot;). </p></li>\n<li><p>Cochran Q is particularly useful for measuring changes in frequencies (proportions) across time</p></li>\n</ul>\n<p> _ <strong>Relationships between variables.</strong></p>\n<p> To express a relationship between two variables one usually computes the correlation coefficient.</p>\n<p>  Nonparametric equivalents to the standard correlation coefficient are_</p>\n<ul>\n<li><p><a href=\"http://www.statsoft.com/textbook/statistics-glossary/s.aspx?button=s#Spearman%20R\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>Spearman R</em></a><em>,</em></p></li>\n<li><p><a href=\"http://www.statsoft.com/textbook/statistics-glossary/k.aspx?button=k#kendall%20Tau\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>Kendall Tau</em></a><em>, and</em></p></li>\n<li><a href=\"http://www.statsoft.com/textbook/statistics-glossary/g.aspx?button=g#gamma%20coefficient\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>coefficient Gamma</em></a> \n <em>(see</em><a href=\"http://www.statsoft.com/textbook/nonparametric-statistics/#correlations\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>Nonparametric correlations</em></a>_).</li>\n</ul>\n<p> If the two variables of interest are categorical in nature (e.g., &quot;passed&quot; vs. &quot;failed&quot; by &quot;male&quot; vs. &quot;female&quot;) appropriate nonparametric statistics for testing the relationship between the two variables are </p>\n<ul>\n<li><p>the Chi-square test, </p></li>\n<li><p>the Phi coefficient, </p></li>\n<li><p>and the Fisher exact test.</p></li>\n</ul>\n<p> In addition, a simultaneous test for relationships between multiple cases is available: </p>\n<ul>\n<li><p>Kendall coefficient of concordance.</p></li>\n</ul>\n<p> This test is often used for expressing inter-rater agreement among independent judges who are rating (ranking) the same stimuli_</p>\n <p><a href=\"http://en.wikipedia.org/wiki/Non-parametric_statistics\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Non-parametric statistics</strong></a></p>\n<p> In several mostly elementary situations when the assumptions of parametric tests cannot be met, one may resort to non-parametric tests rather than parametric tests such as the <em>t</em>-test, the Pearson correlation test, analysis of variance, etc. In such situations the power of the non-parametric or distribution-free tests is often as good as the parametric ones or better. It often is a good idea to use both types of tests if they are available and compare the resulting <em>p</em>-values. If these values are roughly the same there is little to worry about, if they are different there is something to be sorted out.</p>\n <p>_ <strong>Nonparametric Correlations</strong> _</p>\n<p> <em>The following are three types of commonly used nonparametric correlation coefficients (</em><a href=\"http://www.statsoft.com/textbook/statistics-glossary/s.aspx?button=s#Spearman%20R\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>Spearman R</em></a><em>,</em><a href=\"http://www.statsoft.com/textbook/statistics-glossary/k.aspx?button=k#kendall%20Tau\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>Kendall Tau</em></a><em>, and</em><a href=\"http://www.statsoft.com/textbook/statistics-glossary/g.aspx?button=g#gamma%20coefficient\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><em>Gamma coefficients</em></a><em>).\n  Violating Normality Assumptions has less grave consequences than previously thought although this can only be proven on a case by case basis.</em></p>\n <ul>\n<li><p>** Normal vs Nonparametric Methods</p></li>\n<li><p>Distribution Tables (Z T Chi-Square F-Tables)</p></li>\n<li><p>Charting Difference Between (F/T Tests, Anova/Ancova, T-Test/ Anova, 1 vs 2-way Anova )</p></li>\n</ul>\n <p><a href=\"https://en.wikipedia.org/wiki/Correlation_and_dependence\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Correlation</strong></a></p>\n<ul>\n<li><p>Pearson product-<a href=\"https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">moment</a></p></li>\n<li>Partial <a href=\"https://en.wikipedia.org/wiki/Partial_correlation\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">correlation</a>\n -Confounding <a href=\"https://en.wikipedia.org/wiki/Confounding\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">variable</a></li>\n<li><p>Coefficient of <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">determination</a></p></li>\n</ul>\n <p>* stats.zscore(df)</p>\n<p> Two types Major types of Statistics: Inferential and Descriptive. Inferential statistics use more complex calculations to infer trends and make predictions/assumptions. A descriptive analysis should be performed first before moving into inferencing.</p>\n <p>First and foremost show of the descriptives <strong>Confidence intervals</strong> on scatter plots/ pearson coefficients/ linear regression</p>\n <p> Group by group -total X&#39;s in Y</p>\n<p> The Chi-Square test helps you determine if two discrete variables are associated</p>\n<p> Too much overlapping in the x-axis labels renders the whole plot useless.</p>\n <p> <a href=\"https://blog.socialcops.com/academy/resources/cross-tabulation-how-why/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://blog.socialcops.com/academy/resources/cross-tabulation-how-why/</a></p>\n<p> &quot;Also known as contingency tables or cross tabs, cross tabulation groups variables to understand the correlation between different variables. It also shows how correlations change from one variable grouping to another. It is usually used in statistical analysis to find patterns, trends, and probabilities within raw data.&quot;&quot;Cross tabulation is usually performed on <a href=\"http://www.stat.yale.edu/Courses/1997-98/101/catdat.htm\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">categorical data</a> â€” data that can be divided into mutually exclusive groups.&quot;</p>\n<p> <a href=\"http://blog.minitab.com/blog/understanding-statistics/using-cross-tabulation-and-chi-square-the-survey-says\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">http://blog.minitab.com/blog/understanding-statistics/using-cross-tabulation-and-chi-square-the-survey-says</a></p>\n<p> &quot;The Chi-Square test helps you determine if two discrete variables are associated. If there&#39;s an association, the distribution of one variable will differ depending on the value of the second variable. But if the two variables are independent, the distribution of the first variable will be similar for all values of the second variable.&quot;</p>\n<p> Cross tabulation:</p>\n<ol>\n<li><p>preprocess?</p>\n</li>\n<li><p>figure out which 2 features you want to crosstab</p>\n</li>\n<li><p>Create a column for every category of feature 1</p>\n</li>\n<li><p>Determin how you will aggregate (sum,count,avg,max,min,product)</p>\n</li>\n<li><p>create a record for every unique feature 1</p>\n</li>\n</ol>\n<p> 5.1) create a column for the unique labels name</p>\n<ol start=\"6\">\n<li><p>aggregate feature 1 columns</p></li>\n</ol>\n <p><strong>Cross tabulation</strong> :</p>\n<ol>\n<li><p>preprocess?</p></li>\n<li><p>figure out which 2 features you want to crosstab</p></li>\n<li><p>Create a column for every category of feature 1</p></li>\n<li><p>Determine how you will aggregate (sum,count,avg,max,min,product)</p></li>\n<li>create a record for every unique feature 1\n  5.1) create a column for the unique labels name</li>\n<li><p>aggregate feature 1 columns</p></li>\n</ol>\n <p><a href=\"http://en.wikipedia.org/wiki/T-test\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Comparison of means:</strong></a><a href=\"http://en.wikipedia.org/wiki/T-test\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">_ <strong>t</strong> _</a><a href=\"http://en.wikipedia.org/wiki/T-test\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>-test</strong></a></p>\n<p> The <em>t</em>-test is used in many ways in statistics. The more common uses are (1) comparing one mean with a known mean, (2) testing whether two means are distinct, (3) testing whether the means from matched pairs are equal. Also called Student&#39;s <em>t</em> test (equal variances) or Welch&#39;s <em>t</em> test (unequal variances). Applicable for means from one sample, two independent samples, and paired samples. See analysis of variance for comparing more than two means. The <em>t</em>-test is also used in other contexts.</p>\n<p> <a href=\"http://en.wikipedia.org/wiki/Correlation\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Correlation</strong></a></p>\n<p> The (Pearson) correlation coefficient is a measure of the strength of the <em>linear</em> relationship between two interval or numeric variables. Other correlation coefficients exist to measure the relationship between ordinal two variables, such the Spearman&#39;s rank correlation coefficient. The highest value of the correlation coefficient is 1 or -1 (perfect relationship), the lowest value 0 (no relationship). The <em>t</em>-test is used to test whether a sample Pearson correlation differs from 0.</p>\n<p> <a href=\"http://en.wikipedia.org/wiki/Chi-squared_test\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Chi-square tests</strong></a></p>\n<p> The (Pearson) chi-square coefficient is primarily used with one or two categorical variables. The coefficient is a measure of difference between observed and expected scores.</p>\n<p> <em>One categorical variable</em>: In case of one categorical variable the test measures whether the observed values can reasonably come from a known distribution (or model). In other words the observed values are compared to the expected values from this known distribution. In such cases the test is primarily used for model testing.</p>\n<p> <em>Two categorical variables</em>: In case of two categorical variables the expected values usually are the values under the nulhypothesis that there is no relationship between the two variables. Therefore the chi-square coefficient of two variables is a measure of relationship.</p>\n<p> The chi-square coefficient is tested by comparing it with the chi-square distribution given the degrees of freedom. Other coefficients to measure the relationship between two variables in two-way contingency tables exist as well (for a list, see for instance the SPSS output with Crosstabs).</p>\n<p> Note that if possible exact <em>p</em>-values are preferred over the standard (asymptotic) ones.</p>\n<p> <a href=\"http://en.wikipedia.org/wiki/Non-parametric_statistics\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>Non-parametric statistics</strong></a></p>\n<p> In several mostly elementary situations when the assumptions of parametric tests cannot be met, one may resort to non-parametric tests rather than parametric tests such as the <em>t</em>-test, the Pearson correlation test, analysis of variance, etc. In such situations the power of the non-parametric or distribution-free tests is often as good as the parametric ones or better. It often is a good idea to use both types of tests if they are available and compare the resulting <em>p</em>-values. If these values are roughly the same there is little to worry about, if they are different there is something to be sorted out.</p>\n<p> Unfortunately, appropriate non-parametric techniques are not available for all comparable parametric techniques (see, however, the method <a href=\"http://three-mode.leidenuniv.nl/mtl/mtl_smc3_selectingmethods.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">selection charts</a> for comparable tests).</p>\n<p> Amazing Graphics\n <a href=\"https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/</a></p>\n <p>The <strong>Spearman correlation</strong> evaluates the monotonic <strong>relationship between two continuous or ordinal variables</strong>. In a monotonic relationship, the variables tend to change together, but not necessarily at a constant rate. <strong>The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data</strong>.</p>\n <p> Spearman correlation is often used to evaluate relationships involving ordinal variables. For example, you might use a Spearman correlation to evaluate whether the order in which employees complete a test exercise is related to the number of months they have been employed.</p>\n <p><strong>The correlation coefficient is a function of the covariance</strong>. The correlation coefficient is equal to the covariance divided by the product of the standard deviations of the variables. `Therefore, a positive covariance always results in a positive correlation and a negative covariance always results in a negative correlation.</p>\n<p> <a href=\"https://lindeloev.github.io/tests-as-linear/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://lindeloev.github.io/tests-as-linear/</a></p>\n <p>Parametric vs NonParametric Tests</p>\n<table>\n<thead>\n<tr>\n<th><strong>Parametric tests (means)</strong></th>\n<th><strong>Nonparametric tests (medians)</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>1-sample t test</strong></td>\n<td><strong>1-sample Sign, 1-sample Wilcoxon</strong></td>\n</tr>\n<tr>\n<td><strong>2-sample t test</strong></td>\n<td><strong>Mann-Whitney test</strong></td>\n</tr>\n<tr>\n<td><strong>One-Way ANOVA</strong></td>\n<td><strong>Kruskal-Wallis, Mood&#39;s median test</strong></td>\n</tr>\n<tr>\n<td><strong>Factorial DOE with one factor and one blocking variable</strong></td>\n<td><strong>Friedman test</strong></td>\n</tr>\n</tbody></table>\n <p><a href=\"https://keydifferences.com/difference-between-t-test-and-f-test.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">t-test vs f-test</a></p>\n<table>\n<thead>\n<tr>\n<th><strong>BASIS FOR COMPARISON</strong></th>\n<th><strong>T-TEST</strong></th>\n<th><strong>F-TEST</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Meaning</td>\n<td>T-test is a univariate hypothesis test, that is applied when standard deviation is not known and the sample size is small.</td>\n<td>F-test is statistical test, that determines the equality of the variances of the two normal populations.</td>\n</tr>\n<tr>\n<td>Test statistic</td>\n<td>T-statistic follows Student t-distribution, under null hypothesis.</td>\n<td>F-statistic follows Snedecor f-distribution, under null hypothesis.</td>\n</tr>\n<tr>\n<td>Application</td>\n<td>Comparing the means of two populations.</td>\n<td>Comparing two population variances.</td>\n</tr>\n</tbody></table>\n <p><a href=\"https://keydifferences.com/difference-between-anova-and-ancova.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">anova and ancova</a></p>\n<table>\n<thead>\n<tr>\n<th><strong>BASIS FOR COMPARISON</strong></th>\n<th><strong>ANOVA</strong></th>\n<th><strong>ANCOVA</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Meaning</td>\n<td>ANOVA is a process of examining the difference among the means of multiple groups of data for homogeneity.</td>\n<td>ANCOVA is a technique that remove the impact of one or more metric-scaled undesirable variable from dependent variable before undertaking research.</td>\n</tr>\n<tr>\n<td>Uses</td>\n<td>Both linear and non-linear model are used.</td>\n<td>Only linear model is used.</td>\n</tr>\n<tr>\n<td>Includes</td>\n<td>Categorical variable.</td>\n<td>Categorical and interval variable.</td>\n</tr>\n<tr>\n<td>Covariate</td>\n<td>Ignored</td>\n<td>Considered</td>\n</tr>\n<tr>\n<td>BG variation</td>\n<td>Attributes Between Group (BG) variation, to treatment.</td>\n<td>Divides Between Group (BG) variation, into treatment and covariate.</td>\n</tr>\n<tr>\n<td>WG variation</td>\n<td>Attributes Within Group (WG) variation, to individual differences.</td>\n<td>Divides Within Group (WG) variation, into individual differences and covariate.</td>\n</tr>\n</tbody></table>\n <p><a href=\"https://keydifferences.com/difference-between-t-test-and-anova.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">T-Test V Anova</a></p>\n<table>\n<thead>\n<tr>\n<th><strong>BASIS FOR COMPARISON</strong></th>\n<th><strong>T-TEST</strong></th>\n<th><strong>ANOVA</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Meaning</td>\n<td>T-test is a hypothesis test that is used to compare the means of two populations.</td>\n<td>ANOVA is a statistical technique that is used to compare the means of more than two populations.</td>\n</tr>\n<tr>\n<td>Test statistic</td>\n<td>(x Ì„-Âµ)/(s/âˆšn)</td>\n<td>Between Sample Variance/Within Sample Variance</td>\n</tr>\n</tbody></table>\n <p><a href=\"https://keydifferences.com/difference-between-one-way-and-two-way-anova.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">ONE WAY ANOVA** VS **TWO WAY ANOVA</a></p>\n<table>\n<thead>\n<tr>\n<th><strong>BASIS FOR COMPARISON</strong></th>\n<th><strong>ONE WAY ANOVA</strong></th>\n<th><strong>TWO WAY ANOVA</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Meaning</td>\n<td>One way ANOVA is a hypothesis test, used to test the equality of three of more population means simultaneously using variance.</td>\n<td>Two way ANOVA is a statistical technique wherein, the interaction between factors, influencing variable can be studied.</td>\n</tr>\n<tr>\n<td>Independent Variable</td>\n<td>One</td>\n<td>Two</td>\n</tr>\n<tr>\n<td>Compares</td>\n<td>Three or more levels of one factor.</td>\n<td>Effect of multiple level of two factors.</td>\n</tr>\n<tr>\n<td>Number of Observation</td>\n<td>Need not to be same in each group.</td>\n<td>Need to be equal in each group.</td>\n</tr>\n<tr>\n<td>Design of experiments</td>\n<td>Need to satisfy only two principles.</td>\n<td>All three principles needs to be satisfied.</td>\n</tr>\n</tbody></table>\n <p><strong>Compare <a href=\"%5Bhttp://www.statsoft.com/Textbook/Distribution-Tables%5D(http://www.statsoft.com/Textbook/Distribution-Tables)\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Distribution</a> Tables</strong></p>\n<p> Compared to probability calculators (e.g., the one included in <em>STATISTICA</em>), the traditional format of distribution tables such as those presented below, has the advantage of showing many values simultaneously and, thus, enables the user to examine and quickly explore ranges of probabilities.</p>\n <p><strong>Standard Normal (Z) Table</strong></p>\n<p> <img src=\"RackMultipart20200720-4-kb6onf_html_4ccbc6aa034a2e89.gif\" alt=\"\"></p>\n<p> The Standard Normal distribution is used in various hypothesis tests including tests on single means, the difference between two means, and tests on proportions.</p>\n <p> <strong>Student&#39;s t Table</strong></p>\n<p> <img src=\"RackMultipart20200720-4-kb6onf_html_8298d216d86533f.gif\" alt=\"\"></p>\n<p> The Shape of the Student&#39;s t distribution is determined by the degrees of freedom. As shown in the animation above, its shape changes as the degrees of freedom increases. For more information on how this distribution is used in hypothesis testing, see <a href=\"http://www.statsoft.com/textbook/basic-statistics/#t-test%20for%20independent%20samples\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">t-test for independent samples</a> and <a href=\"http://www.statsoft.com/textbook/basic-statistics/#t-test%20for%20dependent%20samples\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">t-test for dependent samples</a> in <a href=\"http://www.statsoft.com/textbook/basic-statistics/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Basic Statistics and Tables</a></p>\n <p> <strong>Chi-Square Table</strong></p>\n<p> <img src=\"RackMultipart20200720-4-kb6onf_html_1ada53c1efb455b3.gif\" alt=\"\"></p>\n<p> Like the Student&#39;s <em>t</em>-Distribution, the <em>Chi-square</em> distribution&#39;s shape is determined by its degrees of freedom. The animation above shows the shape of the <em>Chi-square</em> distribution as the degrees of freedom increase (1, 2, 5, 10, 25 and 50). For examples of tests of hypothesis that use the <em>Chi-square distribution</em>, see <a href=\"http://www.statsoft.com/textbook/basic-statistics/#Statistics%20in%20crosstabulation%20tables\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Statistics in crosstabulation tables</a> in <a href=\"http://www.statsoft.com/textbook/basic-statistics/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Basic Statistics and Tables</a> as well as <a href=\"http://www.statsoft.com/textbook/nonlinear-estimation/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Nonlinear Estimation</a> . See also, <a href=\"http://www.statsoft.com/textbook/statistics-glossary/c.aspx?button=c#Chi-square%20Distribution\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Chi-square Distribution</a>.</p>\n <p><strong>F Distribution Tables</strong></p>\n<p> <img src=\"RackMultipart20200720-4-kb6onf_html_82e9a5f816ba4c09.gif\" alt=\"\"></p>\n<p> The <a href=\"http://www.statsoft.com/textbook/statistics-glossary/f.aspx?button=f#F%20Distribution\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">F distribution</a> is a right-skewed distribution used most commonly in Analysis of Variance (see <a href=\"http://www.statsoft.com/textbook/anova-manova/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">ANOVA/MANOVA</a>). The F distribution is a ratio of two <em>Chi-square</em> distributions, and a specific F distribution is denoted by the degrees of freedom for the numerator Chi-square and the degrees of freedom for the denominator Chi-square. An example of the F(10,10)distribution is shown in the animation above</p>\n  <p>Gamma exponential bi bernoulli poisson</p>\n <h1><strong>3 Pearson and Spearman correlation</strong></h1>\n<p> the <strong>Spearman rank correlation</strong> is a <strong>Pearson correlation</strong> on rank-transformed x and y\n  rank(y)=Î²0+Î²1â‹…rank(x) H0:Î²1=0\n  value = &#39;rho&#39;</p>\n<p> Although this correlation is fairly obvious your data may contain unsuspected correlations. You may also suspect there are correlations, but don&#39;t know which are the strongest.</p>\n<p>  While correlation coefficients are normally reported as r = (a value between -1 and +1), squaring them makes then easier to understand. The square of the coefficient (or r square) is equal to the percent of the variation in one variable that is related to the variation in the other. After squaring r, ignore the decimal point. An r of .5 means 25% of the variation is related (.5 squared =.25). An r value of .7 means 49% of the variance is related (.7 squared = .49).\n A correlation report can also show a second result of each test - statistical significance. In this case, the significance level will tell you how likely it is that the correlations reported may be due to chance in the form of random sampling error. If you are working with small sample sizes, choose a report format that includes the significance level. This format also reports the sample size.\n <a href=\"https://www.medcalc.org/manual/correlation.php\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://www.medcalc.org/manual/correlation.php</a>\n The P-value is the probability that you would have found the current result if the correlation coefficient were in fact zero (null hypothesis). If this probability is lower than the conventional 5% (P&amp;lt;0.05) the correlation coefficient is called statistically significant.\n <strong>95% confidence interval (CI) for the correlation coefficient</strong>: this is the range of values that contains with a 95% confidence the &#39;true&#39; correlation coefficient.</p>\n <h1><strong>4 One mean</strong></h1>\n<p> 4.1 One sample t-test and Wilcoxon signed-rank\n <strong>t-test</strong> model: A single number predicts y.\n  Y = mx+b where x = 0. Null Hypothesis b=0 <img src=\"RackMultipart20200720-4-1eeqd6o_html_cdcf0d66b6c0da6a.png\" alt=\"\">\n <strong>Wilcoxon signed-rank test</strong> , just with the signed ranks of yy instead of y itself\n signed_rank(y)=Î²0</p>\n<p> 4.2 Paired samples t-test and Wilcoxon matched pairs\n <strong>t-test</strong> model: a single number (intercept) predicts the pairwise differences.\n  Y2-y1=mx+b where x = 0. Null Hypothesis: b=0</p>\n<p> <strong>This means that there is just one</strong>  <strong>y</strong>** = <strong><strong>y</strong></strong> 2 <strong><strong>âˆ’</strong></strong> y **<strong>1</strong>  <strong>to predict and it becomes a</strong> <a href=\"https://lindeloev.github.io/tests-as-linear/#t1\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><strong>one-sample t-test</strong></a> <strong>on the pairwise differences</strong>. The visualization is therefore also the same as for the one-sample t-test. At the risk of overcomplicating a simple substraction, you can think of these pairwise differences as slopes (see left panel of the figure), which we can represent as y-offsets (see right panel of the figure):\n  <img src=\"RackMultipart20200720-4-1eeqd6o_html_3052cc7d86822899.png\" alt=\"\">\n  Similarly, the <strong>Wilcoxon matched pairs</strong> only differ from <strong>Wilcoxon signed-rank</strong> in that it&#39;s testing the signed ranks of the pairwise yâˆ’x differences.</p>\n <h1><strong>5 Two means</strong></h1>\n<p> 5.1 Independent t-test and Mann-Whitney U\n  Independent t-test model: two means predict yy.</p>\n<p> yi=Î²0+Î²1xiÎ²1=0\n where xixi is an indicator (0 or 1) saying whether data point ii was sampled from one or the other group. <a href=\"https://en.wikipedia.org/wiki/Dummy_variable_(statistics)\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">Indicator variables (also called &quot;dummy coding&quot;)</a> underly a lot of linear models and we&#39;ll take an aside to see how it works in a minute.\n <strong>Mann-Whitney U</strong> (also known as <strong>Wilcoxon signed-rank test</strong> for two independent groups) is the same model to a very close approximation, just on the ranks of xx and yy instead of the actual values:\n rank(yi)=Î²0+Î²1xi</p>\n<p> <img src=\"RackMultipart20200720-4-1eeqd6o_html_862ee34917ee9f6.png\" alt=\"\">\n 5.2 Welch&#39;s t-test</p>\n<p> This is identical to the (Student&#39;s) <a href=\"https://lindeloev.github.io/tests-as-linear/#t2\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">independent t-test</a> above except that Student&#39;s assumes identical variances and <strong>Welch&#39;s t-test</strong> does not.</p>\n <h1><strong>6 Three or more means</strong></h1>\n<p> ANOVAs are linear models with (only) categorical predictors. They simply extend everything we did above, relying heavily on dummy coding.</p>\n <p><strong>Multivariate</strong></p>\n<p> Relationship Between Variables</p>\n<ul>\n<li><p>Correlation Coefficient</p></li>\n<li><p>Correlation Coefficient by Variable Combination</p></li>\n<li><p>Correlation Plot of Numerical Variables</p></li>\n</ul>\n<p> Target based Analysis</p>\n<ul>\n<li><p>Grouped Descriptive Statistics</p>\n</li>\n<li><p>Grouped Numerical Variables</p>\n</li>\n<li><p>Grouped Categorical Variables</p>\n</li>\n<li><p>Grouped Relationship Between Variables</p>\n</li>\n<li><p>Grouped Correlation Coefficient</p>\n</li>\n<li><p>Grouped Correlation Plot of Numerical Variables</p>\n</li>\n<li><p>Anova </p>\n</li>\n<li><p>ancova </p>\n</li>\n<li><p>Support </p>\n</li>\n<li><p>Confidence </p>\n</li>\n<li><p>Lift</p>\n</li>\n</ul>\n<p> Multivariate</p>\n<ul>\n<li><p>Correlation Coefficient</p></li>\n<li><p>Correlation Coefficient by Variable Combination</p></li>\n<li><p>Correlation Plot of Numerical Variables</p></li>\n<li><p>Target based Analysis</p></li>\n<li><p>Gruoped Descriptive Statistics</p></li>\n<li><p>Gruoped Numerical Variables</p></li>\n<li><p>Gruoped Categorical Variables</p></li>\n<li><p>Gruoped Relationship Between Variables</p></li>\n<li><p>Grouped Correlation Coefficient</p></li>\n<li><p>Grouped Correlation Plot of Numerical Variables</p></li>\n</ul>\n<p> Major columns</p>\n<p> Anova ancova </p>\n<p> Support Confidence Lift</p>\n<p> The bivariate distribution plots help us to study the relationship between two variables by analyzing the scatter plot the bivariate distributions:</p>\n<p> If the denominators used to calculate the two percentages represent the same people, we use a one-sample t-test between percents to compare the two percents. If the denominators represent different people, we use the two-sample t-test between percents.</p>\n <p>Median difference from Quartiles represent skew. Whiskers represent variance.</p>\n<p> Cross tabulation:</p>\n<ul>\n<li><ol>\n<li><p>preprocess?</p></li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li><p>figure out which 2 features you want to crosstab</p></li>\n</ol>\n</li>\n<li><ol start=\"3\">\n<li><p>Create a column for every category of feature 1</p></li>\n</ol>\n</li>\n<li><ol start=\"4\">\n<li><p>Determine how you will aggregate (sum,count,avg,max,min,product)</p></li>\n</ol>\n</li>\n<li><ol start=\"5\">\n<li><p>create a record for every unique feature 1</p></li>\n</ol>\n</li>\n<li><p>5.1) create a column for the unique labels name</p></li>\n<li><ol start=\"6\">\n<li><p>aggregate feature 1 columns</p></li>\n</ol>\n</li>\n</ul>\n <h2>Data Assumptions Parametric Vs Non</h2>\n <p><a href=\"http://www.statsoft.com/Textbook/Nonparametric-Statistics\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">http://www.statsoft.com/Textbook/Nonparametric-Statistics</a></p>\n<p> Relations between Variables - Magnitude, Reliability, PValue, </p>\n<p> Small relations can only be proven in large samples</p>\n<h4>Parametric</h4>\n<ul>\n<li><p>Differences between independent groups -&gt; T-Test for Independent Samples</p></li>\n<li><p>Differences between dependent groups -&gt; T-Test for Dependent Samples, If there are more than two variables that were measured in the same sample, then we would customarily use repeated measures ANOVA.</p></li>\n<li><p>Relationships between variables -&gt; correlation coefficient</p></li>\n</ul>\n<h4>Nonparametric</h4>\n<ul>\n<li><p>Differences between independent groups -&gt; Mann-Whitney U test, and the Kolmogorov-Smirnov two-sample test and Wald-Wolfowitz</p></li>\n<li><p>Differences between dependent groups -&gt; Sign test and Wilcoxon&#39;s matched pairs test. If the variables of interest are dichotomous in nature, &quot;pass&quot; vs. &quot;no pass&quot; then McNemar&#39;s Chi-square test. If there are more than two variables that were measured in the same sample, then we would use Friedman&#39;s two-way analysis of variance and Cochran Q test (if the variable was measured in terms of categories, e.g., &quot;passed&quot; vs. &quot;failed&quot;). Cochran Q is particularly useful for measuring changes in frequencies (proportions) across time.</p></li>\n<li><p>Relationships between variables -&gt; Spearman R, Kendall Tau, and coefficient Gamma.  If the two variables of interest are categorical in nature (e.g., &quot;passed&quot; vs. &quot;failed&quot; by &quot;male&quot; vs. &quot;female&quot;) appropriate nonparametric statistics for testing the relationship between the two variables are the Chi-square test, the Phi coefficient, and the Fisher exact test. In addition, a simultaneous test for relationships between multiple cases is available: Kendall coefficient of concordance. This test is often used for expressing inter-rater agreement among independent judges who are rating (ranking) the same stimuli</p></li>\n<li><p><a href=\"http://www.statsoft.com/Textbook/Nonparametric-Statistics#correlations\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">http://www.statsoft.com/Textbook/Nonparametric-Statistics#correlations</a></p></li>\n</ul>\n <h2>Common Tests</h2>\n <p>Basic Statistics: Descriptive Statistics, Correlations, t-tests, frequency tables, cross tabulation</p>\n <p> measures of location (mean, median, mode, etc.) \n and dispersion (variance, average deviation, quartile range, etc.)</p>\n<p> Parametric(Means) Vs NonParametric (Medians)</p>\n<ul>\n<li><p>1-sample t test VS 1-sample Sign, 1-sample Wilcoxon</p></li>\n<li><p>2-sample t test VS Mann-Whitney test</p></li>\n<li><p>One-Way ANOVA VS Kruskal-Wallis, Moodâ€™s median test</p></li>\n</ul>\n<p> T-Test</p>\n<ul>\n<li><p>Meaning: T-test is a univariate hypothesis test, that is applied when standard deviation is not known and the sample size is small. T-test is a hypothesis test that is used to compare the means of two populations.</p></li>\n<li><p>Test Statistic: T-statistic follows Student t-distribution, under null hypothesis.</p></li>\n<li><p>Application: Comparing the means of two populations.</p></li>\n<li><p>Test statistic - (x Ì„-Âµ)/(s/âˆšn)</p></li>\n</ul>\n<p> F-Test</p>\n<ul>\n<li><p>Meaning: F-test is statistical test, that determines the equality of the variances of the two normal populations.</p></li>\n<li><p>Test Statistic: F-statistic follows Snedecor f-distribution, under null hypothesis.</p></li>\n<li><p>Application: Comparing two population variances.</p></li>\n</ul>\n<p> Anova</p>\n<ul>\n<li><p>Meaning - ANOVA is a process of examining the difference among the means of multiple groups of data for homogeneity.</p></li>\n<li><p>Uses - Both linear and non-linear model are used. ANOVA is a statistical technique that is used to compare the means of more than two populations.</p></li>\n<li><p>Includes - Categorical variable.</p></li>\n<li><p>Covariate - Ignored</p></li>\n<li><p>BG variation - Attributes Between Group (BG) variation, to treatment.</p></li>\n<li><p>WG variation - Attributes Within Group (WG) variation, to individual differences.</p></li>\n<li><p>Test statistic - Between Sample Variance/Within Sample Variance</p></li>\n</ul>\n<p> Ancova</p>\n<ul>\n<li><p>Meaning - ANCOVA is a technique that remove the impact of one or more metric-scaled undesirable variable from dependent variable before undertaking research. </p></li>\n<li><p>Uses -  Only linear model is used.</p></li>\n<li><p>Includes - Categorical and interval variable.</p></li>\n<li><p>Covariate - Considered</p></li>\n<li><p>BG variation - Divides Between Group (BG) variation, into treatment and covariate.</p></li>\n<li><p>WG variation - Divides Within Group (WG) variation, into individual differences and covariate.</p></li>\n</ul>\n<p> ONE WAY ANOVA</p>\n<ul>\n<li><p>Meaning - One way ANOVA is a hypothesis test, used to test the equality of three of more population means simultaneously using variance.</p></li>\n<li><p>Independent Variable - One</p></li>\n<li><p>Compares - Three or more levels of one factor.</p></li>\n<li><p>Number of Observation - Need not to be same in each group.</p></li>\n<li><p>Design of experiments - Need to satisfy only two principles.</p></li>\n</ul>\n<p> TWO WAY ANOVA</p>\n<ul>\n<li><p>Meaning - Two way ANOVA is a statistical technique wherein, the interaction between factors, influencing variable can be studied.</p></li>\n<li><p>Independent Variable - Two</p></li>\n<li><p>Compares - Effect of multiple level of two factors.</p></li>\n<li><p>Number of Observation - Need to be equal in each group.</p></li>\n<li><p>Design of experiments - All three principles needs to be satisfied.</p></li>\n</ul>\n<p> Anova Ancova Sarima Arima ARMA Spatial Temporal</p>\n <p>ANOVA Requirements</p>\n<ul>\n<li><p>Normal Distribution</p></li>\n<li><p>Independent Samples/Groups</p></li>\n<li>Independent Samples t Test requires the assumption of homogeneity of variance. \n a test for the homogeneity of variance, called Levene&#39;s Test, whenever you run an independent samples T test</li>\n</ul>\n <h2>Test Erors</h2>\n <p>Most researches test a null hyppothesis with alpha at .05</p>\n<p> Type 1 Error - Erroneously rejecting the null hypothesis with a statistical analysis, when the null hypothesis is in fact true in the population.</p>\n<p> Single Analysis \n Test the null H of equal mean IQs between adult males and adult females.\n This is done by Testing the null H with an independent samples t-test\n If the t-test p-value is less than .05, reject the null hypothesis of equal means.</p>\n<p> The Bonferroni Correction is an adjustent applied to p-values that is &#39;supposed to be&#39; applied, when two or more statistical analyses have been performed on the same sample of data.</p>\n<p> specifies the chances of erroneously rejecting the null hypothesis at least once amongst the family of analyses is equal to X</p>\n<p> ALPHA(fam-wise-error-rate) = 1 - (1-.05)^#tests</p>\n<p> Approach 1. Divide the per analsysis alpha rate by the number of statistical analyses performed (.05/3 = .017) ... =&gt; any observed p-value less than the corrected p-value (.017) is declared to be statistically significant. </p>\n<p> This can sometimes oblitterate all statistically significant results.</p>\n <p>Common Error Measures: </p>\n<ul>\n<li><p>(root) Mean Squared Error - continuous data, sensitivity to outliers</p></li>\n<li><p>median absolute deviation - continuous data, often more robust</p></li>\n<li><p>sensitivity (Recall) - if you want few missed positives</p></li>\n<li><p>specificity - if you want few negatives called positives</p></li>\n<li><p>accuracy - weights false positives / negatives equally</p></li>\n<li><p>Concordance - one example is kappa</p></li>\n</ul>\n <p>Challenges With Data</p>\n<ul>\n<li><p>Some of this was covered in the prior sections and will not be repeated here.</p></li>\n</ul>\n<ul>\n<li><p>Options tree with Pessimistic, Nominal &amp; Optimistic ...</p></li>\n<li><p>Performance vs Risk vs Design analysis</p></li>\n</ul>\n<p> Key issues: accuracy, overfitting, interpretability, computational speed.</p>\n<p> Pay attention to - confounding variables, complicated interactions, skewness, outliers, nonlinear patterns, variance changes, \n units/scale issues, overloading, regression, correlation and causation</p>\n <p>measures of effectiveness of the classifier:</p>\n<ul>\n<li><p>Predictive accuracy: How well does it predict the categories for new observations?</p></li>\n<li><p>Speed: What is the computational cost of using the classifier?</p></li>\n<li><p>Robustness: How well do the models created perform if data quality is low?</p></li>\n<li><p>Scalability: Does the classifier function efficiently with large amounts of data?</p></li>\n<li><p>Interpretability: Are the results understandable to users?</p></li>\n</ul>\n <p>Confounder: a variable that is correlated with both the outcome and covariates</p>\n<ul>\n<li><p>confounders can change the regression line.</p></li>\n<li><p>detected with exploration</p></li>\n</ul>\n<p> Descriptive Statistics : Location (mean,median,mode) Spread(standard deviation, variance, range, iqr, absolute deviation, mean absolute difference, distance standard deviation... Coefficient of variation and Gini Coefficients. shape( skewness ie kurtosis, distance skewness). Dependence ( Pearson, Spearman)</p>\n<p> Prefered Data Format : yyyy-mm-dd&#39;T&#39;hh:mm:ss.mmm</p>\n<p> workhorse estimation technique of frequentist statistics: maximum likelihood estimation.</p>\n <p>Changing variance - what can you do</p>\n<ul>\n<li><p>box cox transform</p></li>\n<li><p>variance stabilizing transform</p></li>\n<li><p>weighted least squares</p></li>\n<li><p>huber white standard errors</p></li>\n</ul>\n <p>MISC</p>\n<ul>\n<li><p>P Values require knowing how many records exist are in the database.</p></li>\n<li><p>Outliers profoundly influence on the slope of the regression line and the correlation coefficient. </p></li>\n<li><p>Correlation coefficient alone is not enough for decision making (i.e., scatterplots are always recommended)</p></li>\n<li><p>Prefered Data Format : yyyy-mm-dd&#39;T&#39;hh:mm:ss.mmm</p></li>\n</ul>\n <p>Bias:</p>\n<ul>\n<li><p>Quantitative Approach to Outliers.</p></li>\n<li><p>Correlations in Non-homogeneous Groups</p></li>\n<li><p>Nonlinear Relations between Variables. - Pearson R measures linearity</p></li>\n<li><p>Exploratory Examination of Correlation Matrices</p></li>\n<li><p>Casewise vs. Pairwise Deletion of Missing Data</p></li>\n<li><p>Overfitting: Pruning/ Cross Validation</p></li>\n<li><p>Breakdown Analysis</p></li>\n<li><p>Frequency Tables</p></li>\n<li><p>Cross Tabulation</p></li>\n<li><p>Marginal Frequencies</p></li>\n<li><p>Association Rules</p></li>\n</ul>\n"}