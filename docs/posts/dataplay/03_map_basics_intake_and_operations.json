{"meta":{"title":"Geo-Data Intake and Operations","summary":"This notebook was made to demonstrate how to work with geographic data.","toc":"true","prettify":"true","default_exp":"geoms","filename":"03_map_basics_intake_and_operations"},"content":"<pre class='prettyprint'># from google.colab import drive\n # drive.mount('/gdrive')\n # cd ../gdrive/MyDrive/'Software Development Documents'/dataplay</pre> <p><a href=\"https://mybinder.org/v2/gh/bnia/dataplay/main?filepath=%2Fnotebooks%2F03_Map_Basics_Intake_and_Operations.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Binder\"></a>\n <a href=\"https://colab.research.google.com/github/bnia/dataplay/blob/main/notebooks/03_Map_Basics_Intake_and_Operations.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://pete88b.github.io/fastpages/assets/badges/colab.svg\" alt=\"Binder\"></a>\n <a href=\"https://github.com/bnia/dataplay/tree/main/notebooks/03_Map_Basics_Intake_and_Operations.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://pete88b.github.io/fastpages/assets/badges/github.svg\" alt=\"Binder\"></a>\n <a href=\"https://github.com/ellerbrock/open-source-badges/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://badges.frapsoft.com/os/v3/open-source.svg?v=103\" alt=\"Open Source Love svg3\"></a></p>\n<p> <a href=\"https://github.com/bnia/dataplay/blob/main/LICENSE\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://img.shields.io/npm/l/all-contributors.svg?style=flat\" alt=\"NPM License\"></a>\n <a href=\"https://bnia.github.io\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"http://img.shields.io/badge/Status-Active-green.svg\" alt=\"Active\"></a> \n <a href=\"https://pypi.python.org/pypi/dataplay/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://img.shields.io/pypi/pyversions/dataplay.svg\" alt=\"Python Versions\"></a>\n <a href=\"\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://img.shields.io/github/last-commit/bnia/dataplay.svg?style=flat\" alt=\"GitHub last commit\"></a>  </p>\n<p> <a href=\"https://github.com/bnia/dataplay\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://img.shields.io/github/stars/bnia/dataplay.svg?style=social&label=Star\" alt=\"GitHub stars\"></a> \n <a href=\"https://github.com/bnia/dataplay\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://img.shields.io/github/watchers/bnia/dataplay.svg?style=social&label=Watch\" alt=\"GitHub watchers\"></a> \n <a href=\"https://github.com/bnia/dataplay\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://img.shields.io/github/forks/bnia/dataplay.svg?style=social&label=Fork\" alt=\"GitHub forks\"></a> \n <a href=\"https://github.com/bnia/dataplay\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://img.shields.io/github/followers/bnia.svg?style=social&label=Follow\" alt=\"GitHub followers\"></a> </p>\n<p> <a href=\"https://twitter.com/intent/tweet?text=Check%20out%20this%20%E2%9C%A8%20colab%20by%20@bniajfi%20https://github.com/bnia/dataplay%20%F0%9F%A4%97\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://img.shields.io/twitter/url/https/github.com/bnia/dataplay.svg?style=social\" alt=\"Tweet\"></a> \n <a href=\"https://twitter.com/bniajfi\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\"><img src=\"https://img.shields.io/twitter/follow/bniajfi.svg?style=social\" alt=\"Twitter Follow\"></a></p>\n <details open>\n <summary> <h2 id=\"about-this-tutorial\">About this Tutorial:</h2>\n </summary> <p>In this notebook, the basics of working with geographic data are introduced.</p>\n<ul>\n<li><p>Reading in data (points/ geoms)</p></li>\n<li><ul>\n<li><p>Convert lat/lng columns to point coordinates</p></li>\n</ul>\n</li>\n<li><ul>\n<li><p>Geocoding address to coordinates</p></li>\n</ul>\n</li>\n<li><ul>\n<li><p>Changing coordinate reference systems</p></li>\n</ul>\n</li>\n<li><ul>\n<li><p>Connecting to PostGisDB&#39;s</p></li>\n</ul>\n</li>\n<li><p>Basic Operations</p></li>\n<li><p>Saving shape data</p></li>\n<li><p>Get Polygon Centroids</p></li>\n<li><p>Working with Points and Polygons</p></li>\n<li><ul>\n<li><p>Map Points and Polygons</p></li>\n</ul>\n</li>\n<li><ul>\n<li><p>Get Points in Polygons</p></li>\n</ul>\n</li>\n<li><ul>\n<li><p>Create Choropleths</p></li>\n</ul>\n</li>\n<li><ul>\n<li><p>Create Heatmaps (KDE?)</p></li>\n</ul>\n</li>\n</ul>\n <h3 id=\"objectives\">Objectives</h3>\n<p> By the end of this tutorial users should have an understanding of:</p>\n<ul>\n<li><p>How to read in and process geo-data asa geo-dataframe.</p></li>\n<li><p>The Coordinate Reference System and Coordinate Encoding</p></li>\n<li><p>Basic geo-visualization strategies</p></li>\n</ul>\n </details>\n <details>\n <summary> <h2 id=\"background\">Background</h2>\n </summary> <p>An expansice discursive on programming and cartography can be found <a href=\"https://medium.com/@mbostock/command-line-cartography-part-1-897aa8f8ca2c\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">here</a></p>\n <details>\n <summary> <h3 id=\"datatypes-and-geo-data\">Datatypes and Geo-data</h3>\n </summary> <p>Geographic data must be <a href=\"https://www.blender.org/fileadmin/verse/spec/protocol-encoding.html#:~:text=3.3.-,Data%20Encoding,is%20specified%20in%20this%20section.\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">encoded</a> properly order to attain the full potential of the spatial nature of your geographic data.</p>\n<p> If you have read in a dataset using <em>pandas</em> it&#39;s data type will be a <strong>Dataframe</strong>.</p>\n<p> It may be converted into a <strong>Geo-Dataframe</strong> using <em>Geopandas</em> as demonstrated in the sections below. </p>\n<p> You can check a variables at any time using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">dtype</a> command:</p>\nundefined\n </details>\n <details>\n <summary> <h3 id=\"coordinate-reference-systems-crs\">Coordinate Reference Systems (CRS)</h3>\n </summary> <p><strong>Make sure</strong> the appropriate spatial <em>Coordinate Reference System</em> (CRS) is used when reading in your data!</p>\n<p> ala <a href=\"https://en.wikipedia.org/wiki/Spatial_reference_system\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">wiki</a>:</p>\n<blockquote>\n<p>A spatial reference system (SRS) or coordinate reference system (CRS) is a coordinate-based local, regional or global system used to locate geographical entities</p>\n</blockquote>\n<p> <strong>CRS 4326</strong> is the CRS most people are familar with when refering to latiude and longitudes.</p>\n<p> Baltimore&#39;s 4326 CRS should be at (39.2, -76.6)</p>\n<p> BNIA uses <a href=\"http://www.spatialreference.org/ref/epsg/2248/\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">CRS 2248</a> <em>internally</em></p>\n<p> Additional Information: <a href=\"https://docs.qgis.org/testing/en/docs/gentle_gis_introduction/coordinate_reference_systems.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://docs.qgis.org/testing/en/docs/gentle_gis_introduction/coordinate_reference_systems.html</a></p>\n<p> Ensure your geodataframes&#39; coordinates are using the same CRS using the geopandas command: </p>\nundefined\n </details>\n <details>\n <summary> <h3 id=\"coordinate-encoding\">Coordinate Encoding</h3>\n </summary> <p>When first recieving a spatial dataset, the spatial column may need to be encoded to convert its &#39;text&#39; data type values into understood &#39;coordinate&#39; data types before it can be understood/processed accordingly. </p>\n<p> Namely, there are two ways to encode text into coordinates: </p>\n<ul>\n<li><p>df[geom] = df[geom].apply(lambda x: loads( str(x) ))</p></li>\n<li><p>df[geom] = [Point(xy) for xy in zip(df.x, df.y)]</p></li>\n</ul>\n<p> The first approach can be used for text taking the form &quot;Point(-76, 39)&quot; and will encode the text too coordinates.\n The second approach is useful when creating a point from two columns containing lat/lng information and will create Point coordinates from the two columns.</p>\n<p> More on this later</p>\n </details>\n <details>\n <summary> <h3 id=\"raster-vs-vector-data\">Raster Vs Vector Data</h3>\n </summary> <p>There exists two types of Geospatial Data, Raster and Vector. \n Both have different file formats.</p>\n<p> This lab will only cover vector data.</p>\n <h4 id=\"vector-data\">Vector Data</h4>\n<p> Vector Data: Individual points stored as (x,y) coordinates pairs. These points can be joined to create lines or polygons.</p>\n<p> Format of Vector data</p>\n<p> Esri Shapefile — .shp, .dbf, .shx\n Description - Industry standard, most widely used. The three files listed above are needed to make a shapefile. Additional file formats may be included.</p>\n<p> Geographic JavaScript Object Notation — .geojson, .json\n Description — Second most popular, Geojson is typically used in web-based mapping used by storing the coordinates as JSON.</p>\n<p> Geography Markup Language — .gml\n Description — Similar to Geojson, GML has more data for the same amount of information.</p>\n<p> Google Keyhole Markup Language  — .kml, .kmz\n Description — XML-based and predominantly used for google earth. KMZ is a the newer, zipped version of KML.</p>\n <h4 id=\"raster-data\">Raster Data</h4>\n<p> Raster Data: Cell-based data where each cell represent geographic information. An Aerial photograph is one such example where each pixel has a color value</p>\n<p> Raster Data Files: \n GeoTIFF — .tif, .tiff, .ovr\n ERDAS Imagine — .img\n IDRISI Raster — .rst, .rdc</p>\n<p> Information Sourced From: <a href=\"https://towardsdatascience.com/getting-started-with-geospatial-works-1f7b47955438\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://towardsdatascience.com/getting-started-with-geospatial-works-1f7b47955438</a></p>\n <p><strong>Vector Data: Census Geographic Data</strong>:</p>\n<ul>\n<li><p><strong>Geographic Coordinate Data</strong> is provided by the census and compliments their census geographies </p></li>\n<li><p><a href=\"https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.2010.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.2010.html</a></p></li>\n<li><p><a href=\"https://www.census.gov/programs-surveys/acs/geography-acs/geography-boundaries-by-year.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://www.census.gov/programs-surveys/acs/geography-acs/geography-boundaries-by-year.html</a> </p></li>\n<li><p>Bnia created and provides for free geographic boundary data that compliment these CSA&#39;s</p></li>\n</ul>\n </details>\n </details>\n <details>\n <summary> <h2 id=\"setup\">SETUP:</h2>\n </summary> <details>\n <summary> <h3 id=\"import-modules\">Import Modules</h3>\n </summary>  <pre class='prettyprint'>!apt install libspatialindex-dev\n !pip install rtree</pre> <pre class='prettyprint'># @title Run: Install Modules\n %%capture\n ! pip install geopy \n ! pip install geoplot </pre> <pre class='prettyprint'>t = \"\"\"\n !pip install nbdev\n from google.colab import drive\n drive.mount('/content/drive')\n %cd /content/drive/My Drive/'Software Development Documents'/dataplay/\n \"\"\"\n # !pip install dataplay</pre> <pre class='prettyprint'># @title Run: Import Modules\n \n # These imports will handle everything\n import os\n import sys\n import csv\n import numpy as np\n import pandas as pd\n import pyproj\n from pyproj import Proj, transform\n # conda install -c conda-forge proj4\n from shapely.geometry import LineString\n # from shapely import wkb\n # https://pypi.org/project/geopy/\n import folium\n \n # In case file is KML, enable support\n import fiona\n fiona.drvsupport.supported_drivers['kml'] = 'rw'\n fiona.drvsupport.supported_drivers['KML'] = 'rw'\n \n import psycopg2</pre> <pre class='prettyprint'>import matplotlib.pyplot as plt\n import IPython\n from IPython.core.display import HTML\n \n import os \n from branca.colormap import linear</pre> <pre class='prettyprint'>import pandas as pd\n import geopandas as gpd\n from geopandas import GeoDataFrame\n from shapely.geometry import Point\n from shapely.wkt import loads\n from geopy.geocoders import Nominatim\n from IPython.display import clear_output\n from folium import plugins\n from folium.plugins import TimeSliderChoropleth \n from folium.plugins import MarkerCluster</pre> <pre class='prettyprint'>!pip install dataplay</pre> <pre class='prettyprint'>from dataplay import intaker \n from dataplay import merge</pre> <pre class='prettyprint'>from VitalSigns import acsDownload</pre> </details>\n <details>\n <summary> <h3 id=\"configure-enviornment\">Configure Enviornment</h3>\n </summary> <pre class='prettyprint'># This will just beautify the output\n  \n pd.set_option('display.expand_frame_repr', False)\n pd.set_option('display.precision', 2)\n from IPython.core.interactiveshell import InteractiveShell\n InteractiveShell.ast_node_interactivity = \"all\"\n  \n # pd.set_option('display.expand_frame_repr', False)\n # pd.set_option('display.precision', 2)\n # pd.reset_option('max_colwidth')\n pd.set_option('max_colwidth', 50)\n # pd.reset_option('max_colwidth')</pre> <pre class='prettyprint'># %matplotlib inline\n # !jupyter nbextension enable --py widgetsnbextension</pre> </details>\n </details>\n <details>\n <summary> <h2 id=\"retrieve-gis-data\">Retrieve GIS Data</h2>\n </summary> <p>As mentioned earlier: </p>\n<p> When you use a pandas function to &#39;read-in&#39; a dataset, the returned value is of a datatype called a &#39;Dataframe&#39;. </p>\n<p> We need a &#39;Geo-Dataframe&#39;, however, to effectively work with spatial data. </p>\n<p> While Pandas does not support Geo-Dataframes; Geo-pandas does. </p>\n<p> Geopandas has everything you love about pandas, but with added support for geo-spatial data.</p>\n<p> Principle benefits of using Geopandas over Pandas when working with spatial data: </p>\n<ul>\n<li><p>The geopandas plot function will now render a map by default using your &#39;spatial-geometries&#39; column.</p></li>\n<li><p>Libraries exist spatial-operations and interactive map usage.</p></li>\n</ul>\n <p>There are many ways to have our spatial-data be read-in using geo-pandas into a geo-dataframe.</p>\n<p> Namely, it means reading in Geo-Spatial-data from a:</p>\n<ol>\n<li><p>(.geojson or .shp) file directly using Geo-pandas</p></li>\n<li><p>(.csv, .json) file using Pandas and convert it to Geo-Pandas</p></li>\n</ol>\n<ul>\n<li><p>using a prepared &#39;geometry&#39; column</p></li>\n<li><p>by transformting latitude and longitude columns into a &#39;geometry&#39; column.</p></li>\n<li><p>acquiring coordinates from an address</p></li>\n<li><p>mapping your non-spatial-data to data-with-space</p></li>\n</ul>\n<ol start=\"3\">\n<li><p>Connecting to a DB</p></li>\n</ol>\n<p> We will review each one below</p>\n <details>\n <summary> <h3 id=\"approach-1-reading-in-data-directly\">Approach 1: Reading in Data Directly</h3>\n </summary> <p>If you are using Geopandas, direct imports <strong>only work</strong> with geojson and shape files. </p>\n<p> spatial coordinate data is properly encoded with these types of files soas to make them particularly easy to use.</p>\n<p> You can perform this using geopandas&#39; <code>read_file()</code> function.</p>\n <pre class='prettyprint'># This dataset is taken from the public database provided by BNIAJFI hosted by Esri / ArcGIS\n # BNIA ArcGIS Homepage: https://data-bniajfi.opendata.arcgis.com/\n csa_gdf = intaker.Intake.getData(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Hhchpov/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\")</pre> <p>As you can see, the resultant variable is of type GeoDataFrame.</p>\n <pre class='prettyprint'>type(csa_gdf)</pre> <p>GeoDataFrames are only possible when one of the columns are of a &#39;Geometry&#39; Datatype</p>\n <pre class='prettyprint'>csa_gdf.dtypes</pre> <p>Awesome. So that means, now you can plot maps all prety like: </p>\n <pre class='prettyprint'>csa_gdf.plot(column='hhchpov15')</pre> <p>And now lets take a peak at the raw data:</p>\n <pre class='prettyprint'>csa_gdf.head(1)</pre> <p>I&#39;ll show you more ways to save the data later, but for our example in the next section to work, we need a csv. </p>\n<p> We can make one by saving the geo-dataframe avove using the <code>to_gdf</code> function.</p>\n<p> The spatial data will be stored in an encoded form that will make it easy to re-open up in the future.</p>\n <pre class='prettyprint'>csa_gdf.to_csv('example.csv')</pre> </details>\n <details>\n <summary> <h3 id=\"approach-2-converting-pandas-into-geopandas\">Approach 2: Converting Pandas into Geopandas</h3>\n </summary> <details>\n <summary> <h4 id=\"approach-2-method-1-convert-using-a-pre-formatted-geometry-column\">Approach 2: Method 1: Convert using a pre-formatted &#39;geometry&#39; column</h4>\n </summary> <p>This approach loads a map using a geometry column</p>\n <p>In our previous example, we saved a geo-dataframe as a csv. </p>\n<p> Now lets re-open it up using pandas!</p>\n <pre class='prettyprint'># A url to a public Dataset.\n url = \"example.csv\"\n geom = 'geometry'\n # An example of loading in an internal BNIA file\n crs = {'init' :'epsg:2248'} \n  \n # Read in the dataframe\n csa_gdf = intaker.Intake.getData(url)</pre> <p>Great!</p>\n<p> But now what?</p>\n<p> Well, for starters, regardless of the project you are working on: It&#39;s always a good idea to inspect your data. </p>\n<p> This is particularly important if you don&#39;t know what you&#39;re working with. </p>\n <pre class='prettyprint'>csa_gdf.head(1)</pre> <p>Take notice of how the geometry column has a special.. foramatting. </p>\n<p> All spatial data must take on a similar form encoding for it to be properly interpretted as a spatial data-type. </p>\n<p> As far as I can tell, This is near-identical to the table I printed out in our last example. </p>\n<p> BUT WAIT!</p>\n <p>You&#39;ll notice, that if I run the plot function a pretty map will not de-facto appear</p>\n <pre class='prettyprint'>csa_gdf.plot()</pre> <p>Why is this? Because you&#39;re not working with a geo-dataframe but just a dataframe!</p>\n<p> Take a look:</p>\n <pre class='prettyprint'>type(csa_gdf)</pre> <p>Okay... So thats not right..</p>\n <p>What can we do about this?</p>\n <p>Well for one, our spatial data (in the geometry-column) is not of the right data-type even though it takes on the right form.</p>\n <pre class='prettyprint'>csa_gdf.dtypes</pre> <p>Ok. So how do we change it? Well, since it&#39;s already been properly encoded... </p>\n<p> You can convert a columns data-type from an object (or whatver else) to a &#39;geometry&#39; using the <code>loads</code> function. </p>\n<p> In the example below, we convert the datatypes for all records in the &#39;geometry&#39; column </p>\n <pre class='prettyprint'># Convert the geometry column datatype from a string of text into a coordinate datatype\n csa_gdf[geom] = csa_gdf[geom].apply(lambda x: loads( str(x) ))</pre> <p>Thats all! Now lets see the geometry columns data-type and the entire tables&#39;s data-type</p>\n <pre class='prettyprint'>csa_gdf.dtypes</pre> <pre class='prettyprint'>type(csa_gdf)</pre> <p>As you can see, we have a geometry column of the right datatype, but our table is still only just a dataframe. </p>\n<p> But now, you are ready to convert your entire pandas dataframe into a geo-dataframe.</p>\n <p>You can do that by running the following function: </p>\n <pre class='prettyprint'># Process the dataframe as a geodataframe with a known CRS and geom column\n csa_gdf = GeoDataFrame(csa_gdf, crs=crs, geometry=geom)</pre> <p>Aaaand BOOM.</p>\n <pre class='prettyprint'>csa_gdf.plot(column='hhchpov18')</pre> <p>goes the dy-no-mite</p>\n <pre class='prettyprint'>type(csa_gdf)</pre> </details>\n <details>\n <summary> <h4 id=\"approach-2-method-2-convert-columns-to-coordinate\">Approach 2: Method 2: Convert Column(s) to Coordinate</h4>\n </summary> <p>This is the generic example but it will not work since no URL is given.</p>\n <pre class='prettyprint'># More Information: https://geopandas.readthedocs.io/en/latest/gallery/create_geopandas_from_pandas.html#from-longitudes-and-latitudes\n  \n # If your data has coordinates in two columns run this cell\n # It will create a geometry column from the two.\n # A public dataset is not provided for this example and will not run.\n  \n # Load DF HERE. Accidently deleted the link. Need to refind. \n # Just rely on example 2 for now. \n \"\"\"\n exe_df['x'] = pd.to_numeric(exe_df['x'], errors='coerce')\n exe_df['y'] = pd.to_numeric(exe_df['y'], errors='coerce')\n # exe_df = exe_df.replace(np.nan, 0, regex=True)\n  \n # An example of loading in an internal BNIA file\n geometry=[Point(xy) for xy in zip(exe_df.x, exe_df.y)]\n exe_gdf = gpd.GeoDataFrame( exe_df.drop(['x', 'y'], axis=1), crs=crs, geometry=geometry)\n \"\"\"</pre>  <h5 id=\"approach-2-method-2--example-geoloom\">Approach 2: Method 2:  Example: Geoloom</h5>\n <p>Since I do not readily have a dataset with lat and long&#39;s I will have to make one.</p>\n<p> We can split the coordinates from a geodataframe like so...</p>\n <pre class='prettyprint'># Alternate Primary Table\n # Table: Geoloom, \n # Columns:  \n # In this example, we are going to read in a shapefile\n geoloom_gdf = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Geoloom_Crowd/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\");\n # then create columns for its x and y coords\n geoloom_gdf['POINT_X'] = geoloom_gdf['geometry'].centroid.x\n geoloom_gdf['POINT_Y'] = geoloom_gdf['geometry'].centroid.y\n # Now lets just drop the geometry column and save it to have our example dataset. \n geoloom_gdf = geoloom_gdf.dropna(subset=['geometry'])\n geoloom_gdf.to_csv('example.csv')</pre> <p>The first thing you will want to do when given a dataset with a coordinates column is ensure its datatype.</p>\n <pre class='prettyprint'>geoloom_df = pd.read_csv('example.csv')\n # We already know the x and y columns because we just saved them as such.\n geoloom_df['POINT_X'] = pd.to_numeric(geoloom_df['POINT_X'], errors='coerce')\n geoloom_df['POINT_Y'] = pd.to_numeric(geoloom_df['POINT_Y'], errors='coerce')\n # df = df.replace(np.nan, 0, regex=True)\n  \n # And filter out for points only in Baltimore City. \n geoloom_df = geoloom_df[ geoloom_df['POINT_Y'] > 39.3  ]\n geoloom_df = geoloom_df[ geoloom_df['POINT_Y'] < 39.5  ]</pre> <pre class='prettyprint'># An example of loading in an internal BNIA file\n crs = {'init' :'epsg:2248'} \n geometry=[Point(xy) for xy in zip(geoloom_df['POINT_X'], geoloom_df['POINT_Y'])]\n geoloom_gdf = gpd.GeoDataFrame( geoloom_df.drop(['POINT_X', 'POINT_Y'], axis=1), crs=crs, geometry=geometry)\n # 39.2904° N, 76.6122°</pre> <pre class='prettyprint'>geoloom_gdf.head(1)</pre> <p>Heres a neat trick to make it more presentable, because those points mean nothing to me.</p>\n <pre class='prettyprint'># Create our base layer.\n ax = csa_gdf.plot(column='hhchpov18', edgecolor='black')\n  \n # now plot our points over it.\n geoloom_gdf.plot(ax=ax, color='red')\n  \n plt.show()</pre> </details>\n <details>\n <summary> <h4 id=\"approach-2-method-3-using-a-crosswalk-need-crosswalk-on-esri\">Approach 2: Method 3: Using a Crosswalk (Need Crosswalk on Esri)</h4>\n </summary> <p>When you want to merge two datasets that do not share a common column, it is often useful to create a &#39;crosswalk&#39; file that &#39;maps&#39; records between two datasets. We can do this to append spatial data when a direct merge is not readily evident. </p>\n<p> Check out this next example where we pull ACS Census data and use its &#39;tract&#39; column and map it to a community. We can then aggregate the points along a the communities they belong to and map it on a choropleth!</p>\n <p>We will set up our ACS query variables right here for easy changing</p>\n <pre class='prettyprint'># Our download function will use Baltimore City's tract, county and state as internal paramters\n # Change these values in the cell below using different geographic reference codes will change those parameters\n tract = '*'\n county = '510' # '059' # 153 '510'\n state = '24' #51\n  \n # Specify the download parameters the function will receieve here\n tableId = 'B19049' # 'B19001'\n year = '17'\n saveAcs = True </pre> <p>And now we will call the function with those variables and check out the result</p>\n <pre class='prettyprint'>retrieve_acs_data = acsDownload.retrieve_acs_data\n IPython.core.display.HTML(\"<style>.rendered_html th {max-width: 200px; overflow:auto;}</style>\")\n # state, county, tract, tableId, year, saveOriginal, save \n df = retrieve_acs_data(state, county, tract, tableId, year)\n df.head(1)\n df.to_csv('tracts_data.csv')</pre> <p>This contains the CSA labels we will map our tracts to. This terminal command will download it</p>\n <pre class='prettyprint'>!wget https://raw.githubusercontent.com/bniajfi/bniajfi/main/CSA-to-Tract-2010.csv</pre> <p>Here </p>\n <pre class='prettyprint'>crosswalk = pd.read_csv('CSA-to-Tract-2010.csv')\n crosswalk.tail(1)</pre> <pre class='prettyprint'>mergeDatasets = merge.mergeDatasets\n \n merged_df_geom = mergeDatasets(left_ds=df, right_ds=crosswalk, crosswalk_ds=False,\n                   left_col='tract', right_col='TRACTCE10',\n                   crosswalk_left_col = False, crosswalk_right_col = False,\n                   merge_how='outer', # left right or columnname to retrieve\n                   interactive=False)\n merged_df_geom.head(1)</pre> <pre class='prettyprint'>import geopandas as gpd\n Hhchpov = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Hhchpov/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\")\n Hhchpov = Hhchpov[['CSA2010', 'hhchpov15',\t'hhchpov16',\t'hhchpov17',\t'hhchpov18', 'geometry']]\n Hhchpov.to_file(\"Hhchpov.geojson\", driver='GeoJSON')\n Hhchpov.to_csv('Hhchpov.csv')\n gpd.read_file(\"Hhchpov.geojson\").head(1)</pre> <pre class='prettyprint'># A simple merge\n # df.merge(crosswalk, left_on='tract', right_on='TRACTCE10')</pre> <p>A simple example of how this would work</p>\n <pre class='prettyprint'># A simple merge\n merged_df = mergeDatasets(left_ds=merged_df_geom, right_ds=Hhchpov, crosswalk_ds=False,\n                   left_col='CSA2010', right_col='CSA2010',\n                   crosswalk_left_col = False, crosswalk_right_col = False,\n                   merge_how='outer', # left right or columnname to retrieve\n                   interactive=False)</pre> <pre class='prettyprint'># geoms.readInGeometryData(url='Hhchpov.geojson').head(0)</pre> <pre class='prettyprint'>\n # The attributes are what we will use.\n in_crs = 2248 # The CRS we recieve our data \n out_crs = 4326 # The CRS we would like to have our data represented as\n geom = 'geometry' # The column where our spatial information lives.\n \n # To create this dataset I had to commit a full outer join. \n # In this way geometries will be included even if there merge does not have a direct match. \n # What this will do is that it means at least one (near) empty record for each community will exist that includes (at minimum) the geographic information and name of a Community.\n # That way if no point level information existed in the community, that during the merge the geoboundaries are still carried over.\n \n # Primary Table\n # Description: I created a public dataset from a google xlsx sheet 'Bank Addresses and Census Tract'.\n # Table: FDIC Baltimore Banks\n # Columns: Bank Name, Address(es), Census Tract\n left_ds = 'tracts_data.csv'\n left_col = 'tract'\n \n # Crosswalk Table\n # Table: Crosswalk Census Communities\n # 'TRACT2010', 'GEOID2010', 'CSA2010'\n crosswalk_ds = 'CSA-to-Tract-2010.csv'\n use_crosswalk = True\n crosswalk_left_col = 'TRACTCE10'\n crosswalk_right_col = 'CSA2010'\n \n # Secondary Table\n # Table: Baltimore Boundaries => HHCHPOV\n # 'TRACTCE10', 'GEOID10', 'CSA', 'NAME10', 'Tract', 'geometry'\n right_ds = 'Hhchpov.geojson'\n right_col ='CSA2010'\n \n interactive = True\n merge_how = 'outer'\n \n # reutrns a pandas dataframe\n mergedf = merge.mergeDatasets( left_ds=left_ds, left_col=left_col, \n               crosswalk_ds=crosswalk_ds,\n               crosswalk_left_col = crosswalk_left_col, crosswalk_right_col = crosswalk_right_col,\n               right_ds=right_ds, right_col=right_col, \n               merge_how=merge_how, interactive = interactive )</pre> <pre class='prettyprint'>mergedf.dtypes</pre> <pre class='prettyprint'># Convert the geometry column datatype from a string of text into a coordinate datatype\n # mergedf[geom] = mergedf[geom].apply(lambda x: loads( str(x) ) ) \n \n # Process the dataframe as a geodataframe with a known CRS and geom column \n mergedGdf = GeoDataFrame(mergedf, crs=in_crs, geometry=geom) </pre> <pre class='prettyprint'>mergedGdf.plot()</pre> <h5 id=\"approach-2-method-4-geocoding-addresses-and-landmarks-to-coordinates\">Approach 2: Method 4: Geocoding Addresses and Landmarks to Coordinates</h5>\n <p>Sometimes (usually) we just don&#39;t have the coordinates of a place, but we do know it&#39;s address or that it is an established landmark. </p>\n<p> In such cases we attempt &#39;geo-coding&#39; these points in an automated manner.</p>\n<p> While convenient, this process is error prone, so be sure to check it&#39;s work!</p>\n <p>For this next example to take place, we need a dataset that has a bunch of addresses. </p>\n <p>We can use the geoloom dataset from before in this example. We&#39;ll just drop geo&#39;spatial data.</p>\n <pre class='prettyprint'>geoloom = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Geoloom_Crowd/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\");\n geoloom = geoloom.dropna(subset=['geometry'])\n geoloom = geoloom.drop(columns=['geometry','GlobalID', 'POINT_X',\t'POINT_Y'])\n geoloom.head(1)</pre> <p>But if for whatever reason the link is down, you can use this example dataframe mapping just some of the many malls in baltimore.</p>\n <pre class='prettyprint'>address_df = pd.DataFrame({ \n     'Location' : pd.Series([\n     '100 N. Holliday St, Baltimore, MD 21202',\n     '200 E Pratt St, Baltimore, MD',\n     '2401 Liberty Heights Ave, Baltimore, MD',\n     '201 E Pratt St, Baltimore, MD',\n     '3501 Boston St, Baltimore, MD',\n     '857 E Fort Ave, Baltimore, MD',\n     '2413 Frederick Ave, Baltimore, MD'\n   ]),\n     'Address' : pd.Series([ \n     'Baltimore City Council',\n     'The Gallery at Harborplace',\n     'Mondawmin Mall',\n     'Harborplace',\n     'The Shops at Canton Crossing',\n     'Southside Marketplace',\n     'Westside Shopping Center'\n   ])\n })\n \n address_df.head()</pre> <p>You can use either the Location or Address column to perform the geo-coding on.</p>\n <pre class='prettyprint'>address_df = geoloom.copy()\n addrCol = 'Location'</pre> <p>This function takes a while. The less columns/data/records the faster it executes.</p>\n <pre class='prettyprint'># More information vist: https://geopy.readthedocs.io/en/stable/#module-geopy.geocoders\n \n # In this example we retrieve and map a dataset with no lat/lng but containing an address\n \n # In this example our data is stored in the 'STREET' attribute\n geometry = []\n geolocator = Nominatim(user_agent=\"my-application\")\n \n for index, row in address_df.iterrows():\n   # We will try and return an address for each Street Name\n   try: \n       # retrieve the geocoded information of our street address\n       geol = geolocator.geocode(row[addrCol], timeout=None)\n \n       # create a mappable coordinate point from the response object's lat/lang values.\n       pnt = Point(geol.longitude, geol.latitude)\n       \n       # Append this value to the list of geometries\n       geometry.append(pnt)\n       \n   except: \n       # If no street name was found decide what to do here.\n       # df.loc[index]['geom'] = Point(0,0) # Alternate method\n       geometry.append(Point(0,0))\n       \n # Finally, we stuff the geometry data we created back into the dataframe\n address_df['geometry'] = geometry</pre> <pre class='prettyprint'>address_df.head(1)</pre> <p>Awesome! Now convert the dataframe into a geodataframe and map it!</p>\n <pre class='prettyprint'>gdf = gpd.GeoDataFrame( address_df, geometry=geometry)\n gdf = gdf[ gdf.centroid.y > 39.3  ]\n gdf = gdf[ gdf.centroid.y < 39.5  ]</pre> <pre class='prettyprint'># Create our base layer.\n ax = csa_gdf.plot(column='hhchpov18', edgecolor='black')\n \n # now plot our points over it.\n geoloom_gdf.plot(ax=ax, color='red')</pre> <p>A litte later down, we&#39;ll see how to make this even-more interactive.</p>\n </details>\n </details>\n <details>\n <summary> <h3 id=\"approach-3-connecting-to-a-postgis-database\">Approach 3: Connecting to a PostGIS database</h3>\n </summary> </summary> <p>In the following example pulls point geodata from a Postgres database.</p>\n<p> We will pull the postgres point data in two manners. </p>\n<ul>\n<li><p>SQL query where an SQL query uses ST_Transform(the_geom,4326) to transform the_geom&#39;s CRS from a DATABASE Binary encoding into standard Lat Long&#39;s</p></li>\n<li><p>Using a plan SQL query and performing the conversion using gpd.io.sql.read_postgis() to pull the data in as 2248 and convert the CRS using .to_crs(epsg=4326)</p></li>\n<li><p>These examples will not work in colabs as their is no local database to connect to and has been commented out for that reason</p></li>\n</ul>\n <pre class='prettyprint'># This Notebook can be downloaded to connect to a database\n '''\n conn = psycopg2.connect(host='', dbname='', user='', password='', port='')\n \n # DB Import Method One\n sql1 = 'SELECT the_geom, gid, geogcode, ooi, address, addrtyp, city, block, lot, desclu, existing FROM housing.mdprop_2017v2 limit 100;'\n pointData = gpd.io.sql.read_postgis(sql1, conn, geom_col='the_geom', crs=2248)\n pointData = pointData.to_crs(epsg=4326)\n \n # DB Import Method Two\n sql2 = 'SELECT ST_Transform(the_geom,4326) as the_geom, ooi, desclu, address FROM housing.mdprop_2017v2;'\n pointData = gpd.GeoDataFrame.from_postgis(sql2, conn, geom_col='the_geom', crs=4326)\n pointData.head()\n pointData.plot()\n '''</pre> </details>\n </details>\n <details>\n <summary> <h2 id=\"basic-operations\">Basic Operations</h2>\n </summary>\n <details>\n <summary> <h3 id=\"inspection\">Inspection</h3>\n </summary> <pre class='prettyprint'>def geomSummary(gdf): return type(gdf), gdf.crs, gdf.columns;\n # for p in df['Tract'].sort_values(): print(p)\n geomSummary(csa_gdf)</pre> </details>\n <details>\n <summary> <h3 id=\"converting-crs\">Converting CRS</h3>\n </summary> <pre class='prettyprint'># Convert the CRS of the dataset into one you desire\n # The gdf must be loaded with a known crs in order for the to_crs conversion to work\n # We use this often to converting BNIAs custom CRS to the common type \n out_crs = 4326\n csa_gdf = csa_gdf.to_crs(epsg=out_crs)</pre> </details>\n <details>\n <summary> <h3 id=\"saving\">Saving</h3>\n </summary> <pre class='prettyprint'># Here is code to comit a simple save\n filename = 'TEST_FILE_NAME'\n csa_gdf.to_file(f\"{filename}.geojson\", driver='GeoJSON')</pre> <pre class='prettyprint'># Here is code to save this new projection as a geojson file and read it back in\n csa_gdf = csa_gdf.to_crs(epsg=2248) #just making sure\n csa_gdf.to_file(filename+'.shp', driver='ESRI Shapefile')\n csa_gdf = gpd.read_file(filename+'.shp')</pre> </details>\n <details>\n <summary> <h3 id=\"draw-tool\">Draw Tool</h3>\n </summary> <pre class='prettyprint'>import folium\n from folium.plugins import Draw\n # Draw tool. Create and export your own boundaries\n m = folium.Map()\n draw = Draw()\n draw.add_to(m)\n m = folium.Map(location=[39.28759453969165, -76.61278931706487], zoom_start=12)\n draw = Draw(export=True)\n draw.add_to(m)\n # m.save(os.path.join('results', 'Draw1.html'))\n m</pre> </details>\n <details>\n <summary> <h3 id=\"geometric-manipulations\">Geometric Manipulations</h3>\n </summary> <p>Boundary</p>\n <pre class='prettyprint'>newcsa = csa_gdf.copy()\n newcsa['geometry'] = csa_gdf.boundary\n newcsa.plot(column='CSA2010' )</pre> <p>envelope</p>\n <pre class='prettyprint'>newcsa = csa_gdf.copy()\n newcsa['geometry'] = csa_gdf.envelope\n newcsa.plot(column='CSA2010' )</pre> <p>convex_hull</p>\n <pre class='prettyprint'>newcsa = csa_gdf.copy()\n newcsa['geometry'] = csa_gdf.convex_hull\n newcsa.plot(column='CSA2010' )\n # , cmap='OrRd', scheme='quantiles'\n # newcsa.boundary.plot(  )</pre> <p>simplify</p>\n <pre class='prettyprint'>newcsa = csa_gdf.copy()\n newcsa['geometry'] = csa_gdf.simplify(30)\n newcsa.plot(column='CSA2010' )</pre> <p>buffer</p>\n <pre class='prettyprint'>newcsa = csa_gdf.copy()\n newcsa['geometry'] = csa_gdf.buffer(0.01)\n newcsa.plot(column='CSA2010' )</pre> <p>rotate</p>\n <pre class='prettyprint'>newcsa = csa_gdf.copy()\n newcsa['geometry'] = csa_gdf.rotate(30)\n newcsa.plot(column='CSA2010' )</pre> <p>scale</p>\n <pre class='prettyprint'>newcsa = csa_gdf.copy()\n newcsa['geometry'] = csa_gdf.scale(3, 2)\n newcsa.plot(column='CSA2010' )</pre> <p>skew</p>\n <pre class='prettyprint'>newcsa = csa_gdf.copy()\n newcsa['geometry'] = csa_gdf.skew(1, 10)\n newcsa.plot(column='CSA2010' )</pre> </details>\n </details>\n <details>\n <summary> <h2 id=\"advanced\">Advanced</h2>\n </summary> <h3 id=\"create-geospatial-functions\">Create Geospatial Functions</h3>\n </summary> <p><strong>Operations:</strong> </p>\n<ul>\n<li>Reading in data (points/ geoms)\n -- Convert lat/lng columns to point coordinates\n -- Geocoding address to coordinates\n -- Changing coordinate reference systems\n -- Connecting to PostGisDB&#39;s</li>\n<li><p>Basic Operations</p></li>\n<li><p>Saving shape data</p></li>\n<li><p>Get Polygon Centroids</p></li>\n<li>Working with Points and Polygons\n -- Map Points and Polygons\n -- Get Points in Polygons</li>\n</ul>\n<p> <strong>Input(s):</strong> </p>\n<ul>\n<li><p>Dataset (points/ bounds) url</p></li>\n<li><p>Points/ bounds geometry column(s)</p></li>\n<li><p>Points/ bounds crs&#39;s</p></li>\n<li><p>Points/ bounds mapping color(s)</p></li>\n<li><p>New filename</p></li>\n</ul>\n<p> <strong>Output:</strong> File</p>\n <p>This function will handle common geo spatial exploratory methods. It covers everything discussed in the basic operations and more!</p>\n <pre class='prettyprint'>#\n # Work With Geometry Data\n # Description: geomSummary, getPointsInPolygons, getPolygonOnPoints, mapPointsInPolygons, getCentroids\n #\n def workWithGeometryData(method=False, df=False, polys=False, ptsCoordCol=False, polygonsCoordCol=False, polyColorCol=False, polygonsLabel='polyOnPoint', pntsClr='red', polysClr='white', interactive=False):\n   def geomSummary(df): return type(df), df.crs, df.columns;\n   def getCentroid(df, col): return df[col].representative_point() # df['geometry'].centroid\n \n   # To 'import' a script you wrote, map its filepath into the sys\n   def getPolygonOnPoints(pts, polygons, ptsCoordCol, polygonsCoordCol, polygonsLabel, interactive):\n       count = 0\n       # We're going to keep a list of how many points we find.\n       boundaries = []\n \n       # Loop over polygons with index i.\n       for i, pt in pts.iterrows():\n           # print('Searching for point within Geom:', pt )\n           # Only one Label is accepted.\n           poly_on_this_point = []\n           # Now loop over all polygons with index j.\n           for j, poly in polygons.iterrows():\n               if poly[polygonsCoordCol].contains(pt[ptsCoordCol]):\n                   # Then it's a hit! Add it to the list\n                   poly_on_this_point.append(poly[polygonsLabel])\n                   count = count + 1\n                   # pts = pts.drop([j])\n \n           # We could do all sorts, like grab a property of the\n           # points, but let's just append the number of them.\n           boundaries.append(poly_on_this_point)\n           clear_output(wait=True)\n \n       # Add the number of points for each poly to the dataframe.\n       pts = pts.assign(CSA2010 = boundaries)\n       if (interactive):\n         print( 'Total Points: ', (pts.size / len(pts.columns) ) )\n         print( 'Total Points in Polygons: ', count )\n         print( 'Prcnt Points in Polygons: ', count / (pts.size / len(pts.columns) ) )\n       return pts\n \n   # To 'import' a script you wrote, map its filepath into the sys\n   def getPointsInPolygons(pts, polygons, ptsCoordCol, polygonsCoordCol, interactive):\n     count = 0\n     total = pts.size / len(pts.columns)\n     # We're going to keep a list of how many points we find.\n     pts_in_polys = []\n \n     # Loop over polygons with index i.\n     for i, poly in polygons.iterrows():\n         # print('Searching for point within Geom:', poly )\n         # Keep a list of points in this poly\n         pts_in_this_poly = 0\n \n         # Now loop over all points with index j.\n         for j, pt in pts.iterrows():\n             if poly[polygonsCoordCol].contains(pt[ptsCoordCol]):\n                 # Then it's a hit! Add it to the list,\n                 pts_in_this_poly += 1\n                 # and drop it so we have less hunting. # pts = pts.drop([j])\n \n         # We could do all sorts, like grab a property of the\n         # points, but let's just append the number of them.\n         pts_in_polys.append(pts_in_this_poly)\n         if (interactive): print('Found this many points within the Geom:', pts_in_this_poly )\n         count += pts_in_this_poly\n         clear_output(wait=True)\n \n     # Add the number of points for each poly to the dataframe.\n     polygons['pointsinpolygon'] = gpd.GeoSeries(pts_in_polys)\n     if (interactive):\n       print( 'Total Points: ', total )\n       print( 'Total Points in Polygons: ', count )\n       print( 'Prcnt Points in Polygons: ', count / total )\n     return polygons\n \n   def mapPointsandPolygons(pnts, polys, pntsCl, polysClr, polyColorCol):\n     print('mapPointsandPolygons');\n     # We restrict to South America.\n     ax = 1\n     if polyColorCol:\n       ax = polys.plot( column=polyColorCol, legend=True)\n     else:\n       ax = polys.plot( color=polysClr, edgecolor='black')\n \n     # We can now plot our ``GeoDataFrame``.\n     pnts.plot(ax=ax, color=pntsClr)\n \n     return plt.show()\n \n   if method=='summary': return geomSummary(df);\n   if method=='ponp': return getPolygonOnPoints(df, polys, ptsCoordCol, polygonsCoordCol, polygonsLabel, interactive);\n   if method=='pinp': return getPointsInPolygons(df, polys, ptsCoordCol, polygonsCoordCol, interactive);\n   if method=='pandp': return mapPointsandPolygons(df, polys, pntsClr, polysClr, polyColorCol);\n   if method=='centroid': return getCentroid(df, col);</pre> <pre class='prettyprint'>def maps_points(df, lat_col='POINT_Y', lon_col='POINT_X', zoom_start=11, \\\n                 plot_points=False, pt_radius=15, \\\n                 draw_heatmap=True, heat_map_weights_col=None, \\\n                 heat_map_weights_normalize=True, heat_map_radius=15):\n     \"\"\"Creates a map given a dataframe of points. Can also produce a heatmap overlay\n     Arg:\n         df: dataframe containing points to maps\n         lat_col: Column containing latitude (string) \n     \"\"\"\n \n     ## center map in the middle of points center in\n     middle_lat = df[lat_col].median()\n     middle_lon = df[lon_col].median()\n \n     curr_map = folium.Map(location=[middle_lat, middle_lon],\n                           zoom_start=zoom_start)\n \n     # add points to map\n     if plot_points:\n         for _, row in df.iterrows():\n             folium.CircleMarker([row[lat_col], row[lon_col]],\n                                 radius=pt_radius,\n                                 popup=row['name'],\n                                 fill_color=\"#3db7e4\", # divvy color\n                                ).add_to(curr_map)\n \n     # add heatmap\n     if draw_heatmap:\n         # convert to (n, 2) or (n, 3) matrix format\n         if heat_map_weights_col is None:\n             stations = zip(df[lat_col], df[lon_col])\n         else:\n             # if we have to normalize\n             if heat_map_weights_normalize:\n                 df[heat_map_weights_col] = \\\n                     df[heat_map_weights_col] / df[heat_map_weights_col].sum()\n \n             stations = zip(df[lat_col], df[lon_col], df[heat_map_weights_col])\n \n         curr_map.add_child(plugins.HeatMap(stations, radius=heat_map_radius))\n \n     return curr_map</pre> <pre class='prettyprint'>\n # reverseGeoCode, readFile, getGeoParams, main\n def readInGeometryData(url=False, porg=False, geom=False, lat=False, lng=False, revgeocode=False,  save=False, in_crs=4326, out_crs=False):\n \n   def reverseGeoCode(df, lat ):\n     # STREET\tCITY\tSTATE ZIP NAME\n     # , format_string=\"%s, BALTIMORE MD\"\n     geometry = []\n     geolocator = Nominatim(user_agent=\"my-application\")\n     for index, row in df.iterrows():\n       try:\n           geol = geolocator.geocode(row[lat], timeout=None)\n           pnt = Point(geol.longitude, geol.latitude)\n           geometry.append(pnt)\n       except:\n           geometry.append(Point(-76, 39) )\n           print(row[lat]);\n     return geometry\n \n   def readFile(url, geom, lat, lng, revgeocode, in_crs, out_crs):\n     df = False\n     gdf = False\n     ext = isinstance(url, pd.DataFrame)\n     if ext: ext='csv'\n     else: ext = url[-3:]\n \n     #XLS\n     # b16 = pd.read_excel('Jones.BirthsbyCensus2016.XLS', sheetname='Births')\n \n     # The file extension is used to determine the appropriate import method.\n     if ext in ['son', 'kml', 'shp', 'pgeojson']: gdf = gpd.read_file(url)\n     if ext == 'csv':\n       df = url if isinstance(url, pd.DataFrame) else pd.read_csv(url)\n       # Read using Geom, Lat, Lat/Lng, revGeoCode\n       if revgeocode=='y': df['geometry'] = reverseGeoCode(df, lat)\n       elif geom: df['geometry'] = df[geom].apply(lambda x: loads( str(x) ))\n       elif lat==lng: df['geometry'] = df[lat].apply(lambda x: loads( str(x) ))\n       elif lat!=lng: df['geometry'] = gpd.points_from_xy(df[lng], df[lat]);\n \n       gdf = GeoDataFrame(df, crs=in_crs, geometry='geometry') #crs=2248\n       if not out_crs == in_crs: gdf = gdf.to_crs(epsg=out_crs)\n     return gdf\n \n   def getGeoParams(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs):\n     addr=False\n \n     if not url: url = input(\"Please enter the location of your dataset: \" )\n     # if url[-3:] == 'csv' :\n     #   df = pd.read_csv(url,index_col=0,nrows=1)\n     #   print(df.columns)\n \n     # Geometries inside\n     if geom and not (lat and lng): porg = 'g'\n     # Point data inside\n     elif not geom and lat or lng:\n       porg = 'p';\n       if not lat: lat = lng\n       if not lng: lng = lat\n \n     # If the P/G could not be infered...\n     if not (porg in ['p', 'g']):\n       if not revgeocode in ['y', 'n']: revgeocode = input(\"Do your records need reverse geocoding: (Enter: y/n') \" )\n       if revgeocode == 'y': porg = 'p'; lng = lat = input(\"Please enter the column name where the address is stored: \" );\n       elif revgeocode == 'n': porg = input(\"\"\"Do the records in this dataset use (P)oints or (g)eometric polygons?: (Enter: 'p' or 'g') \"\"\" );\n       else: return getGeoParams(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs);\n \n       if porg=='p':\n           if not lat: lat = input(\"Please enter the column name where the latitude coordinate is stored: \" );\n           if not lng: lng = input(\"Please enter the column name where the longitude cooridnate is stored: (Could be same as the lat) \" );\n       elif porg=='g':\n         if not geom: geom = input(\"Please enter column name where the geometry data is stored: (*optional, skip if unkown)\" );\n       else: return getGeoParams(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs)\n \n     if not out_crs: out_crs=in_crs\n \n     return url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs\n \n   # This function uses all the other functions\n   def main(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs):\n \n     # Check for missing values. retrieve them\n     if (isinstance(url, pd.DataFrame)): print('Converting DF to GDF')\n     elif (not (url and porg) ) or (\n         not (porg == 'p' or porg == 'g') ) or (\n         porg == 'g' and not geom) or (\n         porg == 'p' and (not (lat and lng) ) ):\n       return readInGeometryData( *getGeoParams(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs) );\n \n     # print(f\"RECIEVED url: {url}, \\r\\n porg: {porg}, \\r\\n geom: {geom}, \\r\\n lat: {lat}, \\r\\n lng: {lng}, \\r\\n revgeocode: {revgeocode}, \\r\\n in_crs: {in_crs}, \\r\\n out_crs: {out_crs}\")\n \n     # Quit if the Columns dont exist -> CSV Only\n     # status = checkColumns(url, geom, lat, lng)\n     # if status == False: print('A specified column does not exist'); return False;\n \n     # Perform operation\n     gdf = readFile(url, geom, lat, lng, revgeocode, in_crs, out_crs)\n \n     # Tidy up\n \n     # Save\n     # if save: saveGeoData(gdf, url, fileName, driver='esri')\n \n     return gdf\n \n   return main(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs)</pre> <pre class='prettyprint'># draw_heatmap, cluster_points, plot_points,\n def map_points(data, lat_col='POINT_Y', lon_col='POINT_X', zoom_start=11, plot_points=True, cluster_points=False,\n                pt_radius=15, draw_heatmap=False, heat_map_weights_col=None, heat_map_weights_normalize=True,\n                heat_map_radius=15, popup=False):\n     \"\"\"Creates a map given a dataframe of points. Can also produce a heatmap overlay\n \n     Arg:\n         df: dataframe containing points to maps\n         lat_col: Column containing latitude (string)\n         lon_col: Column containing longitude (string)\n         zoom_start: Integer representing the initial zoom of the map\n         plot_points: Add points to map (boolean)\n         pt_radius: Size of each point\n         draw_heatmap: Add heatmap to map (boolean)\n         heat_map_weights_col: Column containing heatmap weights\n         heat_map_weights_normalize: Normalize heatmap weights (boolean)\n         heat_map_radius: Size of heatmap point\n \n     Returns:\n         folium map object\n     \"\"\"\n     df = data.copy()\n     ## center map in the middle of points center in\n     middle_lat = df[lat_col].median()\n     middle_lon = df[lon_col].median()\n \n     curr_map = folium.Map(location=[middle_lat, middle_lon], zoom_start=zoom_start)\n \n     # add points to map\n     if plot_points:\n       for _, row in df.iterrows():\n         # print([row[lat_col], row[lon_col]], row[popup])\n         folium.CircleMarker([row[lat_col], row[lon_col]],\n           radius=pt_radius,\n           popup=row[popup],\n           fill_color=\"#3db7e4\", # divvy color\n         ).add_to(curr_map)\n     if cluster_points:\n       marker_cluster = MarkerCluster().add_to(curr_map)\n       for index, row in df.iterrows():\n         folium.Marker( location=[row[lat_col],row[lon_col]], popup=row[popup], icon=None ).add_to(marker_cluster)\n \n     # add heatmap\n     if draw_heatmap:\n         # convert to (n, 2) or (n, 3) matrix format\n         if heat_map_weights_col is None:\n             stations = zip(df[lat_col], df[lon_col])\n         else:\n             # if we have to normalize\n             if heat_map_weights_normalize:\n                 df[heat_map_weights_col] = \\\n                     df[heat_map_weights_col] / df[heat_map_weights_col].sum()\n \n             stations = zip(df[lat_col], df[lon_col], df[heat_map_weights_col])\n \n         curr_map.add_child(plugins.HeatMap(stations, radius=heat_map_radius))\n \n     return curr_map</pre> <p>Processing Geometry is tedius enough to merit its own handler</p>\n <pre class='prettyprint'>geoloom_w_csas.head(1)</pre> <pre class='prettyprint'>map_points(geoloom_w_csas, lat_col='POINT_Y', lon_col='POINT_X', zoom_start=11, plot_points=True, cluster_points=True,\n                pt_radius=15, draw_heatmap=False, heat_map_weights_col=None, heat_map_weights_normalize=True,\n                heat_map_radius=15, popup='ProjNm')</pre> <p>As you can see we have a lot of points. \n Lets see if there is any better way to visualize this.</p>\n <h2 id=\"example-using-the-advanced-functions\">Example: Using the advanced Functions</h2>\n <h3 id=\"playing-with-points-geoloom\">Playing with Points: Geoloom</h3>\n <h4 id=\"points-in-polygons\">Points In Polygons</h4>\n <p>The red dots from when we mapped the geoloom points above were a bit too noisy. </p>\n<p> Lets create a choropleth instead!</p>\n<p> We can do this by <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">aggregating</a> by CSA.</p>\n<p> To do this, start of by finding which points are inside of which polygons!</p>\n <p>Since the geoloom data does not have a CSA dataset, we will need merge it to one that does!</p>\n<p> Lets use the childhood poverty link from example one and load it up because it contains the geometry data and the csa labels.</p>\n <pre class='prettyprint'># from dataplay.intaker import Intake\n # csa_gdf = Intake.getData('https://raw.githubusercontent.com/bniajfi/bniajfi/main/CSA-to-Tract-2010.csv')</pre> <pre class='prettyprint'># This dataset is taken from the public database provided by BNIAJFI hosted by Esri / ArcGIS\n # BNIA ArcGIS Homepage: https://data-bniajfi.opendata.arcgis.com/\n csa_gdf_url = \"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Hhchpov/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\"\n csa_gdf = readInGeometryData(url=csa_gdf_url, porg=False, geom='geometry', lat=False, lng=False, revgeocode=False,  save=False, in_crs=2248, out_crs=False)</pre> <p>And now lets pull in our geoloom data. But to be sure, drop the empty geometry columns or the function directly below will now work.</p>\n <pre class='prettyprint'>geoloom_gdf_url = \"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Geoloom_Crowd/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\"\n geoloom_gdf = readInGeometryData(url=geoloom_gdf_url, porg=False, geom='geometry', lat=False, lng=False, revgeocode=False,  save=False, in_crs=4326, out_crs=False)\n geoloom_gdf = geoloom_gdf.dropna(subset=['geometry'])\n # geoloom_gdf = geoloom_gdf.drop(columns=['POINT_X','POINT_Y'])\n geoloom_gdf.head(1)</pre> <p>And now use a point in polygon method &#39;ponp&#39; to get the CSA2010 column from our CSA dataset added as a column to each geoloom record. </p>\n <pre class='prettyprint'>geoloom_w_csas = workWithGeometryData(method='pinp', df=geoloom_gdf, polys=csa_gdf, ptsCoordCol='geometry', polygonsCoordCol='geometry', polyColorCol='hhchpov18', polygonsLabel='CSA2010', pntsClr='red', polysClr='white')</pre> <p>You&#39;ll see you have a &#39;pointsinpolygons&#39; column now.</p>\n <pre class='prettyprint'>geoloom_w_csas.plot( column='pointsinpolygon', legend=True)</pre> <pre class='prettyprint'>geoloom_w_csas.head(1)</pre> <h4 id=\"polygons-in-points\">Polygons in Points</h4>\n <p>Alternately, you can run the ponp function and have returned the geoloom dataset</p>\n <pre class='prettyprint'>geoloom_w_csas = workWithGeometryData(method='ponp', df=geoloom_gdf, polys=csa_gdf, ptsCoordCol='geometry', polygonsCoordCol='geometry', polyColorCol='hhchpov18', polygonsLabel='CSA2010', pntsClr='red', polysClr='white')</pre> <p>We can count the totals per CSA using <code>value_counts</code></p>\n <p>Alternately, we could map the centroid of boundaries within another boundary to find boundaries within boundaries</p>\n <pre class='prettyprint'>geoloom_w_csas['POINT_Y'] = geoloom_w_csas.centroid.y\n geoloom_w_csas['POINT_X'] = geoloom_w_csas.centroid.x\n \n # We already know the x and y columns because we just saved them as such.\n geoloom_w_csas['POINT_X'] = pd.to_numeric(geoloom_w_csas['POINT_X'], errors='coerce')\n geoloom_w_csas['POINT_Y'] = pd.to_numeric(geoloom_w_csas['POINT_Y'], errors='coerce')\n # df = df.replace(np.nan, 0, regex=True)\n \n # And filter out for points only in Baltimore City. \n geoloom_w_csas = geoloom_w_csas[ geoloom_w_csas['POINT_Y'] > 39.3  ]\n geoloom_w_csas = geoloom_w_csas[ geoloom_w_csas['POINT_Y'] < 39.5  ]</pre> <h4 id=\"folium\">Folium</h4>\n <p>But if that doesn&#39;t do it for you, we can also create heat maps and marker clusters</p>\n <pre class='prettyprint'># https://github.com/python-visualization/folium/blob/master/examples/MarkerCluster.ipynb</pre> <pre class='prettyprint'>map_points(geoloom_w_csas, lat_col='POINT_Y', lon_col='POINT_X', zoom_start=11, plot_points=True, cluster_points=False,\n                pt_radius=1, draw_heatmap=True, heat_map_weights_col=None, heat_map_weights_normalize=True,\n                heat_map_radius=15, popup='ProjNm')</pre> <p>And Time Sliders</p>\n <h5 id=\"choropleth-timeslider\">Choropleth Timeslider</h5>\n <p><a href=\"https://github.com/python-visualization/folium/blob/master/examples/TimeSliderChoropleth.ipynb\" onclick=\"window.pingServer(this)\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">https://github.com/python-visualization/folium/blob/master/examples/TimeSliderChoropleth.ipynb</a></p>\n <pre class='prettyprint'>import geopandas as gpd\n import numpy as np\n import pandas as pd\n from branca.colormap import linear\n # conditionally loaded ->  from dataplay import geoms\n \n u = intaker.Intake\n rdf = u.getData('https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Biz1_/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson')\n # rdf.set_index('CSA2010', drop=True, inplace=True)\n rdf.drop(labels=['OBJECTID_1', 'Shape__Area', 'Shape__Length'], axis=1, inplace=True)\n \n ndf = rdf.filter(regex='biz1|CSA2010', axis=1)\n \n # Calculate number of years available\n n_periods = len(ndf.columns) - 1\n # Get starting year.\n startAt = \"20\"+ndf.columns[1][-2:]\n \n # Create a 'YEAR' index with the assumption that all following years exist\n datetime_index = pd.date_range(startAt, periods=n_periods, freq=\"Y\")\n dt_index_epochs = datetime_index.astype(int) // 10 ** 9\n dt_index = dt_index_epochs.astype(\"U10\")</pre> <pre class='prettyprint'>rdf.head()</pre> <pre class='prettyprint'>styledata = {}\n # For the Index of each CSA\n for idx, csa in rdf.iterrows():\n     df = pd.DataFrame( { \"color\": csa.values[1:-1] }, index=dt_index, )\n     styledata[idx] = df\n \n max_color, min_color = 0, 0\n for country, data in styledata.items():\n     max_color = max(max_color, data[\"color\"].max())\n     min_color = min(max_color, data[\"color\"].min())\n \n cmap = linear.PuRd_09.scale(min_color, max_color)\n def norm(x): return (x - x.min()) / (x.max() - x.min())\n for country, data in styledata.items():\n     data[\"color\"] = data[\"color\"].apply(cmap)\n     data[\"opacity\"] = 1\n \n styledict = { str(country): data.to_dict(orient=\"index\") for country, data in styledata.items() }\n \n # { CSA : { timestamp: {color: value, opacity:value } }, \n #    CSA : { timestamp: {color: value, opacity:value } }, \n #    ... \n # }</pre> <pre class='prettyprint'>styledict</pre> <pre class='prettyprint'>import folium\n from folium.plugins import TimeSliderChoropleth\n \n m = folium.Map([39.28759453969165, -76.61278931706487], width='50%', height='50%', zoom_start=12)\n g = TimeSliderChoropleth( rdf.to_json(), styledict=styledict, ).add_to(m)\n m</pre> <pre class='prettyprint'>m.save(outfile= \"test.html\")</pre> <pre class='prettyprint'></details</pre>\n  <script src=\"https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js\"></script>\n  <link rel=\"stylesheet\" href=\"https://cdn.rawgit.com/google/code-prettify/master/styles/desert.css\"/>\n  "}